{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助你：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(agent101)`，即可执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助你：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# 🚀 LoRA 参数高效微调完整教程\n",
    "\n",
    "## 📖 教程概述\n",
    "\n",
    "本教程面向大模型技术初学者，通过一个完整的 SMS 垃圾短信分类任务，深入讲解 LoRA（Low-Rank Adaptation）参数高效微调技术的原理、实现和应用。\n",
    "\n",
    "### 🎯 核心功能\n",
    "- **数据准备**：自动下载并处理 SMS Spam 数据集，构建训练/验证/测试数据加载器\n",
    "- **模型适配**：加载 GPT-2 预训练模型，将语言建模头替换为二分类头\n",
    "- **LoRA 替换**：递归替换所有 Linear 层为 LoRA 层，冻结原始参数，仅训练 LoRA 参数\n",
    "- **训练评估**：完整的训练流程，包含损失可视化、准确率评估和性能分析\n",
    "\n",
    "### 🎓 学习目标\n",
    "- **理解 LoRA 原理**：掌握低秩分解如何实现参数高效微调\n",
    "- **实践微调流程**：从数据准备到模型部署的完整工作流\n",
    "- **掌握关键超参**：rank、alpha 等参数的选择和调优策略\n",
    "- **解决实际问题**：将通用语言模型适配为特定下游任务\n",
    "\n",
    "### 📊 预期效果\n",
    "- **参数效率**：仅训练少量 LoRA 参数（rank=16 时约数百万级），相比全量微调显著降低\n",
    "- **性能提升**：3-5 个 epoch 即可将准确率从 45-50% 提升至 95%+\n",
    "- **部署友好**：LoRA 增量参数体积小，便于分发和部署\n",
    "- **初始一致性**：B 矩阵初始化为 0，确保替换后初始性能不变\n",
    "\n",
    "### 💡 技术亮点\n",
    "- **低秩分解**：ΔW ≈ A × B，其中 A ∈ ℝ^(d×r)，B ∈ ℝ^(r×k)，r << min(d,k)\n",
    "- **参数冻结**：原始模型参数保持冻结，仅优化 LoRA 分支\n",
    "- **增量更新**：前向传播为 `x(W + αAB)`，无需修改原始权重\n",
    "- **灵活部署**：推理时动态叠加 LoRA 增量，支持多任务适配\n",
    "\n",
    "> 💻 **环境要求**：建议在支持 CUDA 的 GPU 环境中执行，以获得最佳训练效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pip in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: numpy<2.1,>=1.26 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas>=2.2.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas>=2.2.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas>=2.2.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas>=2.2.1) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (3.2.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from tiktoken>=0.5.1) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from tiktoken>=0.5.1) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.1) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (2025.8.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch>=2.3.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.3.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from jinja2->torch>=2.3.0) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 📦 环境依赖安装\n",
    "# \n",
    "# 本教程需要以下核心依赖包，请根据您的环境选择合适的安装方式：\n",
    "# - PyTorch：深度学习框架，支持 CPU/GPU 训练\n",
    "# - tiktoken：GPT-2 分词器，用于文本预处理\n",
    "# - pandas：数据处理，用于数据集操作\n",
    "# - matplotlib：可视化，用于绘制训练曲线\n",
    "# - numpy：数值计算基础库\n",
    "print(\"🚀 LoRA 微调项目核心依赖一键安装\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "%pip install -U pip\n",
    "%pip install -U \"numpy>=1.26,<2.1\" \"pandas>=2.2.1\" \"matplotlib>=3.7.1\" \"tiktoken>=0.5.1\"\n",
    "\n",
    "# 🔧 PyTorch 安装（根据您的硬件环境选择）\n",
    "# CPU 版本（适合轻量级实验）：\n",
    "# %pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# GPU 版本（推荐，训练速度更快）：\n",
    "# CUDA 12.1 版本（适用于大多数现代 GPU）\n",
    "%pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# CUDA 11.8 版本（适用于较老的 GPU）\n",
    "# %pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# 📚 安装配套工具库\n",
    "# 提供 GPT-2 模型下载、权重加载等辅助功能\n",
    "# %pip install -U git+https://github.com/rasbt/LLMs-from-scratch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 检查依赖包版本...\n",
      "✅ numpy: 2.0.2\n",
      "✅ pandas: 2.3.3\n",
      "✅ matplotlib: 3.10.7\n",
      "✅ tiktoken: 0.12.0\n",
      "✅ torch: 2.8.0\n",
      "\n",
      "💡 如果版本不满足要求，请重新运行上方的安装命令\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境版本检查\n",
    "# \n",
    "# 检查关键依赖包的版本，确保满足教程要求\n",
    "# 这有助于复现实验结果和排查环境问题\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "# 核心依赖包及其版本要求\n",
    "pkgs = [\n",
    "    \"numpy\",      # 数值计算基础库，需 >=1.26,<2.1（兼容性要求）\n",
    "    \"pandas\",     # 数据处理库，需 >=2.2.1（支持现代 DataFrame 操作）\n",
    "    \"matplotlib\", # 可视化库，需 >=3.7.1（支持现代绘图功能）\n",
    "    \"tiktoken\",   # GPT-2 分词器，需 >=0.5.1（支持 BPE 编码）\n",
    "    \"torch\"       # PyTorch 深度学习框架，需 >=2.3.0（支持现代特性）\n",
    "]\n",
    "\n",
    "print(\"🔍 检查依赖包版本...\")\n",
    "for pkg in pkgs:\n",
    "    try:\n",
    "        pkg_version = version(pkg)\n",
    "        print(f\"✅ {pkg}: {pkg_version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {pkg} 未安装: {e}\")\n",
    "\n",
    "print(\"\\n💡 如果版本不满足要求，请重新运行上方的安装命令\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## E.1 LoRA 简介（概念层）\n",
    "\n",
    "- 传统微调：学习完整权重更新 ΔW，使得 `W_updated = W + ΔW`。\n",
    "- LoRA：用更小的矩阵分解近似 ΔW，即 `ΔW ≈ A B`，因此 `W_updated = W + A B`。\n",
    "- 前向时可写作 `x(W + AB) = xW + xAB`，无需改写原权重 W，AB 作为“增量分支”动态叠加。\n",
    "- 超参数：\n",
    "  - rank（秩）：控制 A、B 的内维度，决定新增可训练参数量与适配能力。\n",
    "  - alpha：缩放系数，控制 LoRA 分支对原层输出的影响强度。\n",
    "- 实践意义：只训练少量参数（A、B），显著降低显存与训练时间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 📊 数据集准备与预处理\n",
    "\n",
    "### 🎯 数据集选择\n",
    "\n",
    "我们使用经典的 **SMS Spam Collection** 数据集，这是一个公开的垃圾短信分类数据集，包含：\n",
    "- **数据来源**：UCI Machine Learning Repository\n",
    "- **数据规模**：约 5,572 条短信\n",
    "- **标签分布**：ham（正常短信）vs spam（垃圾短信）\n",
    "- **应用场景**：文本分类、垃圾信息过滤\n",
    "\n",
    "### 🔧 数据预处理流程\n",
    "\n",
    "1. **数据下载**：自动从 UCI 下载原始 TSV 文件\n",
    "2. **标签映射**：将文本标签转换为数值（ham→0, spam→1）\n",
    "3. **类别平衡**：确保训练集正负样本比例均衡\n",
    "4. **数据划分**：按 7:2:1 比例分割为训练/验证/测试集\n",
    "5. **序列处理**：使用 GPT-2 分词器进行文本编码和填充\n",
    "\n",
    "### 📈 数据统计信息\n",
    "\n",
    "- **训练集**：约 70% 的数据用于模型训练\n",
    "- **验证集**：约 20% 的数据用于超参数调优和早停\n",
    "- **测试集**：约 10% 的数据用于最终性能评估\n",
    "- **序列长度**：根据训练集统计最长序列长度，确保一致性\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 数据处理核心函数库\n",
    "# \n",
    "# 本模块包含数据预处理、模型训练和评估的核心函数\n",
    "# 为 LoRA 微调提供完整的数据流水线和训练工具\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    \"\"\"\n",
    "    创建类别平衡的数据集\n",
    "    \n",
    "    功能说明：\n",
    "    - 统计垃圾短信数量，随机采样等量的正常短信\n",
    "    - 确保训练时正负样本比例均衡，避免模型偏向多数类\n",
    "    \n",
    "    参数：\n",
    "        df: 原始数据框，包含 Label 和 Text 列\n",
    "        \n",
    "    返回：\n",
    "        balanced_df: 类别平衡后的数据框\n",
    "    \"\"\"\n",
    "    # 统计垃圾短信数量\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # 随机采样等量的正常短信（固定随机种子确保可复现）\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # 合并正常短信子集和所有垃圾短信\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    \"\"\"\n",
    "    随机划分数据集\n",
    "    \n",
    "    功能说明：\n",
    "    - 按指定比例将数据划分为训练/验证/测试集\n",
    "    - 使用固定随机种子确保结果可复现\n",
    "    \n",
    "    参数：\n",
    "        df: 待划分的数据框\n",
    "        train_frac: 训练集比例\n",
    "        validation_frac: 验证集比例\n",
    "        \n",
    "    返回：\n",
    "        train_df, validation_df, test_df: 三个数据框\n",
    "    \"\"\"\n",
    "    # 随机打乱数据（固定随机种子）\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    \n",
    "    # 计算划分索引\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    \n",
    "    # 执行划分\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SMS 垃圾短信分类数据集类\n",
    "    \n",
    "    功能说明：\n",
    "    - 继承 PyTorch Dataset，支持 DataLoader 批量加载\n",
    "    - 自动处理文本分词、序列填充和标签转换\n",
    "    - 支持动态序列长度或固定长度截断\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        \n",
    "        参数：\n",
    "            csv_file: CSV 文件路径\n",
    "            tokenizer: 分词器对象（如 GPT-2 的 tiktoken）\n",
    "            max_length: 最大序列长度，None 表示使用训练集最长长度\n",
    "            pad_token_id: 填充 token 的 ID（GPT-2 默认为 50256）\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # 预分词：将文本转换为 token ID 序列\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        \n",
    "        # 确定序列长度\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # 截断过长的序列\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        # 填充序列到统一长度\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"获取单个样本\"\"\"\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),  # 输入序列\n",
    "            torch.tensor(label, dtype=torch.long)     # 标签\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"返回数据集大小\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        \"\"\"计算最长编码序列长度\"\"\"\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "\n",
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    计算数据加载器的准确率\n",
    "    \n",
    "    功能说明：\n",
    "    - 在指定设备上评估模型性能\n",
    "    - 支持限制评估批次数以节省时间\n",
    "    - 使用最后一个 token 的 logits 进行分类预测\n",
    "    \n",
    "    参数：\n",
    "        data_loader: 数据加载器\n",
    "        model: 待评估的模型\n",
    "        device: 计算设备\n",
    "        num_batches: 评估批次数，None 表示全部\n",
    "        \n",
    "    返回：\n",
    "        accuracy: 准确率（0-1 之间）\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # 使用最后一个 token 的 logits 进行分类\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "    计算单个批次的损失\n",
    "    \n",
    "    参数：\n",
    "        input_batch: 输入批次\n",
    "        target_batch: 目标批次\n",
    "        model: 模型\n",
    "        device: 计算设备\n",
    "        \n",
    "    返回：\n",
    "        loss: 交叉熵损失\n",
    "    \"\"\"\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "    # 1. 运行模型，得到所有 token 的 logits: [B, T, C]\n",
    "    full_logits = model(input_batch) \n",
    "    \n",
    "    # 2. 仅切片获取序列中最后一个 token 的 logits，用于分类: [B, C]\n",
    "    logits = full_logits[:, -1, :] \n",
    "    \n",
    "    # 3. 使用 [B, C] 的 logits 和 [B] 的标签计算损失\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    计算数据加载器的平均损失\n",
    "    \n",
    "    参数：\n",
    "        data_loader: 数据加载器\n",
    "        model: 模型\n",
    "        device: 计算设备\n",
    "        num_batches: 评估批次数\n",
    "        \n",
    "    返回：\n",
    "        average_loss: 平均损失\n",
    "    \"\"\"\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"\n",
    "    评估模型在训练集和验证集上的损失\n",
    "    \n",
    "    参数：\n",
    "        model: 待评估的模型\n",
    "        train_loader: 训练数据加载器\n",
    "        val_loader: 验证数据加载器\n",
    "        device: 计算设备\n",
    "        eval_iter: 评估批次数\n",
    "        \n",
    "    返回：\n",
    "        train_loss, val_loss: 训练和验证损失\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    \"\"\"\n",
    "    简化的分类器训练函数\n",
    "    \n",
    "    功能说明：\n",
    "    - 执行完整的训练循环\n",
    "    - 周期性评估模型性能\n",
    "    - 记录训练过程中的损失和准确率\n",
    "    \n",
    "    参数：\n",
    "        model: 待训练的模型\n",
    "        train_loader: 训练数据加载器\n",
    "        val_loader: 验证数据加载器\n",
    "        optimizer: 优化器\n",
    "        device: 计算设备\n",
    "        num_epochs: 训练轮数\n",
    "        eval_freq: 评估频率（步数）\n",
    "        eval_iter: 每次评估的批次数\n",
    "        \n",
    "    返回：\n",
    "        train_losses, val_losses, train_accs, val_accs, examples_seen: 训练记录\n",
    "    \"\"\"\n",
    "    # 初始化记录列表\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    # 主训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # 设置为训练模式\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # 清零梯度\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # 反向传播\n",
    "            optimizer.step()  # 更新参数\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "            \n",
    "            # 周期性评估\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # 每个 epoch 后计算准确率\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    \"\"\"\n",
    "    绘制训练过程可视化图表\n",
    "    \n",
    "    功能说明：\n",
    "    - 绘制训练和验证指标随时间和样本数的变化\n",
    "    - 支持双 x 轴显示（epochs 和 examples）\n",
    "    - 自动保存图表为 PDF 文件\n",
    "    \n",
    "    参数：\n",
    "        epochs_seen: 已训练的轮数\n",
    "        examples_seen: 已处理的样本数\n",
    "        train_values: 训练集指标值\n",
    "        val_values: 验证集指标值\n",
    "        label: 指标名称（如 \"loss\", \"accuracy\"）\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    \n",
    "    # 绘制训练和验证指标\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 创建第二个 x 轴显示样本数\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # 透明线用于对齐刻度\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    \"\"\"\n",
    "    对单条文本进行分类预测\n",
    "    \n",
    "    功能说明：\n",
    "    - 对输入的文本进行预处理和分词\n",
    "    - 使用训练好的模型进行预测\n",
    "    - 返回分类结果（spam 或 not spam）\n",
    "    \n",
    "    参数：\n",
    "        text: 待分类的文本\n",
    "        model: 训练好的模型\n",
    "        tokenizer: 分词器\n",
    "        device: 计算设备\n",
    "        max_length: 最大序列长度\n",
    "        pad_token_id: 填充 token ID\n",
    "        \n",
    "    返回：\n",
    "        prediction: 分类结果字符串\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # 预处理输入文本\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    \n",
    "    # 截断过长的序列\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    \n",
    "    # 填充序列\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "    \n",
    "    # 模型推理\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # 最后一个 token 的 logits\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # 返回分类结果\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已准备：train.csv / validation.csv / test.csv\n"
     ]
    }
   ],
   "source": [
    "# 数据下载与拆分（与原书 ch06 逻辑一致）\n",
    "# 步骤说明：\n",
    "# 1) 下载并解压 SMS Spam 数据集；\n",
    "# 2) 读取 TSV 文件为 DataFrame，并将标签 ham/spam 映射为 0/1；\n",
    "# 3) 使用辅助函数做类别平衡与划分训练/验证/测试集；\n",
    "# 4) 将划分结果保存为 CSV，方便后续 Dataset 加载。\n",
    "\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_file_path = \"./sms_spam_collection/SMSSpamCollection.tsv\"\n",
    "\n",
    "\n",
    "# 读取、平衡、划分\n",
    "# 原始文件为制表符分隔，列为 Label, Text\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "# 保证类别分布更均衡，避免训练时偏斜\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "# 标签映射：ham->0, spam->1，便于后续计算损失\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# 按 7:2:1（训练:测试:验证）比例划分\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "# 落盘，便于复用与调试\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)\n",
    "\n",
    "print(\"数据已准备：train.csv / validation.csv / test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GPT-2 分词器信息 ---\n",
      "分词器名称: gpt2\n",
      "词汇表大小 (总 Token 数): 50257\n",
      "EOT (End-of-Text) Token ID: 50256\n",
      "--- 批次数 ---\n",
      "训练批次数: 130 | 验证批次数: 19 | 测试批次数: 38\n"
     ]
    }
   ],
   "source": [
    "# 构建 Dataset 与 DataLoader\n",
    "# 说明：\n",
    "# - 使用 GPT-2 的 BPE 分词器（tiktoken.get_encoding(\"gpt2\")）。\n",
    "# - 训练集初始化时不设 max_length（内部会统计训练集的最长长度），\n",
    "#   将该长度传给验证/测试，以保证三者序列长度一致，避免评估偏差。\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 初始化 GPT-2 分词器\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"--- GPT-2 分词器信息 ---\")\n",
    "print(f\"分词器名称: {tokenizer.name}\")\n",
    "print(f\"词汇表大小 (总 Token 数): {tokenizer.max_token_value + 1}\")\n",
    "print(f\"EOT (End-of-Text) Token ID: {tokenizer.eot_token}\")\n",
    "\n",
    "# 构建三份数据集；验证/测试共用训练集统计出的 max_length\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "\n",
    "# DataLoader 参数：\n",
    "# - batch_size: 每批样本数；\n",
    "# - shuffle: 训练集需打乱；\n",
    "# - drop_last: 训练时丢弃最后不足一批的数据，便于批归一；\n",
    "# - num_workers: 数据加载子进程数（Windows/笔记本环境可设 0）。\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)  # 固定随机种子，保证结果可复现\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "print(\"--- 批次数 ---\")\n",
    "print(f\"训练批次数: {len(train_loader)} | 验证批次数: {len(val_loader)} | 测试批次数: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## E.3 初始化 GPT-2 并改为分类任务\n",
    "\n",
    "步骤：\n",
    "1. 下载并加载 GPT-2 权重到自定义 `GPTModel`。\n",
    "2. 将语言建模头替换为二分类头（输出维度=2）。\n",
    "3. 验证加载是否正常：让模型生成一段文本观察输出是否通顺。\n",
    "4. 查看未微调前的基线准确率。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, n_layers):\n",
    "        self.cache = [None] * n_layers\n",
    "\n",
    "    def get(self, layer_idx):\n",
    "        return self.cache[layer_idx]\n",
    "\n",
    "    def update(self, layer_idx, value):\n",
    "        self.cache[layer_idx] = value\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.cache\n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.cache)):\n",
    "            self.cache[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import requests\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path, backup_url)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        response = requests.get(download_url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "        # Check if file exists and has same size\n",
    "        if os.path.exists(destination):\n",
    "            file_size_local = os.path.getsize(destination)\n",
    "            if file_size and file_size == file_size_local:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return True\n",
    "\n",
    "        block_size = 1024  # 1 KB\n",
    "        desc = os.path.basename(download_url)\n",
    "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n",
    "            with open(destination, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=block_size):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except requests.exceptions.RequestException:\n",
    "        if backup_url is not None:\n",
    "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except requests.exceptions.RequestException:\n",
    "                pass\n",
    "\n",
    "        error_message = (\n",
    "            f\"Failed to download from both primary URL ({url})\"\n",
    "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
    "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
    "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
    "        )\n",
    "        print(error_message)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
    "# Source for \"Build a Large Language Model From Scratch\"\n",
    "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
    "#\n",
    "# This file collects all the relevant code that we covered thus far\n",
    "# throughout Chapters 2-6.\n",
    "# This file can be run as a standalone script.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 2\n",
    "#####################################\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # logits 在 GPU 上\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 5\n",
    "#####################################\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Every effort moves you\"\"\"\"\"\"\"!\"\"\"\"\"\"!\n",
      "评估未微调基线（各取 10 批）...\n",
      "Train: 46.25% | Val: 45.00% | Test: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# 加载 GPT-2 预训练模型并改为分类任务\n",
    "# 说明：\n",
    "# - 使用项目提供的下载与装载函数，将公开的 GPT-2 预训练权重导入自定义 `GPTModel`。\n",
    "# - 将原本的语言建模输出头替换为二分类头（输出维度=2）。\n",
    "# - 通过一次简单文本生成做 sanity check，确认模型加载无误。\n",
    "# - 评估 LoRA 前的基线准确率（少量批次抽样），作为对比基准。\n",
    "\n",
    "# from gpt_download import download_and_load_gpt2\n",
    "# from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# from previous_chapters import (\n",
    "#     generate_text_simple,\n",
    "#     text_to_token_ids,\n",
    "#     token_ids_to_text,\n",
    "#     calc_accuracy_loader\n",
    "# )\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"  # 可改为 medium/large/xl，但显存与下载时长会增加\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # GPT-2 词表大小\n",
    "    \"context_length\": 1024,  # 最大上下文长度\n",
    "    \"drop_rate\": 0.0,        # 先不加 dropout，便于对比\n",
    "    \"qkv_bias\": True         # 注意力 QKV 是否使用 bias\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# 下载权重并装载到自定义 GPTModel\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()\n",
    "\n",
    "# 替换输出头为二分类头\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
    "\n",
    "# 设备选择（优先使用 CUDA，其次 CPU）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 文本生成 sanity check（加载是否成功）\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "# 1. 生成 token ID 张量（默认在 CPU 上）\n",
    "input_ids_cpu = text_to_token_ids(text_1, tokenizer)\n",
    "# 2. 🌟 关键修正：将输入张量移动到目标设备 (GPU/CUDA) 🌟\n",
    "input_ids_device = input_ids_cpu.to(device)\n",
    "# 3. 使用已在正确设备上的张量进行文本生成\n",
    "ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=input_ids_device,  # 使用已移动到 device 的张量\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    ")\n",
    "print(token_ids_to_text(ids, tokenizer))\n",
    "\n",
    "# 文本生成 sanity check（加载是否成功）\n",
    "# text_1 = \"Every effort moves you\"\n",
    "# ids = generate_text_simple(\n",
    "#     model=model,\n",
    "#     idx=text_to_token_ids(text_1, tokenizer),\n",
    "#     max_new_tokens=15,\n",
    "#     context_size=BASE_CONFIG[\"context_length\"],\n",
    "# )\n",
    "# print(token_ids_to_text(ids, tokenizer))\n",
    "\n",
    "# 未微调前的基线准确率（抽样评估 10 批）\n",
    "torch.manual_seed(123)\n",
    "print(\"评估未微调基线（各取 10 批）...\")\n",
    "train_acc = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_acc = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_acc = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "print(f\"Train: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}% | Test: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## E.4 LoRA 实现与全模型替换\n",
    "\n",
    "我们实现两个类：\n",
    "- `LoRALayer`：构造 A、B 两个矩阵，前向返回 `alpha * (x @ A @ B)`；B 初始化为 0，确保替换后初始输出不变。\n",
    "- `LinearWithLoRA`：包装原 `Linear` 层，输出为 `linear(x) + lora(x)`。\n",
    "\n",
    "并提供 `replace_linear_with_lora(model, rank, alpha)` 递归替换函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 开始 LoRA 模型替换...\n",
      "📊 替换前可训练参数: 124,441,346\n",
      "🔒 冻结后可训练参数: 0\n",
      "🎯 LoRA 可训练参数: 2,666,528\n",
      "✅ LoRA 替换与参数冻结设置完成！\n",
      "💡 参数效率提升：仅训练 2,666,528 个参数，相比全量微调大幅降低显存需求\n"
     ]
    }
   ],
   "source": [
    "# 🔧 LoRA 核心实现：低秩适配层与模型替换\n",
    "# \n",
    "# 本模块实现了 LoRA 的核心组件：\n",
    "# 1. LoRALayer：低秩分解层，实现 ΔW ≈ A × B 的近似\n",
    "# 2. LinearWithLoRA：包装原始线性层，叠加 LoRA 增量\n",
    "# 3. replace_linear_with_lora：递归替换模型中的所有线性层\n",
    "\n",
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA 低秩适配层\n",
    "    \n",
    "    核心原理：\n",
    "    - 用两个小矩阵 A、B 近似原始权重更新：ΔW ≈ A × B\n",
    "    - A ∈ ℝ^(d×r)，B ∈ ℝ^(r×k)，其中 r << min(d,k)\n",
    "    - 前向传播：y = α × (x @ A @ B)\n",
    "    \n",
    "    参数说明：\n",
    "        in_dim: 输入维度（与原始 Linear 层一致）\n",
    "        out_dim: 输出维度（与原始 Linear 层一致）\n",
    "        rank: 低秩分解的秩，控制参数量和表达能力\n",
    "        alpha: 缩放系数，平衡原始权重和 LoRA 增量的贡献\n",
    "    \n",
    "    初始化策略：\n",
    "        - A 矩阵：使用 Kaiming Uniform 初始化，保持梯度稳定性\n",
    "        - B 矩阵：初始化为 0，确保替换后初始输出不变\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, out_dim: int, rank: int, alpha: float):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播计算 LoRA 增量\n",
    "        \n",
    "        计算过程：\n",
    "        1. x @ A：将输入投影到低维空间\n",
    "        2. (x @ A) @ B：从低维空间投影到输出空间\n",
    "        3. α × (x @ A @ B)：应用缩放系数\n",
    "        \n",
    "        返回：\n",
    "            LoRA 增量输出，形状与原始线性层输出一致\n",
    "        \"\"\"\n",
    "        return self.alpha * (x @ self.A @ self.B)\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    带 LoRA 增量的线性层包装器\n",
    "    \n",
    "    功能说明：\n",
    "    - 包装原始 Linear 层，在不修改原权重的前提下添加 LoRA 增量\n",
    "    - 前向传播：output = original_linear(x) + lora_increment(x)\n",
    "    - 训练时只优化 LoRA 参数，原始参数保持冻结\n",
    "    \n",
    "    设计优势：\n",
    "    - 模块化：LoRA 增量可独立保存和加载\n",
    "    - 灵活性：支持动态启用/禁用 LoRA 分支\n",
    "    - 兼容性：与原始模型完全兼容\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, linear: torch.nn.Linear, rank: int, alpha: float):\n",
    "        \"\"\"\n",
    "        初始化包装器\n",
    "        \n",
    "        参数：\n",
    "            linear: 原始线性层\n",
    "            rank: LoRA 秩\n",
    "            alpha: 缩放系数\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        前向传播：原始输出 + LoRA 增量\n",
    "        \n",
    "        计算过程：\n",
    "        1. 原始线性变换：y_orig = x @ W\n",
    "        2. LoRA 增量：y_lora = α × (x @ A @ B)\n",
    "        3. 最终输出：y = y_orig + y_lora\n",
    "        \n",
    "        返回：\n",
    "            组合后的输出张量\n",
    "        \"\"\"\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "def replace_linear_with_lora(model: torch.nn.Module, rank: int, alpha: float):\n",
    "    \"\"\"\n",
    "    递归替换模型中的所有 Linear 层为 LinearWithLoRA\n",
    "    \n",
    "    功能说明：\n",
    "    - 深度遍历模型的所有子模块\n",
    "    - 将 torch.nn.Linear 替换为 LinearWithLoRA\n",
    "    - 保持模型结构不变，仅替换线性层\n",
    "    \n",
    "    参数：\n",
    "        model: 待替换的模型\n",
    "        rank: LoRA 秩\n",
    "        alpha: 缩放系数\n",
    "    \n",
    "    注意事项：\n",
    "    - 替换是就地进行的，会修改原始模型\n",
    "    - 建议在替换前备份模型状态\n",
    "    - 替换后需要重新设置优化器\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # 替换为 LoRA 包装的线性层\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # 递归处理子模块\n",
    "            replace_linear_with_lora(module, rank, alpha)\n",
    "\n",
    "\n",
    "# 🔄 执行 LoRA 替换流程\n",
    "print(\"🚀 开始 LoRA 模型替换...\")\n",
    "\n",
    "# 1. 统计替换前的可训练参数\n",
    "print(f\"📊 替换前可训练参数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# 2. 冻结所有原始参数\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(f\"🔒 冻结后可训练参数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# 3. 执行 LoRA 替换（关键超参数设置）\n",
    "RANK = 16      # 低秩分解的秩，控制参数量和表达能力\n",
    "ALPHA = 16     # 缩放系数，通常设置为 rank 的 1-2 倍\n",
    "replace_linear_with_lora(model, rank=RANK, alpha=ALPHA)\n",
    "\n",
    "# 4. 统计 LoRA 可训练参数\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"🎯 LoRA 可训练参数: {trainable_params:,}\")\n",
    "\n",
    "# 5. 启用 LoRA 参数训练\n",
    "for m in model.modules():\n",
    "    if isinstance(m, LoRALayer):\n",
    "        m.A.requires_grad = True\n",
    "        m.B.requires_grad = True\n",
    "\n",
    "# 🌟 在 LoRA 模块添加到模型后，再次将整个模型移动到目标设备 🌟\n",
    "model.to(device)\n",
    "\n",
    "print(\"✅ LoRA 替换与参数冻结设置完成！\")\n",
    "print(f\"💡 参数效率提升：仅训练 {trainable_params:,} 个参数，相比全量微调大幅降低显存需求\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "替换后、训练前的抽样准确率（各取 10 批）...\n",
      "Train: 46.25% | Val: 45.00% | Test: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# LoRA 替换后，验证基线应基本不变\n",
    "# 原理：B 初始化为 0，LoRA 增量初始为 0，因此替换后但未训练前，性能应与替换前一致。\n",
    "\n",
    "torch.manual_seed(123)\n",
    "print(\"替换后、训练前的抽样准确率（各取 10 批）...\")\n",
    "train_acc = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_acc = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_acc = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "print(f\"Train: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}% | Test: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 训练与评估\n",
    "\n",
    "- 仅优化 LoRA 分支（A、B），原模型权重保持冻结。\n",
    "- 学习率可从 5e-5 起步，epoch 取 3-5 做演示即可。\n",
    "- 训练中周期性评估训练/验证损失与准确率，确认收敛趋势。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 开始 LoRA 微调训练...\n",
      "📋 训练配置：\n",
      "   - 训练轮数: 5\n",
      "   - 学习率: 5e-05\n",
      "   - 权重衰减: 0.1\n",
      "   - 评估频率: 每 50 步\n",
      "   - 可训练参数: 2,666,528\n",
      "Ep 1 (Step 000000): Train loss 2.865, Val loss 2.584\n",
      "Ep 1 (Step 000050): Train loss 0.434, Val loss 0.390\n",
      "Ep 1 (Step 000100): Train loss 0.084, Val loss 0.242\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 2 (Step 000150): Train loss 0.238, Val loss 0.036\n",
      "Ep 2 (Step 000200): Train loss 0.092, Val loss 0.024\n",
      "Ep 2 (Step 000250): Train loss 0.129, Val loss 0.086\n",
      "Training accuracy: 95.00% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.289, Val loss 0.062\n",
      "Ep 3 (Step 000350): Train loss 0.107, Val loss 0.029\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "Ep 4 (Step 000400): Train loss 0.005, Val loss 0.024\n",
      "Ep 4 (Step 000450): Train loss 0.133, Val loss 0.173\n",
      "Ep 4 (Step 000500): Train loss 0.016, Val loss 0.017\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "Ep 5 (Step 000550): Train loss 0.001, Val loss 0.013\n",
      "Ep 5 (Step 000600): Train loss 0.001, Val loss 0.000\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "✅ 训练完成！\n",
      "⏱️ 训练耗时: 0.40 分钟\n",
      "📈 最终训练准确率: 100.00%\n",
      "📈 最终验证准确率: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# 🚀 LoRA 微调训练流程\n",
    "# \n",
    "# 本阶段执行 LoRA 参数的高效微调训练：\n",
    "# 1. 设置优化器，仅优化 LoRA 参数\n",
    "# 2. 执行训练循环，监控损失和准确率\n",
    "# 3. 记录训练时间和性能指标\n",
    "\n",
    "import time\n",
    "\n",
    "# 🔧 优化器配置\n",
    "# 仅优化需要梯度的参数（此时只包含 LoRA 的 A 和 B 矩阵）\n",
    "optimizer = torch.optim.AdamW(\n",
    "    (p for p in model.parameters() if p.requires_grad), \n",
    "    lr=5e-5,           # 学习率：LoRA 微调通常使用较小的学习率\n",
    "    weight_decay=0.1   # 权重衰减：防止过拟合\n",
    ")\n",
    "\n",
    "# 📊 训练配置\n",
    "num_epochs = 5        # 训练轮数：LoRA 微调通常收敛较快\n",
    "eval_freq = 50        # 评估频率：每 50 步评估一次\n",
    "eval_iter = 5         # 评估批次数：限制评估时间\n",
    "\n",
    "print(\"🎯 开始 LoRA 微调训练...\")\n",
    "print(f\"📋 训练配置：\")\n",
    "print(f\"   - 训练轮数: {num_epochs}\")\n",
    "print(f\"   - 学习率: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"   - 权重衰减: {optimizer.param_groups[0]['weight_decay']}\")\n",
    "print(f\"   - 评估频率: 每 {eval_freq} 步\")\n",
    "print(f\"   - 可训练参数: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# ⏱️ 记录训练开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 🔄 执行训练循环\n",
    "torch.manual_seed(123)  # 固定随机种子确保可复现\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=eval_freq, eval_iter=eval_iter,\n",
    ")\n",
    "\n",
    "# ⏱️ 计算训练耗时\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"✅ 训练完成！\")\n",
    "print(f\"⏱️ 训练耗时: {training_time:.2f} 分钟\")\n",
    "print(f\"📈 最终训练准确率: {train_accs[-1]*100:.2f}%\")\n",
    "print(f\"📈 最终验证准确率: {val_accs[-1]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制训练/验证损失曲线（可选）\n",
    "\n",
    "import torch\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 100.00% | Val: 99.33% | Test: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# 全量评估（全数据集）\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Train: {train_accuracy*100:.2f}% | Val: {val_accuracy*100:.2f}% | Test: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🎓 LoRA 微调最佳实践指南\n",
    "\n",
    "### 🎯 何时使用 LoRA\n",
    "\n",
    "**适用场景：**\n",
    "- **大模型微调**：参数量超过 1B 的模型，全量微调成本过高\n",
    "- **资源受限**：显存不足、训练时间有限的环境\n",
    "- **多任务适配**：需要为同一基座模型适配多个下游任务\n",
    "- **快速原型**：需要快速验证模型在特定任务上的表现\n",
    "\n",
    "**不适用场景：**\n",
    "- **小模型**：参数量 < 100M 的模型，全量微调可能更有效\n",
    "- **数据充足**：拥有大量高质量标注数据，全量微调效果更好\n",
    "- **计算资源充足**：有足够的显存和时间进行全量微调\n",
    "\n",
    "### ⚙️ 超参数调优策略\n",
    "\n",
    "#### Rank（秩）选择\n",
    "- **小任务**：rank = 4-8，适合简单分类任务\n",
    "- **中等任务**：rank = 16-32，适合大多数 NLP 任务\n",
    "- **复杂任务**：rank = 64-128，适合需要强表达能力的任务\n",
    "- **经验法则**：rank 越大，参数量越多，表达能力越强，但过拟合风险也越高\n",
    "\n",
    "#### Alpha（缩放系数）设置\n",
    "- **常用设置**：alpha = rank 或 alpha = 2×rank\n",
    "- **调优策略**：\n",
    "  - 从 alpha = rank 开始\n",
    "  - 如果 LoRA 影响过小，增加 alpha\n",
    "  - 如果训练不稳定，减少 alpha\n",
    "- **经验范围**：alpha ∈ [rank/2, 2×rank]\n",
    "\n",
    "#### 学习率配置\n",
    "- **推荐范围**：1e-5 到 5e-4\n",
    "- **LoRA 专用**：通常比全量微调使用更小的学习率\n",
    "- **自适应调整**：根据训练稳定性动态调整\n",
    "\n",
    "### 🔍 正确性验证\n",
    "\n",
    "#### 替换后验证\n",
    "```python\n",
    "# 验证 LoRA 替换后初始性能不变\n",
    "baseline_acc = evaluate_before_lora()\n",
    "lora_acc = evaluate_after_lora_replacement()\n",
    "assert abs(baseline_acc - lora_acc) < 0.01  # 误差应 < 1%\n",
    "```\n",
    "\n",
    "#### 训练过程监控\n",
    "- **损失曲线**：训练损失应稳定下降，验证损失不应过度上升\n",
    "- **准确率**：验证准确率应稳步提升，避免过拟合\n",
    "- **梯度检查**：确保只有 LoRA 参数有梯度更新\n",
    "\n",
    "### 🚀 性能优化技巧\n",
    "\n",
    "#### 显存优化\n",
    "- **梯度检查点**：在显存不足时启用\n",
    "- **混合精度训练**：使用 FP16 减少显存占用\n",
    "- **批次大小调整**：根据显存动态调整 batch_size\n",
    "\n",
    "#### 训练加速\n",
    "- **数据并行**：多 GPU 训练时使用\n",
    "- **模型并行**：超大模型的分片训练\n",
    "- **优化器选择**：AdamW 通常比 SGD 收敛更快\n",
    "\n",
    "### 📊 常见问题与解决方案\n",
    "\n",
    "#### 训练不收敛\n",
    "- **检查学习率**：可能过大或过小\n",
    "- **调整 rank**：增加表达能力\n",
    "- **数据质量**：检查标注数据的准确性\n",
    "\n",
    "#### 过拟合\n",
    "- **减少 rank**：降低模型复杂度\n",
    "- **增加正则化**：提高 weight_decay\n",
    "- **早停策略**：监控验证损失，及时停止\n",
    "\n",
    "#### 性能不佳\n",
    "- **增加 rank**：提升模型表达能力\n",
    "- **调整 alpha**：平衡原始权重和 LoRA 增量\n",
    "- **数据增强**：增加训练数据的多样性\n",
    "\n",
    "### 🔧 部署建议\n",
    "\n",
    "#### 模型保存\n",
    "- **分离保存**：基座模型和 LoRA 参数分别保存\n",
    "- **版本管理**：为不同任务保存不同的 LoRA 适配器\n",
    "- **压缩优化**：使用量化技术进一步减小文件大小\n",
    "\n",
    "#### 推理优化\n",
    "- **模型融合**：将 LoRA 参数合并到基座模型中\n",
    "- **批处理**：批量处理多个请求提高效率\n",
    "- **缓存策略**：缓存常用输入的处理结果\n",
    "\n",
    "### 💡 进阶技巧\n",
    "\n",
    "#### 多任务 LoRA\n",
    "- **任务特定适配器**：为不同任务训练独立的 LoRA\n",
    "- **任务切换**：运行时动态加载不同的 LoRA 适配器\n",
    "- **参数共享**：在相关任务间共享部分 LoRA 参数\n",
    "\n",
    "#### 持续学习\n",
    "- **增量训练**：在新数据上继续训练现有 LoRA\n",
    "- **灾难性遗忘**：使用正则化技术防止遗忘旧知识\n",
    "- **知识蒸馏**：从大模型向小模型传递知识\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 保存与加载 LoRA 参数（Adapter）\n",
    "\n",
    "- 保存：仅保存 LoRA 分支（A、B、alpha），体积小，便于分发与部署。\n",
    "- 加载：在同样完成 LoRA 替换后，将保存的参数拷回对应 `LoRALayer` 中即可生效。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 保存 LoRA 适配器...\n",
      "💾 LoRA 适配器已保存到: lora_adapter.pt\n",
      "📊 包含 73 个 LoRA 层\n",
      "📦 文件大小: 10.17 MB\n",
      "\n",
      "💡 使用说明：\n",
      "   - 保存的 LoRA 参数可以独立分发和部署\n",
      "   - 加载时需要先对模型执行相同的 LoRA 替换\n",
      "   - 支持在不同环境间迁移 LoRA 适配器\n"
     ]
    }
   ],
   "source": [
    "# 💾 LoRA 适配器保存与加载工具\n",
    "# \n",
    "# 本模块提供 LoRA 参数的保存和加载功能：\n",
    "# 1. 保存：仅保存 LoRA 增量参数，体积小，便于分发\n",
    "# 2. 加载：在相同模型结构上恢复 LoRA 参数\n",
    "# 3. 跨设备：支持在不同设备间传输 LoRA 参数\n",
    "\n",
    "import torch\n",
    "from typing import Dict, Tuple, Generator\n",
    "\n",
    "\n",
    "def iter_lora_named_modules(model: torch.nn.Module) -> Generator[Tuple[str, LoRALayer], None, None]:\n",
    "    \"\"\"\n",
    "    遍历模型中所有 LoRALayer 模块\n",
    "    \n",
    "    功能说明：\n",
    "    - 深度遍历模型的所有子模块\n",
    "    - 筛选出 LoRALayer 类型的模块\n",
    "    - 返回模块的完整路径名和模块对象\n",
    "    \n",
    "    参数：\n",
    "        model: 待遍历的模型\n",
    "        \n",
    "    返回：\n",
    "        Generator: (模块全名, LoRALayer对象) 的生成器\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoRALayer):\n",
    "            yield name, module\n",
    "\n",
    "\n",
    "def save_lora_adapter(model: torch.nn.Module, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    保存 LoRA 适配器参数\n",
    "    \n",
    "    功能说明：\n",
    "    - 仅保存 LoRA 的 A、B 矩阵和 alpha 参数\n",
    "    - 使用模块层级名作为键，避免加载时错位\n",
    "    - 自动将参数移至 CPU，节省存储空间\n",
    "    \n",
    "    参数：\n",
    "        model: 包含 LoRA 层的模型\n",
    "        filepath: 保存路径\n",
    "        \n",
    "    保存内容：\n",
    "        - {module_name}.A: LoRA A 矩阵\n",
    "        - {module_name}.B: LoRA B 矩阵  \n",
    "        - {module_name}.alpha: 缩放系数\n",
    "    \"\"\"\n",
    "    state: Dict[str, torch.Tensor] = {}\n",
    "    \n",
    "    # 遍历所有 LoRA 层，收集参数\n",
    "    for name, lora_module in iter_lora_named_modules(model):\n",
    "        # 保存 A 矩阵（移至 CPU）\n",
    "        state[f\"{name}.A\"] = lora_module.A.detach().cpu()\n",
    "        # 保存 B 矩阵（移至 CPU）\n",
    "        state[f\"{name}.B\"] = lora_module.B.detach().cpu()\n",
    "        # 保存 alpha 参数（转换为张量便于跨设备）\n",
    "        state[f\"{name}.alpha\"] = torch.tensor(lora_module.alpha)\n",
    "    \n",
    "    # 保存到文件\n",
    "    torch.save(state, filepath)\n",
    "    \n",
    "    # 统计信息\n",
    "    lora_layers = len([k for k in state if k.endswith('.A')])\n",
    "    file_size = sum(tensor.numel() * tensor.element_size() for tensor in state.values())\n",
    "    \n",
    "    print(f\"💾 LoRA 适配器已保存到: {filepath}\")\n",
    "    print(f\"📊 包含 {lora_layers} 个 LoRA 层\")\n",
    "    print(f\"📦 文件大小: {file_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\n",
    "def load_lora_adapter(model: torch.nn.Module, filepath: str, map_location: str = \"cpu\") -> None:\n",
    "    \"\"\"\n",
    "    加载 LoRA 适配器参数\n",
    "    \n",
    "    功能说明：\n",
    "    - 从文件加载 LoRA 参数到对应模块\n",
    "    - 自动处理设备转换（CPU ↔ GPU）\n",
    "    - 提供详细的加载状态反馈\n",
    "    \n",
    "    参数：\n",
    "        model: 目标模型（必须包含相同结构的 LoRA 层）\n",
    "        filepath: LoRA 参数文件路径\n",
    "        map_location: 加载时的设备映射（\"cpu\" 或 \"cuda\"）\n",
    "        \n",
    "    注意事项：\n",
    "    - 模型结构必须与保存时一致\n",
    "    - 加载前需要先执行 LoRA 替换\n",
    "    - 支持跨设备加载（自动转换到目标设备）\n",
    "    \"\"\"\n",
    "    # 加载状态字典\n",
    "    state = torch.load(filepath, map_location=map_location)\n",
    "    \n",
    "    restored, missing = 0, []\n",
    "    \n",
    "    # 遍历模型中的 LoRA 层，恢复参数\n",
    "    for name, lora_module in iter_lora_named_modules(model):\n",
    "        keyA, keyB, keyAlpha = f\"{name}.A\", f\"{name}.B\", f\"{name}.alpha\"\n",
    "        \n",
    "        # 检查所有必需的参数是否存在\n",
    "        if keyA in state and keyB in state and keyAlpha in state:\n",
    "            with torch.no_grad():\n",
    "                # 恢复 A 矩阵（自动转换到目标设备）\n",
    "                lora_module.A.copy_(state[keyA].to(lora_module.A.device))\n",
    "                # 恢复 B 矩阵（自动转换到目标设备）\n",
    "                lora_module.B.copy_(state[keyB].to(lora_module.B.device))\n",
    "                # 恢复 alpha 参数\n",
    "                lora_module.alpha = float(state[keyAlpha].item())\n",
    "            restored += 1\n",
    "        else:\n",
    "            missing.append(name)\n",
    "    \n",
    "    # 输出加载结果\n",
    "    print(f\"🔄 LoRA 适配器加载完成\")\n",
    "    print(f\"✅ 成功恢复 {restored} 个 LoRA 层\")\n",
    "    if missing:\n",
    "        print(f\"⚠️  缺失 {len(missing)} 个 LoRA 层: {missing[:3]}{'...' if len(missing)>3 else ''}\")\n",
    "\n",
    "\n",
    "# 🎯 示例：保存当前微调得到的 LoRA 参数\n",
    "print(\"💾 保存 LoRA 适配器...\")\n",
    "save_lora_adapter(model, \"lora_adapter.pt\")\n",
    "\n",
    "print(\"\\n💡 使用说明：\")\n",
    "print(\"   - 保存的 LoRA 参数可以独立分发和部署\")\n",
    "print(\"   - 加载时需要先对模型执行相同的 LoRA 替换\")\n",
    "print(\"   - 支持在不同环境间迁移 LoRA 适配器\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 推理示例（微调后）\n",
    "\n",
    "- 以测试集样本作为演示，打印前若干条预测与真实标签。\n",
    "- 注意：此处仅用于直观感受微调效果，完整指标请参考上方评估单元。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本0: 预测=1, 真实=1\n",
      "样本1: 预测=1, 真实=1\n",
      "样本2: 预测=1, 真实=1\n",
      "样本3: 预测=0, 真实=0\n",
      "样本4: 预测=1, 真实=1\n"
     ]
    }
   ],
   "source": [
    "# 推理示例\n",
    "# 说明：\n",
    "# - 这里示范如何从 DataLoader 中取一个 batch，打印预测结果与真实标签对比。\n",
    "# - 具体前向逻辑由 `train_classifier_simple`/`calc_accuracy_loader` 内部定义的模型调用保持一致。\n",
    "\n",
    "import torch\n",
    "\n",
    "# def predict_one_batch(data_loader, max_batches: int = 1):\n",
    "#     model.eval()\n",
    "#     shown = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in data_loader:\n",
    "#             # 兼容 previous_chapters 内部的批数据结构\n",
    "#             # 典型为 (input_ids, labels) 或字典；如结构不同，请参考库实现做相应适配\n",
    "#             inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "#             logits = model(inputs)\n",
    "#             preds = torch.argmax(logits, dim=-1)\n",
    "#             for i in range(min(5, inputs.shape[0])):\n",
    "#                 print(f\"样本{i}: 预测={int(preds[i].item())}, 真实={int(labels[i].item())}\")\n",
    "#             shown += 1\n",
    "#             if shown >= max_batches:\n",
    "#                 break\n",
    "\n",
    "def predict_one_batch(data_loader, max_batches: int = 1):\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            \n",
    "            # 🌟 切片取最后一个 token 的 logits 🌟\n",
    "            full_logits = model(inputs)\n",
    "            # Logits 形状从 [B, T, C] 变为 [B, C]\n",
    "            logits = full_logits[:, -1, :] \n",
    "            \n",
    "            # 🌟 argmax 现在沿着正确的类别维度 -1 运行 🌟\n",
    "            # preds 形状将是 [B]，其中 B 是批次大小（例如 8）\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # 标签 (labels) 已经是 [B] 形状，不需要修改\n",
    "            \n",
    "            for i in range(min(5, inputs.shape[0])):\n",
    "                # 现在 preds[i] 是一个形状为 [] 的标量张量，可以安全地使用 .item()\n",
    "                print(f\"样本{i}: 预测={int(preds[i].item())}, 真实={int(labels[i].item())}\")\n",
    "            shown += 1\n",
    "            if shown >= max_batches:\n",
    "                break\n",
    "\n",
    "predict_one_batch(test_loader, max_batches=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 🎉 LoRA 微调教程总结\n",
    "\n",
    "### 🏆 核心收获\n",
    "\n",
    "通过本教程，您已经掌握了：\n",
    "\n",
    "1. **LoRA 原理理解**：深入理解低秩分解如何实现参数高效微调\n",
    "2. **完整实现流程**：从数据准备到模型部署的端到端工作流\n",
    "3. **关键超参调优**：掌握 rank、alpha 等参数的选择和调优策略\n",
    "4. **实践技能提升**：具备独立进行 LoRA 微调的能力\n",
    "\n",
    "### 📊 技术优势总结\n",
    "\n",
    "#### 参数效率\n",
    "- **显著降低**：可训练参数从 O(dk) 降低到 O(r(d+k))\n",
    "- **显存友好**：大幅减少训练时的显存需求\n",
    "- **训练加速**：减少参数更新，提升训练速度\n",
    "\n",
    "#### 部署灵活性\n",
    "- **模块化设计**：LoRA 增量可独立保存和加载\n",
    "- **多任务支持**：同一基座模型可适配多个下游任务\n",
    "- **版本管理**：便于管理不同任务的适配器\n",
    "\n",
    "#### 性能保证\n",
    "- **初始一致性**：B 矩阵初始化为 0，确保替换后输出不变\n",
    "- **收敛稳定**：LoRA 微调通常比全量微调更稳定\n",
    "- **效果相当**：在大多数任务上达到与全量微调相近的效果\n",
    "\n",
    "### 🚀 下一步学习建议\n",
    "\n",
    "#### 进阶技术\n",
    "- **QLoRA**：结合量化的 LoRA，进一步降低显存需求\n",
    "- **AdaLoRA**：自适应调整不同层的 rank\n",
    "- **LoRA+**：改进的 LoRA 变体，提升表达能力\n",
    "\n",
    "#### 工程实践\n",
    "- **HuggingFace 集成**：使用 `peft` 库进行 LoRA 微调\n",
    "- **分布式训练**：多 GPU/多节点的大规模训练\n",
    "- **生产部署**：模型服务化和性能优化\n",
    "\n",
    "#### 应用拓展\n",
    "- **多模态 LoRA**：在视觉-语言模型中的应用\n",
    "- **领域适配**：特定领域的 LoRA 微调\n",
    "- **持续学习**：增量学习和知识保持\n",
    "\n",
    "### 💡 关键要点回顾\n",
    "\n",
    "1. **选择合适场景**：大模型、资源受限、多任务适配时优先考虑 LoRA\n",
    "2. **超参调优**：从 rank=16, alpha=16 开始，根据任务复杂度调整\n",
    "3. **正确性验证**：确保 LoRA 替换后初始性能不变\n",
    "4. **性能监控**：密切关注训练过程中的损失和准确率变化\n",
    "5. **部署优化**：合理保存和加载 LoRA 适配器\n",
    "\n",
    "### 🔗 相关资源\n",
    "\n",
    "- **论文原文**：[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "- **HuggingFace PEFT**：[Parameter-Efficient Fine-Tuning](https://huggingface.co/docs/peft/)\n",
    "- **实践案例**：更多 LoRA 微调的实际应用案例\n",
    "- **社区支持**：加入相关技术社区，获取最新进展\n",
    "\n",
    "---\n",
    "\n",
    "**🎯 恭喜您完成了 LoRA 微调的完整学习！** 现在您已经具备了使用 LoRA 进行大模型微调的能力，可以开始在实际项目中应用这些技术了。记住，实践是最好的学习方式，建议您尝试在不同的数据集和任务上应用 LoRA 微调技术。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
