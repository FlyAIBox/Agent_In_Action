## 04-vLLM推理服务压测框架：让大模型性能评估有据可依

在部署大模型的过程中，性能评估是不可或缺的一环。传统的性能测试方法往往存在指标单一、场景受限等问题，难以全面、准确地反映大模型的真实性能。本文将介绍一个专门为vLLM推理服务量身定制的性能压测框架，该框架能够提供一整套科学、优雅的大模型性能评估解决方案。

### 痛点：大模型性能评估的“黑盒困境”

在没有专业工具的情况下，大模型的性能评估就像是“黑盒操作”：

- **指标单一**：仅能测试QPS等基本指标，缺乏对延迟等更细粒度指标的分析。
- **场景受限**：无法模拟输入输出长度、并发数等多种组合场景。
- **结果分散**：测试结果缺乏统一管理，难以进行历史数据对比。
- **可视化不足**：数据被埋藏在日志中，无法直观地理解和分析性能表现。

为了解决这些痛点，我们基于vLLM官方benchmarks工具，开发了一套完整的推理服务压测框架。

------

### 解药：专业化的vLLM压测框架

这个框架不仅仅是一个简单的测试工具，它更像是一个为大模型量身定制的**“全身体检设备”**，具备以下核心功能：

#### 六大关键指标全覆盖

该框架能够全面捕捉并分析大模型推理过程中的六大关键性能指标，帮助你洞察模型的每一个性能细节：

- **延迟 (Latency)**：衡量从发送请求到接收完整响应的整体时间。
- **吞吐量 (Throughput)**：反映系统每秒能处理的请求数和token数。
- **首token时间 (TTFT)**：用户感知到的“反应速度”，即从发送请求到接收第一个token的时间。
- **token间延迟 (ITL)**：衡量token与token之间生成的速度和流畅度。
- **每token输出时间 (TPOT)**：反映模型生成每个token的平均效率。
- **端到端延迟 (E2EL)**：完整请求的总体时间开销。

#### 智能化测试管理与可视化

- **批量压测**：通过简单的YAML配置文件，可轻松实现多种参数组合的自动化测试。
- **结果聚合**：自动将分散的测试结果按时间和模型组织，便于历史对比和管理。
- **可视化报告**：一键生成专业的性能分析图表，如吞吐量对比图、延迟热力图等，让数据一目了然。

------

### 架构设计：简洁而不简单

该框架采用分层模块化设计，确保了系统的可扩展性和可维护性。

```bash
vllm_benchmark_serving/
├── main.py                    # 统一入口，集成所有功能
├── config.yaml               # 配置驱动，灵活管理参数
├── src/                      # 核心源码
│   ├── core/                 # 压测引擎
│   ├── backends/             # 后端适配层
│   └── visualize/            # 可视化模块
└── results/                  # 智能结果管理，按模型+时间戳组织
    └── chart/                 # 可视化报告存储
```

**设计亮点**：

- **统一入口**：所有操作通过 `main.py` 即可完成，包括批量测试、结果聚合和可视化。
- **配置文件驱动**：将测试参数（如模型、并发数、输入输出长度）集中在YAML文件中管理，降低使用门槛。
- **智能目录管理**：测试结果自动按**“模型名_时间戳”**组织，避免文件混乱，便于追溯和对比。

------

### 实战演练：三步完成专业压测

以 **DeepSeek-R1-AWQ** 模型为例，展示如何使用该工具完成专业的性能压测。

#### 步骤1：环境准备

首先，安装所需的依赖并启动vLLM推理服务。

```Bash
# 安装依赖
pip install -r requirements.txt
```

#### 步骤2：配置测试参数

编辑 `config.yaml` 文件，定义测试场景。通过配置不同的输入输出长度和并发数，模拟真实世界的多种应用场景。

```YAML
model: "/data/model/cognitivecomputations/DeepSeek-R1-awq"
base_url: "http://localhost:8010"
tokenizer: "/data/model/cognitivecomputations/DeepSeek-R1-awq"

# 测试不同输入输出长度组合
input_output:
- [512, 512]      # 中等长度对话
- [1024, 1024]    # 长文本处理

# 测试不同并发场景
concurrency_prompts:
- [1, 10]         # 串行基准测试
- [4, 40]         # 中等并发压力
```

#### 步骤3：一键执行压测与分析

```bash
# python main.py
usage: main.py [-h] {batch,single,aggregate,visualize} ...

vLLM推理服务压测工具 - 集成批量压测、单次压测和结果聚合功能

positional arguments:
  {batch,single,aggregate,visualize}
                        可用命令
    batch               批量压测（根据config.yaml配置）
    single              单次压测
    aggregate           聚合压测结果
    visualize           生成可视化报告

options:
  -h, --help            show this help message and exit

        使用示例:
        # 批量压测（根据config.yaml配置）
        python main.py batch
        python main.py batch --config custom_config.yaml

        # 单次压测
        python main.py single --model deepseek-ai/DeepSeek-R1 --base-url http://localhost:8010 --num-prompts 100
        python main.py single --model /path/to/model --base-url http://localhost:8010 --max-concurrency 16 --random-input-len 512 --random-output-len 512

        # 聚合结果
        python main.py aggregate                                    # 聚合最新的结果目录
        python main.py aggregate --list                            # 列出所有可用的结果目录
        python main.py aggregate --dir DeepSeek-R1_20250728_145302 # 聚合指定的结果目录

        # 生成可视化报告
        python main.py visualize                                    # 自动查找最新CSV文件，生成完整版报告
        python main.py visualize --csv results/aggregate_results_20250728.csv  # 指定CSV文件
        python main.py visualize --mode simple --output simple_charts          # 生成简化版报告
        python main.py visualize --mode both --output all_charts               # 生成两种模式的报告

        功能说明:
        batch     - 根据config.yaml配置文件执行批量压测，结果按模型名称和时间组织到子目录
        single    - 执行单次压测，结果按模型名称和时间组织到子目录
        aggregate - 聚合指定目录下的JSON结果文件，生成双语CSV报告
                   支持 --list 查看可用目录，--dir 指定目录（默认使用最新的）
        visualize - 生成可视化性能报告，支持simple(基础图表)、advanced(完整报告)、both(两种模式)
                   支持自动查找最新CSV文件，或手动指定CSV文件路径

```

执行以下命令，即可完成批量压测、结果聚合和可视化报告生成。

```Bash
# 执行批量压测
python main.py batch

# 聚合测试结果
python main.py aggregate

# 生成可视化报告
python main.py visualize
```

框架会自动执行所有测试用例，将结果保存并聚合，最终生成包含吞吐量、延迟、性能热力图等在内的专业报告。

------

## 模型推理压测指标

| 指标名称                    | 英文缩写/原始名称                       | 解释                                                         | 实际意义                                                     |
| --------------------------- | --------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **请求吞吐量**              | `REQ_THROUGHPUT` / `Request throughput` | 单位时间内系统处理的请求数量（通常是每秒请求数）。           | 反映模型服务处理并发请求的能力，数值越高表示服务器能处理更多用户请求。 |
| **端到端时间**              | `E2E_TIME` / `benchmark duration`       | 整个基准测试运行的总时间，从开始到结束。                     | 衡量完成所有请求和生成所有输出所需的总时长，时间越短表示整体执行效率越高。 |
| **生成吞吐量**              | `GEN_THROUGHPUT` / `Output throughput`  | 单位时间内模型生成的 token 数量（通常是每秒 token 数），也称**解码吞吐量**或**生成速度**。 | 衡量模型实际生成文本的速度，数值越高意味着用户可以更快地获得完整的响应。 |
| **总吞吐量**                | `TOTAL_THROUGHPUT` / `Total Token`      | 每秒处理的总 token 数量，包括输入 prompt tokens 和生成的 completion tokens。 | 衡量模型整体处理能力的重要指标，综合反映了输入处理和输出生成的效率。 |
| **平均首次 token 时间**     | `TTFT` / `Mean TTFT`                    | 从发送请求到接收到第一个 token 所需的平均时间（毫秒）。      | 对用户体验影响很大，决定用户看到第一个响应所需的时间，时间越短用户感觉响应越快。 |
| **平均每 token 时间**       | `TPOT` / `Mean TPOT`                    | 生成后续每个 token 所需的平均时间（毫秒），也称**解码步长**或**迭代时间**。 | 决定整个响应生成的速度，时间越短，生成整个长文本所需的时间就越少。 |
| **平均迭代延迟**            | `ITL` / `Mean ITL`                      | 模型生成每个 token (或每个解码步骤) 的内部处理延迟。         | 反映模型内部计算效率，更低的 `ITL` 意味着模型在 GPU 上的计算过程更有效率。 |
| **99百分位迭代延迟**        | `P99_ITL` / `P99 ITL`                   | 99% 的迭代延迟都小于或等于这个值。                           | 用于评估系统在最坏情况下的表现，识别和优化少数特别慢的迭代。 |
| **99百分位首次 token 时间** | `P99_TTFT` / `P99 TTFT`                 | 99% 的请求的首次 token 时间都小于或等于这个值。              | 确保绝大多数用户都能获得快速的首次响应，即使在高负载或有少量异常情况时。 |
| **99百分位每 token 时间**   | `P99_TPOT` / `P99 TPOT`                 | 99% 的生成 token 的每 token 时间都小于或等于这个值。         | 衡量模型在绝大多数情况下持续生成 token 的稳定性，有助于发现生成过程中的偶尔卡顿或延迟。 |

------

其中 P99 ITL、P99 TTFT 和 P99 TPOT 这三个指标：

| 指标         | 全称                      | 核心含义                             | 衡量什么？                                 | 对用户体验的影响                         | 关注点                                   |
| ------------ | ------------------------- | ------------------------------------ | ------------------------------------------ | ---------------------------------------- | ---------------------------------------- |
| **P99 ITL**  | P99 Inter-Token Latency   | 第 99 百分位数的**令牌间延迟**       | 模型生成**连续两个输出令牌之间**的时间间隔 | 影响输出的**流畅性和连贯性**，避免卡顿。 | **流式传输**性能，避免单次卡顿。         |
| **P99 TTFT** | P99 Time to First Token   | 第 99 百分位数的**首令牌生成时间**   | 从请求到生成**第一个输出令牌**所需的时间   | 影响用户的**第一印象和响应速度感知**。   | **首次响应速度**，即“模型多快能开始说？” |
| **P99 TPOT** | P99 Time per Output Token | 第 99 百分位数的**每个输出令牌时间** | 生成**每个输出令牌的平均时间**             | 影响**完成整个输出的等待时间**。         | **持续生成效率**，即“模型说得有多快？”   |

------

希望这个表格能让你更清晰地理解这些指标！

### 大模型推理压测结果数据解读

| 字段名              | 代码中的实际变量名  | 英文全称              | 解释                                                         | 实际意义                                                     |
| ------------------- | ------------------- | --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **张量并行度**      | `tp`                | Tensor Parallelism    | 在模型推理过程中使用的**张量并行 (Tensor Parallelism)** 的 GPU 数量。 | 反映了本次测试中模型利用的计算资源量。更高的值通常意味着能处理更大模型或达到更高吞吐量。 |
| **数据类型**        | `data_type`         | Data Type             | 模型推理时使用的数据精度，如 `float16`、`bfloat16` 或 `int8`。 | 影响模型推理的**性能**和**显存占用**。低精度数据类型（如 `float16`）通常能提升速度并减少资源消耗。 |
| **批处理大小**      | `batch`             | Batch Size            | 一次性送入模型进行处理的请求数量。                           | 影响**吞吐量**和**延迟**的关键参数。增大批处理大小通常能提高 GPU 利用率和总吞吐量，但会增加平均延迟。 |
| **输入 token 数量** | `prompt_tokens`     | Prompt Tokens         | 每个请求中，用户输入的提示 (prompt) 所包含的 token 数量。    | 影响**模型预填充 (prefill) 阶段的计算量**。长的 prompt 会增加生成第一个 token 之前的计算负担。 |
| **输出 token 数量** | `completion_tokens` | Completion Tokens     | 模型为每个请求生成的响应 (completion) 所包含的 token 数量。  | **直接决定模型生成阶段的计算量和持续时间**。长的输出 token 对模型的生成吞吐量和每 token 时间影响更大。 |
| **总吞吐量**        | `TOTAL_THROUGHPUT`  | Total Throughput      | 单位时间内模型处理的总 token 数量（包括输入和输出），通常以“token/秒”为单位。 | **衡量模型整体处理能力的核心指标**。它综合反映了模型在给定时间内的输入处理和输出生成效率。 |
| **生成吞吐量**      | `GEN_THROUGHPUT`    | Generation Throughput | 单位时间内模型生成的 token 数量，也称**解码吞吐量**或**生成速度**，通常以“token/秒”为单位。 | 衡量模型**实际生成文本的速度**。数值越高，意味着用户可以更快地获得完整的响应。 |
| **首次 token 时间** | `TTFT`              | Time To First Token   | 从发送请求到接收到第一个 token 所需的**平均时间**（毫秒）。  | **对用户体验影响很大**，因为它决定了用户感知到的模型响应速度。在交互式应用中，TTFT 越低，用户会感觉响应越“实时”。 |
| **每 token 时间**   | `TPOT`              | Time Per Output Token | 生成后续每个 token 所需的**平均时间**（毫秒），也称**解码步长**或**迭代时间**。 | **决定整个响应生成的速度**。TPOT 越低，生成整个长文本所需的时间就越少。它主要反映了模型在连续生成阶段的效率。 |
| **迭代延迟**        | `ITL`               | Inter-Token Latency   | 模型生成每个 token（或每个解码步骤）的**内部处理延迟**（毫秒），不包括网络传输等外部开销。 | 反映模型**内部计算的效率**。更低的 ITL 意味着模型在 GPU 上的计算过程更有效率，有助于提高整体生成速度。 |

### 真实测试结果分析

通过在 **DeepSeek-R1-AWQ** 模型上的实际测试，我们观察到了以下关键性能趋势：

1. **输入长度影响**：随着输入长度的增加，首token时间（TTFT）显著增加，这与模型处理长文本的计算量增加有关。
2. **并发效应**：适当增加并发数可以有效提升模型的整体吞吐量，但同时也会导致单次请求的延迟略有增加。
3. **生成效率**：在不同场景下，每token输出时间（TPOT）表现相对稳定，表明模型的生成效率相对一致。

项目开源地址：https://github.com/FlyAIBox/llm_benchmark.git

