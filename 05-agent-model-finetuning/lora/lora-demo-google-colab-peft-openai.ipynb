{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PEFT LoRA (Google Colab) ??\n",
        "?? Hugging Face PEFT ???? LoRA ??????? torch/transformers ????????? SMS Spam ???????????????????\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# ????????????????????\n",
        "%pip install -q --upgrade torch==2.8.0 --index-url https://download.pytorch.org/whl/cu121             transformers==4.51.3 peft==0.13.2 accelerate==1.0.1             pandas==2.2.2 numpy==2.0.2 tqdm==4.67.1 matplotlib==3.10.7             requests==2.32.5 safetensors==0.6.2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import io\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    get_linear_schedule_with_warmup,\n",
        ")\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from tqdm import tqdm\n",
        "import requests\n",
        "\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ????????? + ?? + ?????\n",
        "- ????????????? UCI ?? SMS Spam Collection?\n",
        "- ????????? 7:1:2 ????????\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def create_balanced_dataset(df):\n",
        "    label_col = df[\"Label\"]\n",
        "    spam_mask = (label_col == \"spam\") | (label_col == 1)\n",
        "    ham_mask = (label_col == \"ham\") | (label_col == 0)\n",
        "    spam_count = int(spam_mask.sum())\n",
        "    if spam_count == 0 or ham_mask.sum() == 0:\n",
        "        raise ValueError(\"?????? spam ? ham ??????????\")\n",
        "    ham_subset = df[ham_mask].sample(spam_count, random_state=123)\n",
        "    balanced = pd.concat([ham_subset, df[spam_mask]])\n",
        "    return balanced.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "def random_split(df, train_frac=0.7, val_frac=0.1):\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    val_end = train_end + int(len(df) * val_frac)\n",
        "    train_df = df[:train_end]\n",
        "    val_df = df[train_end:val_end]\n",
        "    test_df = df[val_end:]\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def ensure_sms_file(local_path=\"sms_spam_collection/SMSSpamCollection.tsv\"):\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "    resp = requests.get(url, timeout=30)\n",
        "    resp.raise_for_status()\n",
        "    with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
        "        raw = zf.read(\"SMSSpamCollection\").decode(\"utf-8\")\n",
        "    out_path = \"SMSSpamCollection.tsv\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(raw)\n",
        "    return out_path\n",
        "\n",
        "def load_sms_dataframe():\n",
        "    data_path = ensure_sms_file()\n",
        "    df = pd.read_csv(data_path, sep=\"\t\", names=[\"Label\", \"Text\"], header=None)\n",
        "    balanced = create_balanced_dataset(df)\n",
        "    label_map = {\"ham\": 0, \"spam\": 1}\n",
        "    balanced[\"Label\"] = balanced[\"Label\"].apply(lambda v: label_map[v] if v in label_map else int(v))\n",
        "    return random_split(balanced, train_frac=0.7, val_frac=0.1)\n",
        "\n",
        "class SpamSequenceDataset(Dataset):\n",
        "    '''?? Hugging Face ???????????'''\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_length=96):\n",
        "        encodings = tokenizer(\n",
        "            df[\"Text\"].tolist(),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        self.input_ids = encodings[\"input_ids\"]\n",
        "        self.attention_mask = encodings[\"attention_mask\"]\n",
        "        self.labels = torch.tensor(df[\"Label\"].tolist(), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],\n",
        "            \"attention_mask\": self.attention_mask[idx],\n",
        "            \"labels\": self.labels[idx],\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "train_df, val_df, test_df = load_sms_dataframe()\n",
        "max_length = 96\n",
        "\n",
        "train_dataset = SpamSequenceDataset(train_df, tokenizer, max_length=max_length)\n",
        "val_dataset = SpamSequenceDataset(val_df, tokenizer, max_length=max_length)\n",
        "test_dataset = SpamSequenceDataset(test_df, tokenizer, max_length=max_length)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Train/Val/Test sizes: {len(train_dataset)}/{len(val_dataset)}/{len(test_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ?? Hugging Face PEFT ?? LoRA\n",
        "- ??? AutoModelForSequenceClassification ??? LoRA?\n",
        "- ? LoRA ???????????????\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\"c_attn\", \"c_fc\", \"c_proj\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ????????? LoRA ???\n",
        "???? warmup??????????????\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    total_loss, correct, total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            preds = outputs.logits.argmax(dim=-1)\n",
        "            total_loss += loss.item()\n",
        "            correct += (preds == batch[\"labels\"]).sum().item()\n",
        "            total += batch[\"labels\"].size(0)\n",
        "    avg_loss = total_loss / max(len(data_loader), 1)\n",
        "    acc = correct / total if total else 0.0\n",
        "    return acc, avg_loss\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=3, lr=5e-4):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=max(10, int(0.1 * total_steps)),\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        val_acc, val_loss = evaluate(model, val_loader)\n",
        "        print(\n",
        "            f\"Epoch {epoch+1}: train_loss={running_loss/len(train_loader):.4f} \"\n",
        "            f\"| val_loss={val_loss:.4f} | val_acc={val_acc*100:.1f}%\"\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "train(model, train_loader, val_loader, epochs=3, lr=5e-4)\n",
        "\n",
        "test_acc, test_loss = evaluate(model, test_loader)\n",
        "print(f\"Test: loss={test_loss:.4f}, acc={test_acc*100:.1f}%\")\n",
        "\n",
        "sample_texts = [\n",
        "    \"Hey, want to grab lunch together?\",\n",
        "    \"URGENT! You've won ! Click now to claim your prize!\",\n",
        "    \"The meeting has been moved to 2pm\",\n",
        "    \"Free iPhone! Limited time offer! Call immediately!\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "for text in sample_texts:\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "    with torch.no_grad():\n",
        "        logits = model(**encoded).logits\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "    pred = torch.argmax(probs).item()\n",
        "    label = \"spam\" if pred == 1 else \"ham\"\n",
        "    print(f\"{label.upper()} | p(spam)={probs[1]:.3f} | text={text[:50]}...\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}