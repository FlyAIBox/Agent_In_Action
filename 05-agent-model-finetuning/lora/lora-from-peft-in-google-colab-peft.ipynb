{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlyAIBox/Agent_In_Action/blob/main/05-agent-model-finetuning/lora/lora-from-peft-in-google-colab-peft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5wPUmY-5kgH"
      },
      "source": [
        "# ğŸš€ ä½¿ç”¨ Hugging Face PEFT åº“å®ç° LoRA å¾®è°ƒ - å®Œæ•´æ•™ç¨‹\n",
        "\n",
        "## ğŸ“– æ•™ç¨‹æ¦‚è¿°\n",
        "\n",
        "æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ **Hugging Face PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰** åº“å®ç° LoRA å¾®è°ƒï¼Œè¿™æ˜¯å·¥ä¸šç•Œæ¨èçš„æ ‡å‡†æ–¹æ³•ã€‚\n",
        "\n",
        "### ğŸ¯ ä¸ºä»€ä¹ˆä½¿ç”¨ PEFT åº“ï¼Ÿ\n",
        "\n",
        "#### âœ… PEFT åº“çš„ä¼˜åŠ¿\n",
        "\n",
        "1. **å·¥ä¸šçº§å®ç°**ï¼šç»è¿‡å¤§è§„æ¨¡éªŒè¯ï¼Œç¨³å®šå¯é \n",
        "2. **æ˜“äºä½¿ç”¨**ï¼šå‡ è¡Œä»£ç å³å¯åº”ç”¨ LoRA\n",
        "3. **åŠŸèƒ½ä¸°å¯Œ**ï¼šæ”¯æŒ LoRAã€LoHaã€LoKr ç­‰å¤šç§æ–¹æ³•\n",
        "4. **ç”Ÿæ€é›†æˆ**ï¼šä¸ Transformersã€Accelerate æ— ç¼é›†æˆ\n",
        "5. **æŒç»­ç»´æŠ¤**ï¼šHugging Face å®˜æ–¹æ”¯æŒ\n",
        "\n",
        "#### ğŸ“Š ä¸æ‰‹åŠ¨å®ç°çš„å¯¹æ¯”\n",
        "\n",
        "| ç‰¹æ€§ | æ‰‹åŠ¨å®ç° | PEFT åº“ |\n",
        "|------|---------|---------|\n",
        "| **ä»£ç é‡** | å¤šï¼ˆéœ€å®ç° LoRA å±‚ï¼‰ | å°‘ï¼ˆå‡ è¡Œé…ç½®ï¼‰ |\n",
        "| **å­¦ä¹ ä»·å€¼** | æ·±å…¥ç†è§£åŸç† | å¿«é€Ÿåº”ç”¨å®è·µ |\n",
        "| **ç»´æŠ¤æˆæœ¬** | é«˜ï¼ˆéœ€è‡ªå·±ç»´æŠ¤ï¼‰ | ä½ï¼ˆå®˜æ–¹ç»´æŠ¤ï¼‰ |\n",
        "| **åŠŸèƒ½å®Œæ•´æ€§** | åŸºç¡€åŠŸèƒ½ | ä¸°å¯ŒåŠŸèƒ½ |\n",
        "| **é”™è¯¯é£é™©** | è¾ƒé«˜ | è¾ƒä½ |\n",
        "| **é€‚ç”¨åœºæ™¯** | å­¦ä¹ ç ”ç©¶ | ç”Ÿäº§ç¯å¢ƒ |\n",
        "\n",
        "### ğŸ“ å­¦ä¹ è·¯å¾„å»ºè®®\n",
        "\n",
        "1. **ç¬¬ä¸€æ­¥**ï¼šå­¦ä¹  PEFT åº“ï¼ˆæŒæ¡å·¥å…·ï¼‰\n",
        "2. **ç¬¬äºŒæ­¥**ï¼šå­¦ä¹ æ‰‹åŠ¨å®ç°ï¼ˆç†è§£åŸç†ï¼‰\n",
        "3. **ç¬¬ä¸‰æ­¥**ï¼šå®é™…é¡¹ç›®ä¸­ä½¿ç”¨ PEFT\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ æ ¸å¿ƒä¾èµ–è¯´æ˜\n",
        "\n",
        "æœ¬æ•™ç¨‹ä½¿ç”¨ä»¥ä¸‹ç»è¿‡æµ‹è¯•çš„ä¾èµ–ç‰ˆæœ¬ç»„åˆï¼š\n",
        "\n",
        "- **torch==2.8.0**ï¼šPyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
        "- **transformers==4.51.3**ï¼šHugging Face æ¨¡å‹åº“\n",
        "- **peft==0.13.2**ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒåº“ï¼ˆæ ¸å¿ƒï¼‰\n",
        "- **accelerate==1.0.1**ï¼šåˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿåº“\n",
        "- **pandas==2.2.2**ï¼šæ•°æ®å¤„ç†\n",
        "- **numpy==2.0.2**ï¼šæ•°å€¼è®¡ç®—\n",
        "\n",
        "ğŸ’¡ **æ³¨æ„**ï¼šè¿™äº›ç‰ˆæœ¬å·²ç»è¿‡å…¼å®¹æ€§æµ‹è¯•ï¼Œå»ºè®®ä½¿ç”¨ç›¸åŒç‰ˆæœ¬ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zxStQdi5kgI"
      },
      "source": [
        "# 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡å’Œä¾èµ–å®‰è£…\n",
        "\n",
        "## ğŸ“¦ ä¸€é”®å®‰è£…æ‰€æœ‰ä¾èµ–\n",
        "\n",
        "ä»¥ä¸‹å‘½ä»¤å°†å®‰è£…æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åŒ…ã€‚æˆ‘ä»¬ä½¿ç”¨ `-q` å‚æ•°æ¥å‡å°‘è¾“å‡ºä¿¡æ¯ã€‚\n",
        "\n",
        "### ğŸ” ä¾èµ–åŒ…è¯´æ˜\n",
        "\n",
        "- **torch==2.9.0+cu126**ï¼šPyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆæ”¯æŒ CUDA 12.6ï¼Œè´Ÿè´£å¼ é‡è®¡ç®—ä¸è®­ç»ƒæ‰§è¡Œæ ¸å¿ƒï¼‰\n",
        "- **transformers==4.57.2**ï¼šHugging Face é¢„è®­ç»ƒå¤§æ¨¡å‹åº“ï¼Œæä¾› LLMã€Diffusion ç­‰ä¸»æµæ¨¡å‹è°ƒç”¨èƒ½åŠ›\n",
        "- **peft==0.18.0**ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFT, Parameter-Efficient Fine-Tuningï¼‰æ ¸å¿ƒåº“ï¼Œæ”¯æŒ LoRAã€QLoRAã€AdaLoRA ç­‰æŠ€æœ¯\n",
        "- **accelerate==1.12.0**ï¼šåˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿåº“ï¼Œç®€åŒ–å¤šå¡/å¤šæœºè®­ç»ƒï¼Œè®© Trainer æ— ç¼æ‰©å±•\n",
        "- **pandas==2.2.2**ï¼šç»“æ„åŒ–æ•°æ®å¤„ç†ä¸æ•°æ®åˆ†ææ ¸å¿ƒä¾èµ–\n",
        "- **numpy==2.0.2**ï¼šæ•°å€¼è®¡ç®—åŸºç¡€åº“ï¼ŒTensor ä¹‹å‰çš„ä¸€å±‚æ•°å­¦åŸºçŸ³\n",
        "- **tqdm==4.67.1**ï¼šè®­ç»ƒ/æ¨ç†è¿›åº¦æ¡ç»„ä»¶ï¼Œä¾¿äºå¯è§†åŒ–ç›‘æ§å¤„ç†è¿›åº¦\n",
        "- **matplotlib==3.10.0**ï¼šé€šç”¨æ•°æ®å¯è§†åŒ–åº“ï¼Œå¯ç”¨äºç»˜åˆ¶ Loss æ›²çº¿ã€è®­ç»ƒè¡¨ç°è¶‹åŠ¿å›¾\n",
        "- **requests==2.32.4**ï¼šHTTP ç½‘ç»œè¯·æ±‚åº“ï¼Œç”¨äºæ¨¡å‹æƒé‡/æ•°æ®ä¸‹è½½ã€API è®¿é—®\n",
        "- **safetensors==0.7.0**ï¼šå®‰å…¨æ¨¡å‹æƒé‡æ ¼å¼ï¼Œå®ç°æ›´å¿«åŠ è½½é€Ÿåº¦å¹¶é¿å… Pickle æ‰§è¡Œé£é™©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzbG7r5Z5kgJ",
        "outputId": "3b8ac9a6-2ddc-4295-cf12-9fe2bedb36c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼\n",
            "ğŸ’¡ å»ºè®®é‡å¯ Runtime ä»¥ç¡®ä¿æ–°åŒ…æ­£å¸¸å·¥ä½œï¼ˆRuntime -> Restart Runtimeï¼‰\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ å®‰è£…ä¾èµ–åŒ…\n",
        "# è¯´æ˜ï¼šä½¿ç”¨ -q å‚æ•°å®‰é™æ¨¡å¼å®‰è£…ï¼Œ--upgrade ç¡®ä¿å®‰è£…æœ€æ–°ç‰ˆæœ¬\n",
        "# å¦‚æœåœ¨ Colab ä¸­è¿è¡Œï¼ŒæŸäº›åŒ…å¯èƒ½å·²é¢„è£…ï¼Œä¼šè‡ªåŠ¨è·³è¿‡\n",
        "\n",
        "%pip install -q --upgrade \\\n",
        "    torch==2.9.0+cu126  \\\n",
        "    transformers==4.57.2 \\\n",
        "    peft==0.18.0 \\\n",
        "    accelerate==1.12.0 \\\n",
        "    pandas==2.2.2 \\\n",
        "    numpy==2.0.2 \\\n",
        "    tqdm==4.67.1 \\\n",
        "    matplotlib==3.10.0 \\\n",
        "    requests==2.32.4 \\\n",
        "    safetensors==0.7.0\n",
        "\n",
        "print(\"âœ… ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN_sfOsl5kgK"
      },
      "source": [
        "# 2ï¸âƒ£ å¯¼å…¥ä¾èµ–åº“å’Œç¯å¢ƒæ£€æŸ¥\n",
        "\n",
        "## ğŸ”§ æ ¸å¿ƒåº“å¯¼å…¥è¯´æ˜\n",
        "\n",
        "ä¸‹é¢æˆ‘ä»¬å°†å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ï¼Œå¹¶æ£€æŸ¥è¿è¡Œç¯å¢ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZYeCSUI5kgK",
        "outputId": "7c9dfd76-ed5e-4331-c077-640ca00331e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ç¯å¢ƒæ£€æŸ¥\n",
            "============================================================\n",
            "ğŸ® ä½¿ç”¨è®¾å¤‡: cuda\n",
            "   GPU å‹å·: Tesla T4\n",
            "   GPU æ•°é‡: 1\n",
            "   CUDA ç‰ˆæœ¬: 12.6\n",
            "ğŸ Python åº“ç‰ˆæœ¬:\n",
            "   PyTorch: 2.9.0+cu126\n",
            "   Transformers: 4.57.2\n",
            "   PEFT: 0.18.0\n",
            "\n",
            "âœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”§ å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åº“\n",
        "# åŠŸèƒ½ï¼šå‡†å¤‡è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ä¾èµ–\n",
        "\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,  # åºåˆ—åˆ†ç±»æ¨¡å‹ï¼ˆç”¨äºæ–‡æœ¬åˆ†ç±»ï¼‰\n",
        "    AutoTokenizer,                        # è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨\n",
        "    get_linear_schedule_with_warmup,     # å¸¦é¢„çƒ­çš„çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,        # LoRA é…ç½®ç±»\n",
        "    TaskType,          # ä»»åŠ¡ç±»å‹æšä¸¾\n",
        "    get_peft_model,    # å°† LoRA åº”ç”¨åˆ°æ¨¡å‹çš„æ ¸å¿ƒå‡½æ•°\n",
        ")\n",
        "from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º\n",
        "import requests        # HTTP è¯·æ±‚ï¼ˆç”¨äºä¸‹è½½æ•°æ®ï¼‰\n",
        "\n",
        "# ğŸ¯ è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
        "# è¯´æ˜ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¾ˆå¤šæ“ä½œæ¶‰åŠéšæœºæ€§ï¼ˆå¦‚å‚æ•°åˆå§‹åŒ–ã€æ•°æ®æ‰“ä¹±ï¼‰\n",
        "#      å›ºå®šéšæœºç§å­å¯ä»¥è®©æ¯æ¬¡è¿è¡Œå¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œä¾¿äºè°ƒè¯•å’Œå¯¹æ¯”\n",
        "torch.manual_seed(42)   # PyTorch çš„éšæœºç§å­\n",
        "random.seed(42)         # Python æ ‡å‡†åº“çš„éšæœºç§å­\n",
        "\n",
        "# ğŸ® è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨ GPUï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ CPU\n",
        "# è¯´æ˜ï¼šCUDA æ˜¯ NVIDIA GPU çš„å¹¶è¡Œè®¡ç®—å¹³å°\n",
        "#      torch.cuda.is_available() æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ğŸ“Š æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯\n",
        "print(\"ğŸ” ç¯å¢ƒæ£€æŸ¥\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU å‹å·: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
        "    print(f\"   CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"   âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPUï¼ˆè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼‰\")\n",
        "\n",
        "print(f\"ğŸ Python åº“ç‰ˆæœ¬:\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"   Transformers: {transformers.__version__}\")\n",
        "    import peft\n",
        "    print(f\"   PEFT: {peft.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"   âš ï¸ éƒ¨åˆ†åº“æœªå®‰è£…: {e}\")\n",
        "\n",
        "print(\"\\nâœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mygKDZAe5kgK"
      },
      "source": [
        "# 3ï¸âƒ£ æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†\n",
        "\n",
        "## ğŸ“Š æ•°æ®é›†ä»‹ç»\n",
        "\n",
        "æˆ‘ä»¬ä½¿ç”¨ **SMS Spam Collection** æ•°æ®é›†è¿›è¡Œåƒåœ¾çŸ­ä¿¡åˆ†ç±»ä»»åŠ¡ï¼š\n",
        "\n",
        "- **æ•°æ®æ¥æº**ï¼šUCI Machine Learning Repository\n",
        "- **æ•°æ®è§„æ¨¡**ï¼šçº¦ 5,572 æ¡çŸ­ä¿¡\n",
        "- **ä»»åŠ¡ç±»å‹**ï¼šäºŒåˆ†ç±»ï¼ˆham æ­£å¸¸çŸ­ä¿¡ vs spam åƒåœ¾çŸ­ä¿¡ï¼‰\n",
        "- **æ ‡ç­¾åˆ†å¸ƒ**ï¼šä¸å¹³è¡¡ï¼ˆham çº¦ 87%, spam çº¦ 13%ï¼‰\n",
        "\n",
        "## ğŸ”§ æ•°æ®å¤„ç†æµç¨‹\n",
        "\n",
        "```\n",
        "åŸå§‹æ•°æ®ä¸‹è½½ â†’ ç±»åˆ«å¹³è¡¡é‡‡æ · â†’ åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›† â†’ åˆ†è¯ç¼–ç  â†’ DataLoaderåŠ è½½\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn1FLJSN5kgL",
        "outputId": "abbd2b06-aa1d-46e6-bc03-2126644f74e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… æ•°æ®å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Š æ•°æ®å¤„ç†å·¥å…·å‡½æ•°\n",
        "# åŠŸèƒ½ï¼šæä¾›æ•°æ®ä¸‹è½½ã€å¹³è¡¡ã€åˆ’åˆ†å’ŒåŠ è½½çš„å®Œæ•´å·¥å…·é›†\n",
        "\n",
        "def create_balanced_dataset(df):\n",
        "    \"\"\"\n",
        "    åˆ›å»ºç±»åˆ«å¹³è¡¡çš„æ•°æ®é›†\n",
        "\n",
        "    ğŸ¯ ç›®çš„ï¼šè§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜\n",
        "    åŸå§‹æ•°æ®ä¸­ hamï¼ˆæ­£å¸¸çŸ­ä¿¡ï¼‰è¿œå¤šäº spamï¼ˆåƒåœ¾çŸ­ä¿¡ï¼‰ï¼Œ\n",
        "    è¿™ä¼šå¯¼è‡´æ¨¡å‹åå‘é¢„æµ‹å¤šæ•°ç±»ã€‚é€šè¿‡ä¸‹é‡‡æ ·å¤šæ•°ç±»ï¼Œ\n",
        "    ä½¿ä¸¤ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ç›¸ç­‰ã€‚\n",
        "\n",
        "    å‚æ•°ï¼š\n",
        "        df: pandas DataFrameï¼ŒåŒ…å« Label å’Œ Text åˆ—\n",
        "\n",
        "    è¿”å›ï¼š\n",
        "        balanced_df: ç±»åˆ«å¹³è¡¡åçš„ DataFrame\n",
        "\n",
        "    ç¤ºä¾‹ï¼š\n",
        "        åŸå§‹ï¼šham(4825æ¡) + spam(747æ¡) = 5572æ¡\n",
        "        å¹³è¡¡åï¼šham(747æ¡) + spam(747æ¡) = 1494æ¡\n",
        "    \"\"\"\n",
        "    label_col = df[\"Label\"]\n",
        "\n",
        "    # ğŸ” å…¼å®¹æ€§å¤„ç†ï¼šæ ‡ç­¾å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆ'spam'/'ham'ï¼‰æˆ–æ•°å­—ï¼ˆ0/1ï¼‰\n",
        "    spam_mask = (label_col == \"spam\") | (label_col == 1)\n",
        "    ham_mask = (label_col == \"ham\") | (label_col == 0)\n",
        "\n",
        "    # ç»Ÿè®¡åƒåœ¾çŸ­ä¿¡æ•°é‡ï¼ˆå°‘æ•°ç±»ï¼‰\n",
        "    spam_count = int(spam_mask.sum())\n",
        "\n",
        "    if spam_count == 0 or ham_mask.sum() == 0:\n",
        "        raise ValueError(\"æ•°æ®é›†ç¼ºå°‘ spam æˆ– ham ç±»åˆ«ï¼Œæ— æ³•å¹³è¡¡\")\n",
        "\n",
        "    # ğŸ² ä»å¤šæ•°ç±»ï¼ˆhamï¼‰ä¸­éšæœºé‡‡æ ·ï¼Œæ•°é‡ä¸å°‘æ•°ç±»ç›¸åŒ\n",
        "    # random_state=123 ç¡®ä¿æ¯æ¬¡é‡‡æ ·ç»“æœç›¸åŒï¼Œä¾¿äºå¤ç°\n",
        "    ham_subset = df[ham_mask].sample(spam_count, random_state=123)\n",
        "\n",
        "    # ğŸ“¦ åˆå¹¶ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®\n",
        "    balanced = pd.concat([ham_subset, df[spam_mask]])\n",
        "\n",
        "    # ğŸ”€ éšæœºæ‰“ä¹±æ•°æ®ï¼Œé¿å…æ•°æ®é¡ºåºåå·®\n",
        "    return balanced.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def random_split(df, train_frac=0.7, val_frac=0.1):\n",
        "    \"\"\"\n",
        "    éšæœºåˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†\n",
        "\n",
        "    ğŸ¯ ç›®çš„ï¼šå°†æ•°æ®åˆ†ä¸ºä¸‰éƒ¨åˆ†\n",
        "    - è®­ç»ƒé›†ï¼šç”¨äºæ¨¡å‹å‚æ•°æ›´æ–°\n",
        "    - éªŒè¯é›†ï¼šç”¨äºè°ƒæ•´è¶…å‚æ•°å’Œæ—©åœ\n",
        "    - æµ‹è¯•é›†ï¼šç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°\n",
        "\n",
        "    å‚æ•°ï¼š\n",
        "        df: pandas DataFrame\n",
        "        train_frac: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.7 = 70%ï¼‰\n",
        "        val_frac: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.1 = 10%ï¼‰\n",
        "\n",
        "    è¿”å›ï¼š\n",
        "        train_df, val_df, test_df: ä¸‰ä¸ªæ•°æ®æ¡†\n",
        "\n",
        "    ç¤ºä¾‹ï¼š\n",
        "        æ€»å…±1494æ¡ â†’ è®­ç»ƒ1046æ¡(70%) + éªŒè¯149æ¡(10%) + æµ‹è¯•299æ¡(20%)\n",
        "    \"\"\"\n",
        "    # ğŸ”€ éšæœºæ‰“ä¹±æ•°æ®\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "    # ğŸ“ è®¡ç®—åˆ’åˆ†ç‚¹\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    val_end = train_end + int(len(df) * val_frac)\n",
        "\n",
        "    # âœ‚ï¸ æ‰§è¡Œåˆ’åˆ†\n",
        "    train_df = df[:train_end]\n",
        "    val_df = df[train_end:val_end]\n",
        "    test_df = df[val_end:]\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def ensure_sms_file(local_path=\"./SMSSpamCollection.tsv\"):\n",
        "    \"\"\"\n",
        "    ç¡®ä¿ SMS æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½\n",
        "\n",
        "    ğŸ¯ ç›®çš„ï¼šè‡ªåŠ¨åŒ–æ•°æ®å‡†å¤‡æµç¨‹\n",
        "    å¦‚æœæœ¬åœ°å·²æœ‰æ•°æ®æ–‡ä»¶ï¼Œç›´æ¥è¿”å›è·¯å¾„ï¼›\n",
        "    å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä» UCI ä»“åº“ä¸‹è½½å¹¶è§£å‹ã€‚\n",
        "\n",
        "    å‚æ•°ï¼š\n",
        "        local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
        "\n",
        "    è¿”å›ï¼š\n",
        "        str: æ•°æ®æ–‡ä»¶çš„å®é™…è·¯å¾„\n",
        "\n",
        "    è¯´æ˜ï¼š\n",
        "        æ•°æ®æ–‡ä»¶æ ¼å¼ä¸º TSVï¼ˆTab-Separated Valuesï¼‰ï¼Œæ¯è¡ŒåŒ…å«æ ‡ç­¾å’Œæ–‡æœ¬\n",
        "    \"\"\"\n",
        "    # âœ… å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "\n",
        "    # ğŸ“¥ ä» UCI æœºå™¨å­¦ä¹ ä»“åº“ä¸‹è½½æ•°æ®\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "    print(f\"ğŸ“¥ æ­£åœ¨ä» UCI ä»“åº“ä¸‹è½½æ•°æ®...\")\n",
        "    print(f\"   URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        # ğŸŒ å‘é€ HTTP è¯·æ±‚ä¸‹è½½ ZIP æ–‡ä»¶\n",
        "        resp = requests.get(url, timeout=30)\n",
        "        resp.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸\n",
        "\n",
        "        # ğŸ“¦ è§£å‹ ZIP æ–‡ä»¶ï¼Œè¯»å–å…¶ä¸­çš„ SMSSpamCollection æ–‡ä»¶\n",
        "        with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
        "            raw = zf.read(\"SMSSpamCollection\").decode(\"utf-8\")\n",
        "\n",
        "        # ğŸ’¾ ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶\n",
        "        out_path = \"SMSSpamCollection.tsv\"\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(raw)\n",
        "\n",
        "        print(f\"âœ… æ•°æ®ä¸‹è½½æˆåŠŸï¼Œä¿å­˜åˆ°: {out_path}\")\n",
        "        return out_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æ•°æ®ä¸‹è½½å¤±è´¥: {e}\")\n",
        "        print(f\"ğŸ’¡ è¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ°å½“å‰ç›®å½•\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_sms_dataframe():\n",
        "    \"\"\"\n",
        "    åŠ è½½å¹¶é¢„å¤„ç† SMS æ•°æ®é›†\n",
        "\n",
        "    ğŸ¯ ç›®çš„ï¼šä¸€ç«™å¼æ•°æ®åŠ è½½å‡½æ•°\n",
        "    æ•´åˆäº†ä¸‹è½½ã€è¯»å–ã€å¹³è¡¡ã€æ ‡ç­¾è½¬æ¢å’Œåˆ’åˆ†çš„å®Œæ•´æµç¨‹\n",
        "\n",
        "    è¿”å›ï¼š\n",
        "        train_df, val_df, test_df: ä¸‰ä¸ªå·²å¤„ç†çš„ DataFrame\n",
        "\n",
        "    æ•°æ®å¤„ç†æ­¥éª¤ï¼š\n",
        "        1. ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰\n",
        "        2. è¯»å– TSV æ–‡ä»¶\n",
        "        3. å¹³è¡¡ç±»åˆ«åˆ†å¸ƒ\n",
        "        4. æ ‡ç­¾è½¬æ¢ï¼ˆhamâ†’0, spamâ†’1ï¼‰\n",
        "        5. åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†\n",
        "    \"\"\"\n",
        "    # ğŸ“‚ è·å–æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚éœ€è¦ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰\n",
        "    data_path = ensure_sms_file()\n",
        "\n",
        "    # ğŸ“– è¯»å– TSV æ–‡ä»¶\n",
        "    # sep=\"\\t\" æŒ‡å®šåˆ¶è¡¨ç¬¦åˆ†éš”\n",
        "    # names æŒ‡å®šåˆ—å\n",
        "    # header=None è¡¨ç¤ºæ–‡ä»¶æ²¡æœ‰è¡¨å¤´è¡Œ\n",
        "    df = pd.read_csv(data_path, sep=\"\\t\", names=[\"Label\", \"Text\"], header=None)\n",
        "\n",
        "    print(f\"ğŸ“Š åŸå§‹æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡\")\n",
        "    print(f\"   - ham (æ­£å¸¸): {(df['Label']=='ham').sum()} æ¡\")\n",
        "    print(f\"   - spam (åƒåœ¾): {(df['Label']=='spam').sum()} æ¡\")\n",
        "\n",
        "    # âš–ï¸ åˆ›å»ºç±»åˆ«å¹³è¡¡çš„æ•°æ®é›†\n",
        "    balanced = create_balanced_dataset(df)\n",
        "    print(f\"âš–ï¸  æ•°æ®å¹³è¡¡å: {len(balanced)} æ¡ï¼ˆæ¯ç±» {len(balanced)//2} æ¡ï¼‰\")\n",
        "\n",
        "    # ğŸ”„ æ ‡ç­¾è½¬æ¢ï¼šå­—ç¬¦ä¸² â†’ æ•°å­—\n",
        "    # æœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦æ•°å­—æ ‡ç­¾\n",
        "    label_map = {\"ham\": 0, \"spam\": 1}\n",
        "    balanced[\"Label\"] = balanced[\"Label\"].apply(\n",
        "        lambda v: label_map[v] if v in label_map else int(v)\n",
        "    )\n",
        "\n",
        "    # âœ‚ï¸ åˆ’åˆ†æ•°æ®é›†\n",
        "    train_df, val_df, test_df = random_split(balanced, train_frac=0.7, val_frac=0.1)\n",
        "    print(f\"âœ‚ï¸  æ•°æ®åˆ’åˆ†å®Œæˆ:\")\n",
        "    print(f\"   - è®­ç»ƒé›†: {len(train_df)} æ¡\")\n",
        "    print(f\"   - éªŒè¯é›†: {len(val_df)} æ¡\")\n",
        "    print(f\"   - æµ‹è¯•é›†: {len(test_df)} æ¡\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "# ğŸ“ åˆå­¦è€…æç¤ºï¼š\n",
        "# ä»¥ä¸Šå‡½æ•°æ„æˆäº†å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿ï¼Œæ¯ä¸ªå‡½æ•°èŒè´£å•ä¸€ï¼Œä¾¿äºç†è§£å’Œè°ƒè¯•ã€‚\n",
        "# åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œå»ºè®®å°†æ•°æ®å¤„ç†ä»£ç æ¨¡å—åŒ–ï¼Œæé«˜ä»£ç å¤ç”¨æ€§ã€‚\n",
        "print(\"âœ… æ•°æ®å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gISqIzXT5kgL",
        "outputId": "102f903b-b9b4-4be0-a945-ee8967e68bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Dataset ç±»å®šä¹‰å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“¦ PyTorch Dataset ç±»å®šä¹‰\n",
        "# åŠŸèƒ½ï¼šå°† DataFrame è½¬æ¢ä¸º PyTorch å¯ç”¨çš„æ•°æ®é›†æ ¼å¼\n",
        "\n",
        "class SpamSequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    SMS åƒåœ¾çŸ­ä¿¡æ•°æ®é›†ç±»\n",
        "\n",
        "    ğŸ¯ ç›®çš„ï¼šå°† pandas DataFrame è½¬æ¢ä¸º PyTorch Dataset\n",
        "    è¿™ä¸ªç±»ç»§æ‰¿è‡ª torch.utils.data.Datasetï¼Œå®ç°äº† PyTorch æ•°æ®åŠ è½½çš„æ ‡å‡†æ¥å£\n",
        "\n",
        "    ä¸æ‰‹åŠ¨å®ç°çš„åŒºåˆ«ï¼š\n",
        "    - æ‰‹åŠ¨å®ç°ï¼šä½¿ç”¨ tiktoken åˆ†è¯å™¨ï¼Œæ‰‹åŠ¨å¤„ç† padding\n",
        "    - PEFTç‰ˆæœ¬ï¼šä½¿ç”¨ Hugging Face Tokenizerï¼Œè‡ªåŠ¨å¤„ç† padding å’Œ attention_mask\n",
        "\n",
        "    å…³é”®ç‰¹æ€§ï¼š\n",
        "    1. ä½¿ç”¨ Hugging Face tokenizerï¼Œä¸æ¨¡å‹å®Œç¾é…åˆ\n",
        "    2. è‡ªåŠ¨ç”Ÿæˆ attention_maskï¼ˆæ ‡è®°å“ªäº›æ˜¯çœŸå® tokenï¼Œå“ªäº›æ˜¯ paddingï¼‰\n",
        "    3. æ‰¹é‡é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬ï¼Œæé«˜æ•ˆç‡\n",
        "\n",
        "    SMS åƒåœ¾çŸ­ä¿¡æ•°æ®é›†æ ¼å¼ç¤ºä¾‹(æ¯ä¸€è¡Œç”±ä¸¤åˆ—ç»„æˆï¼šç¬¬ä¸€åˆ—æ˜¯æ ‡ç­¾ï¼ˆham è¡¨ç¤ºæ­£å¸¸ä¿¡æ¯ï¼Œspam è¡¨ç¤ºåƒåœ¾ä¿¡æ¯ï¼‰ï¼Œç¬¬äºŒåˆ—æ˜¯åŸå§‹æ–‡æœ¬)ï¼š\n",
        "    ham   What you doing?how are you?\n",
        "    ham   Ok lar... Joking wif u oni...\n",
        "    ham   dun say so early hor... U c already then say...\n",
        "    ham   MY NO. IN LUTON 0125698789 RING ME IF UR AROUND! H*\n",
        "    ham   Siva is in hostel aha:-.\n",
        "    ham   Cos i was out shopping wif darren jus now n i called him 2 ask wat present he wan lor. Then he started guessing who i was wif n he finally guessed darren lor.\n",
        "    spam   FreeMsg: Txt: CALL to No: 86888 & claim your reward of 3 hours talk time to use from your phone now! ubscribe6GBP/ mnth inc 3hrs 16 stop?txtStop\n",
        "    spam   Sunshine Quiz! Win a super Sony DVD recorder if you canname the capital of Australia? Text MQUIZ to 82277. B\n",
        "    spam   URGENT! Your Mobile No 07808726822 was awarded a L2,000 Bonus Caller Prize on 02/09/03! This is our 2nd attempt to contact YOU! Call 0871-872-9758 BOX95QU\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, df, tokenizer, max_length=96):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–æ•°æ®é›†\n",
        "\n",
        "        å‚æ•°ï¼š\n",
        "            df: pandas DataFrameï¼ŒåŒ…å« 'Text' å’Œ 'Label' åˆ—\n",
        "            tokenizer: Hugging Face tokenizer å¯¹è±¡\n",
        "            max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¶…è¿‡ä¼šæˆªæ–­ï¼Œä¸è¶³ä¼šå¡«å……\n",
        "\n",
        "        å¤„ç†æµç¨‹ï¼š\n",
        "            1. æ‰¹é‡åˆ†è¯æ‰€æœ‰æ–‡æœ¬\n",
        "            2. ç»Ÿä¸€é•¿åº¦ï¼ˆæˆªæ–­/å¡«å……åˆ° max_lengthï¼‰\n",
        "            3. è½¬æ¢ä¸º PyTorch tensors\n",
        "            4. ä¿å­˜ labels ä¸ºæ•°å­—å¼ é‡\n",
        "        \"\"\"\n",
        "        # ğŸ”¤ æ‰¹é‡åˆ†è¯æ‰€æœ‰æ–‡æœ¬\n",
        "        # ä½¿ç”¨ Hugging Face Tokenizer å¯¹æ‰€æœ‰æ–‡æœ¬è¿›è¡Œç¼–ç \n",
        "        # tokenizer() å‡½æ•°çš„å‚æ•°è¯´æ˜ï¼š\n",
        "        # - df[\"Text\"].tolist(): å°† DataFrame çš„ 'Text' åˆ—è½¬æ¢ä¸º Python åˆ—è¡¨ä½œä¸ºè¾“å…¥ã€‚\n",
        "        # - truncation=True: å¯ç”¨æˆªæ–­ã€‚å¦‚æœæ–‡æœ¬é•¿åº¦è¶…è¿‡ `max_length`ï¼Œåˆ™å°†å…¶æˆªæ–­ã€‚\n",
        "        # - padding=\"max_length\": å¯ç”¨å¡«å……ã€‚å°†æ‰€æœ‰åºåˆ—å¡«å……åˆ° `max_length` æŒ‡å®šçš„é•¿åº¦ã€‚\n",
        "        # - max_length=96: æŒ‡å®šæ‰€æœ‰åºåˆ—çš„æœ€ç»ˆé•¿åº¦ã€‚è¾ƒçŸ­çš„åºåˆ—å°†è¢«å¡«å……ï¼Œè¾ƒé•¿çš„åºåˆ—å°†è¢«æˆªæ–­ã€‚\n",
        "        # - return_tensors=\"pt\": è¿”å› PyTorch å¼ é‡æ ¼å¼çš„ç¼–ç ç»“æœï¼Œæ–¹ä¾¿åç»­æ¨¡å‹ä½¿ç”¨ã€‚\n",
        "        encodings = tokenizer(\n",
        "            df[\"Text\"].tolist(),          # å°† pandas Series è½¬ä¸º listï¼Œä¾›åˆ†è¯å™¨å¤„ç†\n",
        "            truncation=True,               # å¯ç”¨æˆªæ–­ï¼Œä¿è¯åºåˆ—é•¿åº¦ä¸è¶…è¿‡ max_length\n",
        "            padding=\"max_length\",          # å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œä½¿å¾—æ‰¹æ¬¡å†…çš„æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´\n",
        "            max_length=max_length,         # æœ€å¤§åºåˆ—é•¿åº¦ï¼Œå®šä¹‰äº†è¾“å…¥æ¨¡å‹æ–‡æœ¬çš„æœ€å¤§tokenæ•°\n",
        "            return_tensors=\"pt\",           # è¿”å› PyTorch å¼ é‡æ ¼å¼ï¼Œæ–¹ä¾¿åç»­åœ¨PyTorchä¸­æ“ä½œ\n",
        "        )\n",
        "\n",
        "        # ğŸ“Š ä¿å­˜ç¼–ç ç»“æœ\n",
        "        # input_ids: è¡¨ç¤ºæ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„ token ID åºåˆ—ï¼Œå½¢çŠ¶ä¸º [N, max_length]\n",
        "        # å…¶ä¸­ N æ˜¯æ•°æ®é›†ä¸­çš„æ ·æœ¬æ•°é‡ã€‚\n",
        "        self.input_ids = encodings[\"input_ids\"]\n",
        "\n",
        "        # attention_mask: æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶ä¸º [N, max_length]\n",
        "        # å€¼ä¸º 1 çš„ä½ç½®è¡¨ç¤ºæ˜¯åŸå§‹æ–‡æœ¬çš„æœ‰æ•ˆ tokenï¼Œå€¼ä¸º 0 çš„ä½ç½®è¡¨ç¤ºæ˜¯å¡«å……ï¼ˆpaddingï¼‰tokenã€‚\n",
        "        # æ¨¡å‹åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ä¼šå¿½ç•¥ attention_mask=0 çš„ä½ç½®ï¼Œé¿å…å¡«å……å¯¹æ¨¡å‹å†³ç­–çš„å½±å“ã€‚\n",
        "        self.attention_mask = encodings[\"attention_mask\"]\n",
        "\n",
        "        # ğŸ·ï¸ ä¿å­˜æ ‡ç­¾\n",
        "        # å°† DataFrame çš„ 'Label' åˆ—è½¬æ¢ä¸º Python åˆ—è¡¨ï¼Œå†è½¬æ¢ä¸º PyTorch å¼ é‡ã€‚\n",
        "        # è½¬æ¢ä¸º torch.long ç±»å‹ï¼ˆå³ int64ï¼‰ï¼Œå› ä¸º PyTorch çš„äº¤å‰ç†µæŸå¤±å‡½æ•°é€šå¸¸è¦æ±‚æ ‡ç­¾ä¸º long ç±»å‹ã€‚\n",
        "        self.labels = torch.tensor(df[\"Label\"].tolist(), dtype=torch.long)\n",
        "\n",
        "        # ğŸ“ ä¿å­˜æ•°æ®é›†å¤§å°\n",
        "        self._len = len(self.labels)\n",
        "\n",
        "        print(f\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ:\")\n",
        "        print(f\"   - æ ·æœ¬æ•°: {self._len}\")\n",
        "        print(f\"   - åºåˆ—é•¿åº¦: {max_length}\")\n",
        "        print(f\"   - è¾“å…¥å½¢çŠ¶: {self.input_ids.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        è¿”å›æ•°æ®é›†å¤§å°\n",
        "\n",
        "        ğŸ¯ ç›®çš„ï¼šå‘Šè¯‰ DataLoader æ•°æ®é›†æœ‰å¤šå°‘æ ·æœ¬\n",
        "        è¿™æ˜¯ PyTorch Dataset å¿…é¡»å®ç°çš„æ–¹æ³•ä¹‹ä¸€\n",
        "        \"\"\"\n",
        "        return self._len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        è·å–å•ä¸ªæ ·æœ¬\n",
        "\n",
        "        ğŸ¯ ç›®çš„ï¼šæ ¹æ®ç´¢å¼•è¿”å›ä¸€ä¸ªæ ·æœ¬\n",
        "        è¿™æ˜¯ PyTorch Dataset å¿…é¡»å®ç°çš„æ–¹æ³•ä¹‹ä¸€\n",
        "\n",
        "        å‚æ•°ï¼š\n",
        "            idx: æ ·æœ¬ç´¢å¼•ï¼ˆ0 åˆ° len-1ï¼‰\n",
        "\n",
        "        è¿”å›ï¼š\n",
        "            dict: åŒ…å« input_ids, attention_mask, labels çš„å­—å…¸\n",
        "\n",
        "        è¯´æ˜ï¼š\n",
        "            è¿”å›å­—å…¸æ ¼å¼æ˜¯ Hugging Face æ¨¡å‹çš„æ ‡å‡†è¾“å…¥æ ¼å¼ï¼Œ\n",
        "            å¯ä»¥ç›´æ¥ä½¿ç”¨ model(**batch) è¿›è¡Œè§£åŒ…ä¼ å‚\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],           # Token ID åºåˆ—\n",
        "            \"attention_mask\": self.attention_mask[idx], # æ³¨æ„åŠ›æ©ç \n",
        "            \"labels\": self.labels[idx],                 # æ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰\n",
        "        }\n",
        "\n",
        "\n",
        "# ğŸ“ åˆå­¦è€…æç¤ºï¼š\n",
        "# Dataset ç±»åªè´Ÿè´£æ•°æ®çš„\"ç»„ç»‡\"ï¼Œä¸è´Ÿè´£æ•°æ®çš„\"æ‰¹é‡åŠ è½½\"ã€‚\n",
        "# æ‰¹é‡åŠ è½½ç”± DataLoader å®Œæˆï¼Œå®ƒä¼šï¼š\n",
        "# 1. è°ƒç”¨ __getitem__ è·å–å¤šä¸ªæ ·æœ¬\n",
        "# 2. è‡ªåŠ¨å°†è¿™äº›æ ·æœ¬ç»„åˆæˆ batch\n",
        "# 3. å¯é€‰åœ°æ‰“ä¹±æ•°æ®é¡ºåºï¼ˆshuffle=Trueï¼‰\n",
        "# 4. å¯é€‰åœ°ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿï¼ˆnum_workers>0ï¼‰\n",
        "\n",
        "print(\"âœ… Dataset ç±»å®šä¹‰å®Œæˆ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a26Ygnx65kgM"
      },
      "source": [
        "# 4ï¸âƒ£ æ‰§è¡Œæ•°æ®åŠ è½½\n",
        "\n",
        "## ğŸš€ åˆ›å»º Tokenizer å’ŒåŠ è½½æ•°æ®\n",
        "Tokenizer çš„æ ¸å¿ƒä½œç”¨æ˜¯æŠŠåŸå§‹çš„æ–‡æœ¬ï¼ˆä¸€é•¿ä¸²å­—ç¬¦ï¼‰åˆ†å‰²æˆæ¨¡å‹å¯ä»¥ç†è§£çš„ã€æ›´å°çš„å•å…ƒï¼Œè¿™äº›å•å…ƒè¢«ç§°ä¸º tokensã€‚è¿™ä¸ªè¿‡ç¨‹å°±å«åš tokenization (åˆ†è¯)ã€‚\n",
        "\n",
        "ä»£ç ä¸­ä½¿ç”¨çš„ `AutoTokenizer.from_pretrained(\"gpt2\")` å°±æ˜¯åŠ è½½äº†ä¸€ä¸ªé¢„è®­ç»ƒçš„ GPT-2 åˆ†è¯å™¨ï¼Œå®ƒä¼šå°†æ‚¨çš„è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸º GPT-2 æ¨¡å‹èƒ½å¤Ÿå¤„ç†çš„ token ID åºåˆ—ï¼Œç„¶åè¿™äº› ID ä¼šåœ¨æ¨¡å‹å†…éƒ¨è¢«è½¬æ¢æˆè¯åµŒå…¥ã€‚\n",
        "\n",
        "ç°åœ¨æˆ‘ä»¬å°†ï¼š\n",
        "1. åŠ è½½ GPT-2 tokenizer\n",
        "2. åŠ è½½å¹¶å¤„ç† SMS æ•°æ®é›†\n",
        "3. åˆ›å»º PyTorch DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV0tXlOo5kgM",
        "outputId": "8999e89e-409a-4182-a9c8-42f2e363f41f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¤ åŠ è½½ GPT-2 Tokenizer...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   âš™ï¸  è®¾ç½® pad_token = eos_token (ID: 50256)\n",
            "âœ… Tokenizer åŠ è½½å®Œæˆ\n",
            "   - è¯è¡¨å¤§å°: 50,257\n",
            "   - æ¨¡å‹ç±»å‹: GPT2TokenizerFast\n",
            "\n",
            "ğŸ“Š åŠ è½½ SMS æ•°æ®é›†...\n",
            "ğŸ“Š åŸå§‹æ•°æ®åŠ è½½å®Œæˆ: 5572 æ¡\n",
            "   - ham (æ­£å¸¸): 4825 æ¡\n",
            "   - spam (åƒåœ¾): 747 æ¡\n",
            "âš–ï¸  æ•°æ®å¹³è¡¡å: 1494 æ¡ï¼ˆæ¯ç±» 747 æ¡ï¼‰\n",
            "âœ‚ï¸  æ•°æ®åˆ’åˆ†å®Œæˆ:\n",
            "   - è®­ç»ƒé›†: 1045 æ¡\n",
            "   - éªŒè¯é›†: 149 æ¡\n",
            "   - æµ‹è¯•é›†: 300 æ¡\n",
            "\n",
            "ğŸ“ è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦: 96\n",
            "\n",
            "ğŸ”¨ åˆ›å»º PyTorch Dataset...\n",
            "âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ:\n",
            "   - æ ·æœ¬æ•°: 1045\n",
            "   - åºåˆ—é•¿åº¦: 96\n",
            "   - è¾“å…¥å½¢çŠ¶: torch.Size([1045, 96])\n",
            "âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ:\n",
            "   - æ ·æœ¬æ•°: 149\n",
            "   - åºåˆ—é•¿åº¦: 96\n",
            "   - è¾“å…¥å½¢çŠ¶: torch.Size([149, 96])\n",
            "âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ:\n",
            "   - æ ·æœ¬æ•°: 300\n",
            "   - åºåˆ—é•¿åº¦: 96\n",
            "   - è¾“å…¥å½¢çŠ¶: torch.Size([300, 96])\n",
            "\n",
            "ğŸ“¦ åˆ›å»º DataLoader...\n",
            "âœ… DataLoader åˆ›å»ºå®Œæˆ\n",
            "   - è®­ç»ƒæ‰¹æ¬¡æ•°: 131\n",
            "   - éªŒè¯æ‰¹æ¬¡æ•°: 19\n",
            "   - æµ‹è¯•æ‰¹æ¬¡æ•°: 38\n",
            "   - æ¯æ‰¹å¤§å°: 8\n",
            "\n",
            "ğŸ” æ ·æœ¬æ£€æŸ¥:\n",
            "   - input_ids å½¢çŠ¶: torch.Size([8, 96])\n",
            "   - attention_mask å½¢çŠ¶: torch.Size([8, 96])\n",
            "   - labels å½¢çŠ¶: torch.Size([8])\n",
            "   - æ ‡ç­¾åˆ†å¸ƒ: [0, 0, 0, 1, 1, 0, 0, 1]\n",
            "\n",
            "âœ… æ•°æ®å‡†å¤‡å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”¤ åŠ è½½ Tokenizer\n",
        "# åŠŸèƒ½ï¼šä» Hugging Face Hub åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 åˆ†è¯å™¨\n",
        "\n",
        "print(\"ğŸ”¤ åŠ è½½ GPT-2 Tokenizer...\")\n",
        "\n",
        "# ğŸ“¥ ä» Hugging Face ä¸‹è½½å¹¶åŠ è½½ GPT-2 tokenizer\n",
        "# AutoTokenizer.from_pretrained() ä¼šï¼š\n",
        "# 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆ~/.cache/huggingface/ï¼‰\n",
        "# 2. å¦‚æœæ²¡æœ‰ï¼Œä» Hub ä¸‹è½½\n",
        "# 3. åŠ è½½è¯è¡¨å’Œåˆ†è¯è§„åˆ™\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# ğŸ”§ é…ç½® padding token\n",
        "# GPT-2 åŸå§‹è®¾è®¡ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œæ²¡æœ‰ padding token\n",
        "# ä½†åˆ†ç±»ä»»åŠ¡éœ€è¦å°†ä¸åŒé•¿åº¦çš„æ–‡æœ¬å¯¹é½åˆ°ç›¸åŒé•¿åº¦\n",
        "# è§£å†³æ–¹æ¡ˆï¼šå°† eos_tokenï¼ˆç»“æŸç¬¦ï¼‰å¤ç”¨ä¸º pad_token\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"   âš™ï¸  è®¾ç½® pad_token = eos_token (ID: {tokenizer.eos_token_id})\")\n",
        "\n",
        "print(f\"âœ… Tokenizer åŠ è½½å®Œæˆ\")\n",
        "print(f\"   - è¯è¡¨å¤§å°: {tokenizer.vocab_size:,}\")\n",
        "print(f\"   - æ¨¡å‹ç±»å‹: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "# ğŸ“Š åŠ è½½å¹¶åˆ’åˆ†æ•°æ®\n",
        "print(\"\\nğŸ“Š åŠ è½½ SMS æ•°æ®é›†...\")\n",
        "train_df, val_df, test_df = load_sms_dataframe()\n",
        "\n",
        "# ğŸ“ è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦\n",
        "# è¯´æ˜ï¼šè¿™ä¸ªå€¼éœ€è¦å¹³è¡¡ä»¥ä¸‹å› ç´ ï¼š\n",
        "# - å¤ªçŸ­ï¼šå¯èƒ½æˆªæ–­é‡è¦ä¿¡æ¯\n",
        "# - å¤ªé•¿ï¼šå¢åŠ è®¡ç®—æˆæœ¬å’Œæ˜¾å­˜å ç”¨\n",
        "# - 96 æ˜¯ç»è¿‡å®éªŒçš„åˆç†å€¼ï¼Œè¶³ä»¥è¦†ç›–å¤§éƒ¨åˆ†çŸ­ä¿¡\n",
        "max_length = 96\n",
        "print(f\"\\nğŸ“ è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦: {max_length}\")\n",
        "\n",
        "# ğŸ”¨ åˆ›å»º PyTorch Dataset\n",
        "print(\"\\nğŸ”¨ åˆ›å»º PyTorch Dataset...\")\n",
        "train_dataset = SpamSequenceDataset(train_df, tokenizer, max_length=max_length)\n",
        "val_dataset = SpamSequenceDataset(val_df, tokenizer, max_length=max_length)\n",
        "test_dataset = SpamSequenceDataset(test_df, tokenizer, max_length=max_length)\n",
        "\n",
        "# ğŸ“¦ åˆ›å»º DataLoader\n",
        "# åŠŸèƒ½ï¼šæ‰¹é‡åŠ è½½æ•°æ®ï¼Œæ”¯æŒè‡ªåŠ¨æ‰“ä¹±ã€å¤šè¿›ç¨‹ç­‰\n",
        "print(\"\\nğŸ“¦ åˆ›å»º DataLoader...\")\n",
        "\n",
        "# ğŸ¯ DataLoader å‚æ•°è¯´æ˜ï¼š\n",
        "# - batch_size: æ¯æ‰¹æ ·æœ¬æ•°é‡\n",
        "#   - è¾ƒå¤§ï¼šè®­ç»ƒå¿«ï¼Œä½†æ˜¾å­˜å ç”¨é«˜\n",
        "#   - è¾ƒå°ï¼šæ˜¾å­˜å‹å¥½ï¼Œä½†è®­ç»ƒæ…¢\n",
        "#   - 8 æ˜¯é€‚ä¸­çš„å€¼\n",
        "# - shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®\n",
        "#   - è®­ç»ƒé›†ï¼šTrueï¼ˆé¿å…é¡ºåºåå·®ï¼‰\n",
        "#   - éªŒè¯/æµ‹è¯•é›†ï¼šFalseï¼ˆä¿æŒé¡ºåºï¼Œä¾¿äºåˆ†æï¼‰\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"âœ… DataLoader åˆ›å»ºå®Œæˆ\")\n",
        "print(f\"   - è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}\")\n",
        "print(f\"   - éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}\")\n",
        "print(f\"   - æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}\")\n",
        "print(f\"   - æ¯æ‰¹å¤§å°: 8\")\n",
        "\n",
        "# ğŸ” æ•°æ®æ£€æŸ¥ï¼šæŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬\n",
        "print(\"\\nğŸ” æ ·æœ¬æ£€æŸ¥:\")\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"   - input_ids å½¢çŠ¶: {sample_batch['input_ids'].shape}\")\n",
        "print(f\"   - attention_mask å½¢çŠ¶: {sample_batch['attention_mask'].shape}\")\n",
        "print(f\"   - labels å½¢çŠ¶: {sample_batch['labels'].shape}\")\n",
        "print(f\"   - æ ‡ç­¾åˆ†å¸ƒ: {sample_batch['labels'].tolist()}\")\n",
        "\n",
        "print(\"\\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ•°æ®é›†ä¸­çš„â€œå½¢çŠ¶â€å’Œâ€œæ ‡ç­¾åˆ†å¸ƒâ€ä»£è¡¨ä»€ä¹ˆï¼š\n",
        "\n",
        "æƒ³è±¡ä¸€ä¸‹ä½ çš„æ•°æ®å°±åƒä¸€å †çŸ­ä¿¡ï¼Œæ¯æ¬¡ä¸æ˜¯åªç»™æ¨¡å‹ä¸€æ¡çŸ­ä¿¡ï¼Œè€Œæ˜¯ç»™ä¸€å°â€œæ‰¹â€çŸ­ä¿¡ï¼Œè¿™æ ·æ¨¡å‹å¯ä»¥æ›´é«˜æ•ˆåœ°å­¦ä¹ ã€‚\n",
        "- **`input_ids` å½¢çŠ¶: `torch.Size([8, 96])`**\n",
        "\n",
        "  - `8`ï¼šè¿™ä»£è¡¨ä½ è¿™ä¸€æ‰¹æ¬¡ï¼ˆbatchï¼‰æœ‰ **8 æ¡çŸ­ä¿¡**ã€‚ä½ çš„ DataLoaderï¼ˆæ•°æ®åŠ è½½å™¨ï¼‰ä¸€æ¬¡æ€§æ‰“åŒ…äº† 8 æ¡çŸ­ä¿¡ç»™æ¨¡å‹å¤„ç†ã€‚\n",
        "  - `96`ï¼šè¿™ä»£è¡¨æ¯ä¸€æ¡çŸ­ä¿¡éƒ½è¢«å¤„ç†æˆäº† **96 ä¸ªâ€œæ•°å­—ç¼–ç â€**ã€‚å¦‚æœçŸ­ä¿¡æœ¬èº«ä¸å¤Ÿ 96 ä¸ªç¼–ç ï¼Œå°±ä¼šåœ¨åé¢è¡¥é½ï¼ˆå¡«å……ï¼‰ï¼›å¦‚æœçŸ­ä¿¡å¤ªé•¿ï¼Œå°±ä¼šè¢«æˆªæ–­åˆ° 96 ä¸ªç¼–ç ã€‚æ‰€ä»¥ï¼Œæ¯æ¡çŸ­ä¿¡ç°åœ¨éƒ½æ˜¯ä¸€ä¸ªå›ºå®šé•¿åº¦çš„æ•°å­—åºåˆ—ã€‚\n",
        "  - **é€šä¿—ç†è§£**ï¼šä½ ä¸€æ¬¡æ€§æ‹¿å‡ºäº† 8 å¼ è¡¨æ ¼ï¼Œæ¯å¼ è¡¨æ ¼æœ‰ 96 ä¸ªæ ¼å­ï¼Œæ¯ä¸ªæ ¼å­é‡Œé¢éƒ½å¡«äº†ä¸€ä¸ªæ•°å­—ï¼ˆä»£è¡¨çŸ­ä¿¡é‡Œçš„ä¸€ä¸ªè¯æˆ–è¯çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚\n",
        "\n",
        "- **`attention_mask` å½¢çŠ¶: `torch.Size([8, 96])`**\n",
        "\n",
        "  - åŒæ ·ï¼Œ`8` ä»£è¡¨ **8 æ¡çŸ­ä¿¡**ï¼Œ`96` ä»£è¡¨æ¯æ¡çŸ­ä¿¡çš„ **96 ä¸ªä½ç½®**ã€‚\n",
        "  - æ³¨æ„åŠ›æ©ç ä¼šå‘Šè¯‰æ¨¡å‹ï¼Œåœ¨è¿™ 96 ä¸ªä½ç½®ä¸­ï¼Œå“ªäº›æ˜¯çŸ­ä¿¡â€œçœŸæ­£â€çš„å†…å®¹ï¼ˆå€¼ä¸º 1ï¼‰ï¼Œå“ªäº›æ˜¯åé¢ä¸ºäº†å‡‘é½ 96 ä¸ªä½ç½®è€Œè¡¥ä¸Šçš„â€œç©ºç™½â€ï¼ˆå€¼ä¸º 0ï¼‰ã€‚\n",
        "  - **é€šä¿—ç†è§£**ï¼šä½ è¿˜æœ‰ 8 å¼ è¡¨æ ¼ï¼Œæ¯å¼ è¡¨æ ¼ä¹Ÿæ˜¯ 96 ä¸ªæ ¼å­ã€‚è¿™æ¬¡æ ¼å­é‡Œçš„æ•°å­—ä¸æ˜¯çŸ­ä¿¡å†…å®¹ï¼Œè€Œæ˜¯æ ‡è®°ï¼šå¦‚æœæ˜¯çŸ­ä¿¡çš„çœŸå†…å®¹å°±æ‰“å‹¾ï¼ˆ1ï¼‰ï¼Œæ˜¯å¡«å……çš„ç©ºç™½å°±æ‰“å‰ï¼ˆ0ï¼‰ã€‚è¿™æ ·æ¨¡å‹å°±çŸ¥é“å“ªäº›ä¿¡æ¯æ˜¯é‡è¦çš„ã€‚\n",
        "\n",
        "- **`labels` å½¢çŠ¶: `torch.Size([8])`**\n",
        "\n",
        "  - è¿™ä»£è¡¨ä½ æœ‰ **8 ä¸ªæ ‡ç­¾**ï¼Œæ¯ä¸ªæ ‡ç­¾å¯¹åº”è¿™ä¸€æ‰¹æ¬¡ä¸­çš„ä¸€æ¡çŸ­ä¿¡ã€‚\n",
        "  - **é€šä¿—ç†è§£**ï¼šä½ æœ‰ä¸€å¼ å°çº¸æ¡ï¼Œä¸Šé¢å†™äº† 8 ä¸ªæ•°å­—ï¼Œæ¯ä¸ªæ•°å­—éƒ½å¯¹åº”ç€å‰é¢é‚£ 8 æ¡çŸ­ä¿¡çš„åˆ†ç±»ç»“æœï¼ˆæ¯”å¦‚ç¬¬ä¸€æ¡çŸ­ä¿¡æ˜¯â€œæ­£å¸¸â€ï¼Œç¬¬äºŒæ¡æ˜¯â€œåƒåœ¾â€ï¼‰ã€‚\n",
        "\n",
        "- **æ ‡ç­¾åˆ†å¸ƒ: `[0, 0, 0, 1, 1, 0, 0, 1]`**\n",
        "\n",
        "  - è¿™æ˜¯é‚£ 8 ä¸ªæ ‡ç­¾çš„å…·ä½“å†…å®¹ã€‚\n",
        "\n",
        "  - é€šå¸¸ï¼Œ`0` ä»£è¡¨â€œæ­£å¸¸çŸ­ä¿¡ (ham)â€ï¼›`1` ä»£è¡¨â€œåƒåœ¾çŸ­ä¿¡ (spam)â€ã€‚\n",
        "\n",
        "  - **é€šä¿—ç†è§£**ï¼šåœ¨ä½ è¿™å¼ å°çº¸æ¡ä¸Šï¼Œå†™ç€ï¼š\n",
        "\n",
        "    - ç¬¬ 1 æ¡çŸ­ä¿¡ï¼š0ï¼ˆæ­£å¸¸ï¼‰\n",
        "    - ç¬¬ 2 æ¡çŸ­ä¿¡ï¼š0ï¼ˆæ­£å¸¸ï¼‰\n",
        "    - ç¬¬ 3 æ¡çŸ­ä¿¡ï¼š0ï¼ˆæ­£å¸¸ï¼‰\n",
        "    - ç¬¬ 4 æ¡çŸ­ä¿¡ï¼š1ï¼ˆåƒåœ¾ï¼‰\n",
        "    - ç¬¬ 5 æ¡çŸ­ä¿¡ï¼š1ï¼ˆåƒåœ¾ï¼‰\n",
        "    - ç¬¬ 6 æ¡çŸ­ä¿¡ï¼š0ï¼ˆæ­£å¸¸ï¼‰\n",
        "    - ç¬¬ 7 æ¡çŸ­ä¿¡ï¼š0ï¼ˆæ­£å¸¸ï¼‰\n",
        "    - ç¬¬ 8 æ¡çŸ­ä¿¡ï¼š1ï¼ˆåƒåœ¾ï¼‰\n",
        "\n",
        "    æ‰€ä»¥ï¼Œè¿™ä¸€æ‰¹ 8 æ¡çŸ­ä¿¡é‡Œæœ‰ 5 æ¡æ­£å¸¸çŸ­ä¿¡å’Œ 3 æ¡åƒåœ¾çŸ­ä¿¡ã€‚"
      ],
      "metadata": {
        "id": "WLRNdbVtwlaO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoz3DaTj5kgM"
      },
      "source": [
        "# 5ï¸âƒ£ ä½¿ç”¨ PEFT åº“åº”ç”¨ LoRA\n",
        "\n",
        "## ğŸ¯ PEFT æ ¸å¿ƒæ¦‚å¿µ\n",
        "\n",
        "**PEFT (Parameter-Efficient Fine-Tuning)** æ˜¯ Hugging Face æä¾›çš„å‚æ•°é«˜æ•ˆå¾®è°ƒåº“ï¼Œæ”¯æŒå¤šç§æ–¹æ³•ï¼š\n",
        "\n",
        "- **LoRA**: Low-Rank Adaptationï¼ˆæœ¬æ•™ç¨‹ä½¿ç”¨ï¼‰\n",
        "- **LoHa**: Low-Rank Hadamard Product\n",
        "- **LoKr**: Low-Rank Kronecker Product\n",
        "- **Prefix Tuning**: å¯å­¦ä¹ çš„å‰ç¼€\n",
        "- **P-Tuning**: æç¤ºå¾®è°ƒ\n",
        "- **Adapter**: é€‚é…å™¨å±‚\n",
        "\n",
        "## ğŸ”§ LoRA é…ç½®è¯¦è§£\n",
        "\n",
        "ä½¿ç”¨ PEFT åº“åº”ç”¨ LoRA åªéœ€ä¸‰æ­¥ï¼š\n",
        "1. åˆ›å»º `LoraConfig` é…ç½®å¯¹è±¡\n",
        "2. åŠ è½½åŸºç¡€æ¨¡å‹\n",
        "3. ä½¿ç”¨ `get_peft_model()` åº”ç”¨ LoRA\n",
        "\n",
        "è®©æˆ‘ä»¬æ·±å…¥äº†è§£æ¯ä¸ªå‚æ•°çš„å«ä¹‰ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDNZbN1N5kgM",
        "outputId": "a0ef27b2-1810-4dc4-afea-2b643b475805"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– æ­¥éª¤ 1: åŠ è½½åŸºç¡€æ¨¡å‹...\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ\n",
            "   - æ¨¡å‹ç±»å‹: GPT2ForSequenceClassification\n",
            "   - åˆ†ç±»ç±»åˆ«: 2\n",
            "   - è¯è¡¨å¤§å°: 50,257\n",
            "\n",
            "ğŸ”§ æ­¥éª¤ 2: åˆ›å»º LoRA é…ç½®...\n",
            "============================================================\n",
            "âœ… LoRA é…ç½®åˆ›å»ºå®Œæˆ:\n",
            "   - ç§© (r): 8\n",
            "   - ç¼©æ”¾å› å­ (alpha): 16\n",
            "   - Dropout: 0.05\n",
            "   - ç›®æ ‡æ¨¡å—: {'c_fc', 'c_proj', 'c_attn'}\n",
            "\n",
            "ğŸš€ æ­¥éª¤ 3: åº”ç”¨ LoRA...\n",
            "============================================================\n",
            "âœ… LoRA åº”ç”¨å®Œæˆï¼\n",
            "\n",
            "ğŸ“Š å‚æ•°ç»Ÿè®¡:\n",
            "trainable params: 1,181,184 || all params: 125,622,528 || trainable%: 0.9403\n",
            "\n",
            "ğŸ® æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: cuda\n",
            "\n",
            "ğŸ” æ¨¡å‹ç»“æ„é¢„è§ˆ:\n",
            "   - æ¨¡å‹ç±»å‹: PeftModelForSequenceClassification\n",
            "   - åŸºç¡€æ¨¡å‹: LoraModel\n",
            "   - LoRA é…ç½®: {'default': LoraConfig(task_type=<TaskType.SEQ_CLS: 'SEQ_CLS'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, peft_version='0.18.0', base_model_name_or_path='gpt2', revision=None, inference_mode=False, r=8, target_modules={'c_fc', 'c_proj', 'c_attn'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=True, bias='none', use_rslora=False, modules_to_save=['classifier', 'score'], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, alora_invocation_tokens=None, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, target_parameters=None, arrow_config=None, ensure_weight_tying=False)}\n",
            "\n",
            "âœ… æ¨¡å‹å‡†å¤‡å®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# ğŸ¤– åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨ LoRA\n",
        "# åŠŸèƒ½ï¼šè¿™æ˜¯ PEFT åº“ä½¿ç”¨çš„æ ¸å¿ƒæ­¥éª¤\n",
        "\n",
        "print(\"ğŸ¤– æ­¥éª¤ 1: åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ğŸ“¥ ä» Hugging Face Hub åŠ è½½ GPT-2 åˆ†ç±»æ¨¡å‹\n",
        "# AutoModelForSequenceClassification ä¼šï¼š\n",
        "# 1. åŠ è½½ GPT-2 çš„é¢„è®­ç»ƒæƒé‡\n",
        "# 2. åœ¨é¡¶éƒ¨æ·»åŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰\n",
        "# 3. num_labels=2 è¡¨ç¤ºäºŒåˆ†ç±»ä»»åŠ¡\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",           # æ¨¡å‹åç§°\n",
        "    num_labels=2      # åˆ†ç±»ç±»åˆ«æ•°\n",
        ")\n",
        "\n",
        "# ğŸ”§ é…ç½®æ¨¡å‹çš„ padding token\n",
        "# å¿…é¡»ä¸ tokenizer çš„é…ç½®ä¿æŒä¸€è‡´\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# ğŸ“ è°ƒæ•´ embedding å±‚å¤§å°\n",
        "# GPT-2 çš„è¯è¡¨å¤§å°æ˜¯ 50257ï¼Œå¦‚æœ tokenizer æ·»åŠ äº†æ–° tokenï¼Œéœ€è¦è°ƒæ•´\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
        "print(f\"   - æ¨¡å‹ç±»å‹: {base_model.__class__.__name__}\")\n",
        "print(f\"   - åˆ†ç±»ç±»åˆ«: {base_model.config.num_labels}\")\n",
        "print(f\"   - è¯è¡¨å¤§å°: {base_model.config.vocab_size:,}\")\n",
        "\n",
        "# ğŸ”§ æ­¥éª¤ 2: åˆ›å»º LoRA é…ç½®\n",
        "print(\"\\nğŸ”§ æ­¥éª¤ 2: åˆ›å»º LoRA é…ç½®...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ğŸ“‹ LoRA é…ç½®å‚æ•°è¯¦è§£\n",
        "lora_config = LoraConfig(\n",
        "    # ğŸ¯ task_type: ä»»åŠ¡ç±»å‹\n",
        "    # - TaskType.SEQ_CLS: åºåˆ—åˆ†ç±»ä»»åŠ¡\n",
        "    # - TaskType.CAUSAL_LM: å› æœè¯­è¨€æ¨¡å‹ï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰\n",
        "    # - TaskType.SEQ_2_SEQ_LM: åºåˆ—åˆ°åºåˆ—ä»»åŠ¡\n",
        "    # ä¸åŒä»»åŠ¡ç±»å‹ä¼šå½±å“ LoRA å±‚çš„æ’å…¥ä½ç½®\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "\n",
        "    # ğŸ“Š r: LoRA çš„ç§©ï¼ˆrankï¼‰\n",
        "    # è¿™æ˜¯ LoRA æœ€æ ¸å¿ƒçš„è¶…å‚æ•°ï¼\n",
        "    # - æ§åˆ¶ A å’Œ B çŸ©é˜µçš„ä¸­é—´ç»´åº¦\n",
        "    # - è¾ƒå°çš„å€¼ï¼ˆå¦‚ 4-8ï¼‰ï¼šå‚æ•°å°‘ï¼Œè®­ç»ƒå¿«ï¼Œä½†è¡¨è¾¾èƒ½åŠ›å¼±\n",
        "    # - è¾ƒå¤§çš„å€¼ï¼ˆå¦‚ 16-32ï¼‰ï¼šå‚æ•°å¤šï¼Œè¡¨è¾¾èƒ½åŠ›å¼ºï¼Œä½†æ¥è¿‘å…¨é‡å¾®è°ƒ\n",
        "    # - æ¨èèµ·å§‹å€¼ï¼š8\n",
        "    # - æœ¬æ•™ç¨‹ä½¿ç”¨ï¼š8ï¼ˆå¹³è¡¡æ•ˆç‡å’Œæ€§èƒ½ï¼‰\n",
        "    r=8,\n",
        "\n",
        "    # ğŸšï¸ lora_alpha: LoRA ç¼©æ”¾å› å­\n",
        "    # - æ§åˆ¶ LoRA è¾“å‡ºçš„å½±å“å¼ºåº¦\n",
        "    # - å¸¸è§è®¾ç½®ï¼šalpha = 2Ã—r æˆ– alpha = r\n",
        "    # - æœ¬æ•™ç¨‹ï¼šalpha = 2Ã—r = 16\n",
        "    # - æ•°å­¦å…¬å¼ï¼šLoRA è¾“å‡ºä¼šä¹˜ä»¥ (alpha/r) çš„ç¼©æ”¾ç³»æ•°\n",
        "    lora_alpha=16,\n",
        "\n",
        "    # ğŸ² lora_dropout: Dropout æ¯”ç‡\n",
        "    # lora_dropout å°±æ˜¯è®­ç»ƒLoRAæ—¶éšæœºè®©éƒ¨åˆ†å‚æ•°â€œä¼‘æ¯â€ï¼Œ\n",
        "    #        é¿å…æ¨¡å‹æ­»è®°ç¡¬èƒŒã€æé«˜æ³›åŒ–èƒ½åŠ›çš„æ­£åˆ™æ‰‹æ®µâ€”â€”é€‚å½“åŠ ä¸€ç‚¹æ›´èªæ˜ï¼Œå¤ªé«˜åè€Œå­¦ä¸ä¼šã€‚\n",
        "    # - åœ¨ LoRA å±‚ä¸­åº”ç”¨ dropout æ­£åˆ™åŒ–\n",
        "    # - èŒƒå›´ï¼š0.0ï¼ˆä¸ä½¿ç”¨ï¼‰åˆ° 0.2ï¼ˆè¾ƒå¼ºæ­£åˆ™åŒ–ï¼‰\n",
        "    # - ä½œç”¨ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›\n",
        "    # - æœ¬æ•™ç¨‹ï¼š0.05ï¼ˆè½»å¾®æ­£åˆ™åŒ–ï¼‰\n",
        "    lora_dropout=0.05,\n",
        "\n",
        "    # ğŸ¯ target_modules: è¦æ›¿æ¢çš„æ¨¡å—åç§°åˆ—è¡¨\n",
        "    # è¿™æ˜¯ PEFT åº“çš„å¼ºå¤§åŠŸèƒ½ï¼šé€‰æ‹©æ€§åœ°åªå¯¹éƒ¨åˆ†å±‚åº”ç”¨ LoRA\n",
        "    # GPT-2 çš„æ¨¡å—åç§°ï¼š\n",
        "    # - \"c_attn\": æ³¨æ„åŠ›å±‚çš„ QKV æŠ•å½±ï¼ˆæœ€é‡è¦ï¼‰\n",
        "    # - \"c_proj\": æ³¨æ„åŠ›å±‚çš„è¾“å‡ºæŠ•å½±\n",
        "    # - \"c_fc\": å‰é¦ˆç½‘ç»œçš„ç¬¬ä¸€å±‚\n",
        "    # - \"c_proj\": å‰é¦ˆç½‘ç»œçš„ç¬¬äºŒå±‚ï¼ˆä¸æ³¨æ„åŠ›åŒåï¼ŒPEFT ä¼šåŒºåˆ†ï¼‰\n",
        "    #\n",
        "    # ğŸ’¡ é€‰æ‹©ç­–ç•¥ï¼š\n",
        "    # - åªæ›¿æ¢ [\"c_attn\"]: æœ€å°‘å‚æ•°ï¼Œé€‚åˆæ˜¾å­˜æå°åœºæ™¯\n",
        "    # - æ›¿æ¢ [\"c_attn\", \"c_proj\"]: å¹³è¡¡é€‰æ‹©\n",
        "    # - æ›¿æ¢ [\"c_attn\", \"c_fc\", \"c_proj\"]: æœ¬æ•™ç¨‹ä½¿ç”¨ï¼Œè¦†ç›–ä¸»è¦å±‚\n",
        "    # - ä¸å»ºè®®æ›¿æ¢ embedding å±‚å’Œåˆ†ç±»å¤´\n",
        "    ## ç»™ GPT-2 çš„æ³¨æ„åŠ› QKVã€æ³¨æ„åŠ›è¾“å‡ºã€MLP å‰åä¸¤å±‚éƒ½åŠ ä¸Š LoRA å°â€œå¤–æŒ‚â€ï¼Œ\n",
        "    ## åªæ”¹è¿™äº› Linearï¼Œå°±èƒ½è®©æ¨¡å‹çš„â€œæ³¨æ„åŠ›ä¹ æƒ¯â€å’Œâ€œç‰¹å¾å˜æ¢æ–¹å¼â€é€‚é…æ–°ä»»åŠ¡ã€‚\n",
        "    target_modules=[\"c_attn\", \"c_fc\", \"c_proj\"],\n",
        "\n",
        "    # âš™ï¸ å…¶ä»–å¯é€‰å‚æ•°ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰ï¼š\n",
        "    # - bias=\"none\": ä¸è®­ç»ƒ bias å‚æ•°\n",
        "    # - fan_in_fan_out=False: æƒé‡çŸ©é˜µçš„ç»„ç»‡æ–¹å¼\n",
        "    # - modules_to_save=None: é™¤ LoRA å¤–éœ€è¦è®­ç»ƒçš„æ¨¡å—\n",
        ")\n",
        "\n",
        "print(\"âœ… LoRA é…ç½®åˆ›å»ºå®Œæˆ:\")\n",
        "print(f\"   - ç§© (r): {lora_config.r}\")\n",
        "print(f\"   - ç¼©æ”¾å› å­ (alpha): {lora_config.lora_alpha}\")\n",
        "print(f\"   - Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"   - ç›®æ ‡æ¨¡å—: {lora_config.target_modules}\")\n",
        "\n",
        "# ğŸš€ æ­¥éª¤ 3: åº”ç”¨ LoRA åˆ°æ¨¡å‹\n",
        "print(\"\\nğŸš€ æ­¥éª¤ 3: åº”ç”¨ LoRA...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ğŸ”‘ æ ¸å¿ƒå‡½æ•°ï¼šget_peft_model()\n",
        "# åŠŸèƒ½ï¼šå°† LoRA é…ç½®åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹\n",
        "# å·¥ä½œåŸç†ï¼š\n",
        "# 1. éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—\n",
        "# 2. æ‰¾åˆ° target_modules ä¸­æŒ‡å®šçš„çº¿æ€§å±‚\n",
        "# 3. ç”¨ LoRA å¢å¼ºç‰ˆæœ¬æ›¿æ¢è¿™äº›å±‚\n",
        "# 4. å†»ç»“åŸå§‹å‚æ•°ï¼Œåªè®© LoRA å‚æ•°å¯è®­ç»ƒ\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "print(\"âœ… LoRA åº”ç”¨å®Œæˆï¼\")\n",
        "\n",
        "# ğŸ“Š æ‰“å°å‚æ•°ç»Ÿè®¡\n",
        "# print_trainable_parameters() æ˜¯ PEFT æ¨¡å‹çš„ä¾¿æ·æ–¹æ³•\n",
        "# ä¼šæ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°æ•°é‡å’Œå æ¯”\n",
        "print(\"\\nğŸ“Š å‚æ•°ç»Ÿè®¡:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ğŸ® å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU/CPU\n",
        "model.to(device)\n",
        "print(f\"\\nğŸ® æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}\")\n",
        "\n",
        "# ğŸ” æ¨¡å‹ç»“æ„æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰\n",
        "print(\"\\nğŸ” æ¨¡å‹ç»“æ„é¢„è§ˆ:\")\n",
        "print(f\"   - æ¨¡å‹ç±»å‹: {type(model).__name__}\")\n",
        "print(f\"   - åŸºç¡€æ¨¡å‹: {type(model.base_model).__name__}\")\n",
        "print(f\"   - LoRA é…ç½®: {model.peft_config}\")\n",
        "\n",
        "print(\"\\nâœ… æ¨¡å‹å‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ğŸ” LoRA åº”ç”¨ç»“æœè§£é‡Š\n",
        "\n",
        "| ç»“æœé¡¹                                    | é€šä¿—å«ä¹‰                                          |\n",
        "| ----------------------------------------- | ------------------------------------------------- |\n",
        "| `trainable params: 1,181,184`             | åªè®­ç»ƒçº¦ 118 ä¸‡å‚æ•°ï¼Œéå¸¸è½»é‡                     |\n",
        "| `all params: 125,622,528`                 | GPT-2 åŸå§‹å‚æ•°çº¦ 1.25 äº¿                          |\n",
        "| `trainable%: 0.9403`                      | åªåŠ¨ **0.94%** çš„å‚æ•°ï¼Œ99% ä¸æ”¹åŠ¨                 |\n",
        "| `cuda`                                    | æ¨¡å‹æˆåŠŸä¸Š GPUï¼Œè®­ç»ƒå¯ä»¥å¼€è·‘                      |\n",
        "| ç±»å‹ `PeftModelForSequenceClassification` | ä½ çš„ GPT-2 ç°åœ¨ç”¨äºåˆ†ç±»ä»»åŠ¡                       |\n",
        "| `LoRA é…ç½®` é‡Œ `c_attn, c_fc, c_proj`     | LoRA å®‰è£…åœ¨æ³¨æ„åŠ›ä¸å‰é¦ˆæ ¸å¿ƒå±‚ï¼Œæœ€å¸¸ç”¨æœ€æœ‰æ•ˆ       |\n",
        "| `lora_dropout=0.05`                       | é˜²æ­¢è¿‡æ‹Ÿåˆçš„å°ä¿æŠ¤ï¼Œä¸è®©æ¨¡å‹â€œèƒŒé¢˜â€                |\n",
        "| `modules_to_save=['classifier','score']`  | é™¤äº† LoRAï¼Œä½ è¿˜ä¿ç•™è®­ç»ƒåˆ†ç±»å¤´å±‚ï¼Œé€‚åˆä¸‹æ¸¸åˆ†ç±»ä»»åŠ¡ |\n",
        "\n",
        "ä¸€å¥è¯æ€»ç»“ï¼š\n",
        "\n",
        "> **ä½ ä¸æ˜¯åœ¨â€œé‡è®­ GPT-2â€ï¼Œè€Œæ˜¯åœ¨å®ƒè„‘å­é‡ŒæŒ‚äº†ä¸ªå°å¤–æŒ‚è®©å®ƒå­¦ä¼šæ–°ä»»åŠ¡ã€‚**"
      ],
      "metadata": {
        "id": "GjBlrcyA9AV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GPT-2çš„è¯è¡¨å’Œå‘é‡ç»´åº¦\n",
        "\n",
        "GPT-2 çš„è¯è¡¨å¤§å°æ˜¯ 50257ï¼Œæ„æ€æ˜¯å®ƒèƒ½ç”¨ **50257 ä¸ªåŸºæœ¬ç¬¦å·ï¼ˆtokenï¼‰** æ¥è¡¨ç¤ºæ‰€æœ‰æ–‡æœ¬ï¼Œæ˜¯è‹±æ–‡é‡Œæœ€å¸¸è§å­—è¯ + å­è¯ + æ ‡ç‚¹ç»„åˆæˆçš„ä¸€å¥—â€œæ‹†è¯ç§¯æœ¨â€ã€‚\n",
        "\n",
        "GPT-2 çš„å‘é‡ç»´åº¦æ˜¯ 768ã€‚æ¯ä¸ªè¯è¡¨éƒ½æœ‰ ä¸€ä¸ª 768 ç»´å‚æ•°æè¿°ï¼ˆembedding å‘é‡ï¼‰\n",
        "\n",
        "#### ä¸€å¼ å›¾ï¼Œå½»åº•ç†è§£ 50257 ä¸ 768 çš„å…³ç³»\n",
        "\n",
        "```\n",
        "                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "                â”‚         GPT-2 è¯è¡¨ (Vocab)    â”‚\n",
        "                â”‚     å¤§å° = 50257 ä¸ª Token     â”‚\n",
        "                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                         â”‚       â”‚       â”‚\n",
        "                         â”‚       â”‚       â”‚  ... å…±æœ‰ 50257 æ¡\n",
        "                         â–¼       â–¼       â–¼\n",
        "                    Token#1  Token#2  Token#3             Token#50257\n",
        "                         â”‚       â”‚       â”‚                     â”‚\n",
        "                         â”‚       â”‚       â”‚                     â”‚\n",
        "                         â–¼       â–¼       â–¼                     â–¼\n",
        "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "          â”‚           Token Embedding Matrix (æŸ¥è¯å…¸ç”¨çš„çŸ©é˜µ)             â”‚\n",
        "          â”‚          å½¢çŠ¶ = 50257 Ã— 768  ï¼ˆè¡Œ=è¯è¡¨æ•°ï¼Œåˆ—=å‘é‡ç»´åº¦ï¼‰        â”‚\n",
        "          â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
        "   Row#1  -> â”‚  [ 0.13 , -0.88 , ... , 0.04 ]  â† 768ç»´                       â”‚\n",
        "   Row#2  -> â”‚  [ 0.51 ,  1.22 , ... , -0.91 ] â† 768ç»´                       â”‚\n",
        "   Row#3  -> â”‚  [ -0.07,  0.33 , ... , 1.85 ]  â† 768ç»´                       â”‚\n",
        "    ...    ->â”‚                                                              â”‚\n",
        "   Row50257->â”‚  [ 0.09 , -0.44 , ... , 0.13 ]  â† 768ç»´                       â”‚\n",
        "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "------\n",
        "\n",
        "#### ç”¨å›¾ä¸€å¥è¯æ€»ç»“\n",
        "\n",
        "> **50257 æ˜¯è¡Œæ•°ï¼ˆæœ‰å¤šå°‘ Tokenï¼‰**\n",
        ">  **768 æ˜¯åˆ—æ•°ï¼ˆæ¯ä¸ª Token å‘é‡æœ‰å¤šé•¿ï¼‰**\n",
        "\n",
        "ğŸ”¸ Token å¤š â†’ æ¨¡å‹æ‡‚çš„è¯å—å¤š\n",
        " ğŸ”¸ ç»´åº¦é«˜ â†’ æ¯ä¸ªè¯å—çš„è¯­ä¹‰è¡¨è¾¾æ›´ç»†è…»\n",
        "\n",
        "ä¸¤è€…äº’ä¸æ›¿ä»£ï¼Œä¹Ÿä¸èƒ½æ··ä¸ºä¸€è°ˆã€‚\n"
      ],
      "metadata": {
        "id": "fTiqtOJGz2d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### target_modulesï¼šä¸ºä»€ä¹ˆé€‰è¿™ä¸‰ä¸ªåå­—ï¼Ÿ\n",
        "\n",
        "```\n",
        "target_modules = [\"c_attn\", \"c_fc\", \"c_proj\"]\n",
        "```\n",
        "\n",
        "è¿™ä¸‰ä¸ªåå­—ï¼Œå¯¹åº”çš„æ˜¯ **GPT-2 Block é‡Œçš„å‡ ä¸ªçº¿æ€§å±‚**ï¼ŒPEFT ä¼šæ ¹æ®åå­—å»â€œåŒ¹é…å¹¶æ›¿æ¢â€ï¼š\n",
        "\n",
        "#### 1. `c_attn` â€” æ³¨æ„åŠ›çš„ QKV æŠ•å½±ï¼ˆæœ€å…³é”®ï¼‰\n",
        "\n",
        "åœ¨ GPT-2 é‡Œï¼Œæ¯ä¸€å±‚çš„è‡ªæ³¨æ„åŠ›æ¨¡å—å¤§è‡´æ˜¯ï¼š\n",
        "\n",
        "- è¾“å…¥ hidden state â†’ ç»è¿‡ `c_attn` è¿™ä¸ª Linear\n",
        "- åœ¨è¿™ä¸ª Linear é‡Œé¢ï¼Œæƒé‡è¢«åˆ†æˆä¸‰å—ï¼šQï¼ˆQueryï¼‰ã€Kï¼ˆKeyï¼‰ã€Vï¼ˆValueï¼‰\n",
        "- æ‰€ä»¥ `c_attn` å…¶å®æ˜¯ä¸€ä¸ª **â€œQKV åˆå¹¶æŠ•å½±å±‚â€**\n",
        "\n",
        "ğŸ‘‰ åœ¨è¿™é‡ŒåŠ  LoRA çš„æ„ä¹‰ï¼š\n",
        "\n",
        "- æ”¹å˜ Q/K/V çš„æŠ•å½±æ–¹å¼ï¼Œç›¸å½“äºæ”¹å˜ **æ³¨æ„åŠ›å¦‚ä½•â€œçœ‹ä¸–ç•Œâ€**\n",
        "- å¯¹æ¨¡å‹è¡Œä¸ºå½±å“éå¸¸å¤§ï¼Œä½†å‚æ•°é‡ç›¸å¯¹å¯æ§\n",
        "- å¾ˆå¤šå®è·µé‡Œï¼Œ**åªå¯¹ `c_attn` åŠ  LoRA æ•ˆæœå°±å¾ˆå¥½ï¼Œå‚æ•°æå°‘**\n",
        "\n",
        "å¯¹åº”ä½ çš„æ³¨é‡Šï¼š\n",
        "\n",
        "> - åªæ›¿æ¢ [\"c_attn\"]: æœ€å°‘å‚æ•°ï¼Œé€‚åˆæ˜¾å­˜æå°åœºæ™¯\n",
        "\n",
        "è¿™æ˜¯æœ€â€œå°æˆæœ¬â€çš„ LoRA å¾®è°ƒæ–¹æ¡ˆã€‚\n",
        "\n",
        "------\n",
        "\n",
        "#### 2. `c_proj` â€” æ³¨æ„åŠ›è¾“å‡ºæŠ•å½± + FFN è¾“å‡ºæŠ•å½±\n",
        "\n",
        "GPT-2 é‡Œæœ‰ä¸¤ä¸ªåŒåçš„ `c_proj`ï¼š\n",
        "\n",
        "1. æ³¨æ„åŠ›æ¨¡å—é‡Œçš„ `c_proj`ï¼š\n",
        "   - æŠŠå¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºï¼Œé‡æ–°æ˜ å°„å›éšè—ç»´åº¦\n",
        "2. å‰é¦ˆç½‘ç»œï¼ˆMLPï¼‰é‡Œçš„ `c_proj`ï¼š\n",
        "   - æŠŠä¸­é—´ç»´åº¦ï¼ˆä¸€èˆ¬æ›´å¤§ï¼‰æ˜ å°„å›éšè—ç»´åº¦\n",
        "\n",
        "è™½ç„¶åå­—éƒ½å« `c_proj`ï¼Œä½†ï¼š\n",
        "\n",
        "- åœ¨æ¨¡å‹å±‚çº§ç»“æ„é‡Œï¼Œå®ƒä»¬æœ‰ **ä¸åŒçš„è·¯å¾„**ï¼ˆç±»ä¼¼ `h.0.attn.c_proj` / `h.0.mlp.c_proj`ï¼‰\n",
        "- PEFT æ˜¯æ ¹æ® **å®Œæ•´æ¨¡å—è·¯å¾„** åŒºåˆ†çš„ï¼Œä¸ä¼šææ··ï¼Œåªè¦åå­—åŒ¹é…å°±ä¼šå„è‡ªåŠ  LoRA\n",
        "\n",
        "ğŸ‘‰ åœ¨è¿™é‡ŒåŠ  LoRA çš„æ„ä¹‰ï¼š\n",
        "\n",
        "- æ³¨æ„åŠ› `c_proj`ï¼š\n",
        "  - å†³å®šæ³¨æ„åŠ›è¾“å‡ºå¦‚ä½•â€œèåˆã€å›æµâ€åˆ°ä¸»é€šé“\n",
        "- MLP `c_proj`ï¼š\n",
        "  - å†³å®šéçº¿æ€§å˜æ¢ä¹‹åå¦‚ä½•å›åˆ°åŸç»´åº¦\n",
        "\n",
        "æ‰€ä»¥ï¼š\n",
        "\n",
        "> - æ›¿æ¢ [\"c_attn\", \"c_proj\"]: å¹³è¡¡é€‰æ‹©\n",
        "\n",
        "è¿™æ—¶å€™ LoRA **è¦†ç›–äº†æ³¨æ„åŠ›çš„è¾“å…¥ï¼ˆQKVï¼‰+ è¾“å‡ºï¼ˆc_projï¼‰+ MLP è¾“å‡º**ï¼Œ\n",
        " æ—¢æ§åˆ¶â€œçœ‹å“ªé‡Œâ€ï¼ˆæ³¨æ„åŠ›ï¼‰åˆæ§åˆ¶â€œæœ€åæ€ä¹ˆæ•´åˆâ€ã€‚\n",
        "\n",
        "------\n",
        "\n",
        "#### 3. `c_fc` â€” FFNï¼ˆå‰é¦ˆç½‘ç»œï¼‰çš„ç¬¬ä¸€å±‚\n",
        "\n",
        "åœ¨æ¯ä¸ª Transformer Block çš„ MLP é‡Œï¼š\n",
        "\n",
        "- `c_fc`: hidden_dim â†’ 4 * hidden_dimï¼ˆæ‰©å¤§ç»´åº¦ï¼‰\n",
        "- æ¿€æ´»å‡½æ•°ï¼ˆæ¯”å¦‚ GELUï¼‰\n",
        "- `c_proj`: 4 * hidden_dim â†’ hidden_dimï¼ˆç¼©å›å»ï¼‰\n",
        "\n",
        "`c_fc` å†³å®šäº†ï¼š\n",
        "\n",
        "- æ¨¡å‹åœ¨â€œå• token ç»´åº¦â€ä¸Šçš„éçº¿æ€§å˜æ¢å‰ï¼Œå¦‚ä½•ç»„åˆç‰¹å¾\n",
        "- è¿™ä¸ªå±‚æ”¹å˜çš„æ˜¯ **ç‰¹å¾ç©ºé—´çš„â€œå±•å¼€æ–¹å¼â€**\n",
        "\n",
        "åœ¨ `c_fc` ä¸ŠåŠ  LoRAï¼š\n",
        "\n",
        "- **å¯ä»¥æ›´çµæ´»åœ°è°ƒæ•´â€œç‰¹å¾ç»„åˆæ–¹å¼â€**\n",
        "- **å¯¹ä¸‹æ¸¸ä»»åŠ¡ä¸­å¤æ‚æ¨¡å¼çš„æ‹Ÿåˆæ›´å¼º**\n",
        "\n",
        "å› æ­¤ä½ è¿™å¥—é…ç½®ï¼š\n",
        "\n",
        "> - æ›¿æ¢ [\"c_attn\", \"c_fc\", \"c_proj\"]: æœ¬æ•™ç¨‹ä½¿ç”¨ï¼Œè¦†ç›–ä¸»è¦å±‚\n",
        "\n",
        "å®é™…ä¸Šæ˜¯ï¼š**æ³¨æ„åŠ› + MLP çš„æ ¸å¿ƒçº¿æ€§å±‚éƒ½åŠ äº†ä¸€é LoRA**\n",
        " â†’ å…¼é¡¾æ•ˆæœå’Œå‚æ•°é‡ï¼Œæ˜¯æ•™ç¨‹é‡Œéå¸¸å…¸å‹çš„ä¸€ç§â€œå…¨å±€ä½†ä¸å¤¸å¼ â€çš„æ–¹æ¡ˆã€‚\n",
        "\n",
        "------\n",
        "\n",
        "#### 4. ä¸ºå•¥â€œä¸å»ºè®®æ›¿æ¢ embedding å±‚å’Œåˆ†ç±»å¤´â€ï¼Ÿ\n",
        "\n",
        "- **Embeddingï¼ˆè¯å‘é‡å±‚ï¼‰**ï¼š\n",
        "  - å‚æ•°é‡å·¨å¤§ï¼ˆvocab_size Ã— hidden_dimï¼‰\n",
        "  - **è¿™å±‚æ›´å¤šæ˜¯â€œè¯­ä¹‰åæ ‡ç³»â€**ï¼ŒåŠ¨å¤ªå¤šå®¹æ˜“ç ´åé¢„è®­ç»ƒå­¦åˆ°çš„é€šç”¨è¯­ä¹‰ç©ºé—´\n",
        "  - ç›´æ¥ full fine-tune è¿™å±‚éƒ½è¦å°å¿ƒï¼Œæ›´ä¸ç”¨è¯´ä¹±æ’ LoRA\n",
        "- **åˆ†ç±»å¤´ï¼ˆlm_head / output headï¼‰**ï¼š\n",
        "  - é€šå¸¸æ˜¯ä¸€ä¸ª Linearï¼šhidden_dim â†’ vocab_size æˆ–ç±»åˆ«æ•°\n",
        "  - å¯¹ç”Ÿæˆ/åˆ†ç±»è¡Œä¸ºå½±å“æå¤§\n",
        "  - å¦‚æœä½ éœ€è¦æ”¹åˆ†ç±»å¤´ï¼Œé€šå¸¸æ˜¯ **ç›´æ¥è®­ç»ƒè¿™ä¸ªå¤´æœ¬èº«**ï¼Œè€Œä¸æ˜¯åŠ  LoRA\n",
        "\n",
        "LoRA çš„å…¸å‹å“²å­¦æ˜¯ï¼š\n",
        "\n",
        "> â€œå°½é‡åœ¨ **ä¸­é—´è¡¨å¾å±‚** æ”¹ï¼Œè€Œä¸æ˜¯åœ¨ **è¾“å…¥/è¾“å‡ºæç«¯** å¤§åŠ¨å¹²æˆˆâ€\n",
        "\n",
        "####  ç»¼åˆç»“è®º\n",
        "\n",
        "> `c_attn` è®©æ¨¡å‹å­¦ä¼š**æ–°å…³æ³¨ç‚¹**ï¼Œ\n",
        ">  `c_proj` è®©æ¨¡å‹å­¦ä¼š**æ–°è¾“å‡ºè¡¨è¾¾æ–¹å¼**ï¼Œ\n",
        ">  `c_fc` è®©æ¨¡å‹è·å¾—**æ–°æ€è€ƒèƒ½åŠ›**ã€‚\n",
        "\n",
        "ä¸‰è€…ä¸€èµ·è¦†ç›–äº† **ä¿¡æ¯è·å– â†’ å¤„ç† â†’ è¾“å‡º** å…¨é“¾è·¯ï¼Œ\n",
        "\n",
        "#### æ¢æ¨¡å‹ä¹Ÿèƒ½å‡†ç¡®é€‰ target_modules\n",
        "\n",
        "ä¸åŒæ¨¡å‹æ¨¡å—åä¸ä¸€æ ·ï¼Œä½† Transformer ç»“æ„é«˜åº¦ç»Ÿä¸€ã€‚é€‰æ‹©ç›®æ ‡æ¨¡å—ä¸é è®°åå­—ï¼Œè€Œé **è¯†åˆ«ä¸‰ç±»å…³é”®çº¿æ€§å±‚**ï¼š\n",
        "\n",
        "| å…³é”®ç±»åˆ«                          | åŠŸèƒ½                        | LoRA åº”æ”¾åœ¨å“ªï¼Ÿ                                              |\n",
        "| --------------------------------- | --------------------------- | ------------------------------------------------------------ |\n",
        "| **Attention QKV è¾“å…¥æŠ•å½±**        | æ¨¡å‹å¦‚ä½•å…³æ³¨ä¿¡æ¯            | `q_proj / k_proj / v_proj / in_proj / c_attn`                |\n",
        "| **Attention è¾“å‡ºæŠ•å½±**            | å¤šå¤´attentionåˆå¹¶ä¸è¾“å‡ºå›æµ | `o_proj / out_proj / c_proj / attn.out_proj`                 |\n",
        "| **FeedForward å‰å‘æ‰©å¼  + å‹ç¼©å±‚** | æŠ½è±¡æ¨ç† & ç‰¹å¾ç»„åˆ         | `fc1 / fc2 / gate_proj / up_proj / down_proj / c_fc / mlp.c_proj` |\n",
        "\n",
        "#### ğŸ“Œ å¸¸è§æ¨¡å‹å¯¹åº” target_modules å‚è€ƒå¯¹ç…§è¡¨\n",
        "\n",
        "| æ¨¡å‹                                             | æ¨è target_modules                                          |\n",
        "| ------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| **GPT-2 / GPT-Neo / GPT-J**                      | `[\"c_attn\", \"c_proj\", \"c_fc\"]`                               |\n",
        "| **LLaMA / Qwen / Baichuan / InternLM / Mistral** | `[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] + [\"up_proj\",\"down_proj\",\"gate_proj\"]` |\n",
        "| **BERT / RoBERTa**                               | `[\"query\",\"key\",\"value\",\"output.dense\"] + [\"intermediate.dense\",\"output.dense\"]` |\n",
        "| **T5 / Flan-T5**                                 | `[\"q\",\"k\",\"v\",\"o\"] + [\"wi\",\"wo\"]`ï¼ˆéƒ¨åˆ†æ¨¡å‹æ‹†æˆ `wi_0 + wi_1`ï¼‰ |\n",
        "| **CLIP / ViTæ¨¡å‹**                               | ä¾ç„¶æ˜¯ QKV + FFNï¼š`[\"qkv\",\"proj\",\"fc1\",\"fc2\"]`               |\n",
        "\n",
        "ğŸ“ ä½ ä¼šå‘ç°ï¼šæ— è®ºæ˜¯è¯­è¨€ã€è§†è§‰è¿˜æ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Œæœ¬è´¨é€‰ç‚¹ä¸å˜ã€‚\n",
        "\n",
        "------\n",
        "\n",
        "#### ğŸ§  é€šç”¨å†³ç­–æµç¨‹ï¼ˆæœ€å…³é”®ï¼‰\n",
        "\n",
        "æœªæ¥é‡åˆ°ä»»ä½•æ–°æ¶æ„ï¼Œåªè¦ç…§è¿™ä¸ªæµç¨‹èµ°ï¼š\n",
        "\n",
        "```\n",
        "1. æ‰¾ Attention æ¨¡å—\n",
        "   - æœ‰ Q/K/V çš„ Linear â†’ å¿…æ”¾\n",
        "   - æœ‰è¾“å‡ºæŠ•å½± â†’ é€šå¸¸ä¹Ÿæ”¾\n",
        "\n",
        "2. æ‰¾ FeedForward (MLP) æ¨¡å—\n",
        "   - æ‰©å¼ å±‚ up_proj / wi / fc1\n",
        "   - å›ç¼©å±‚ down_proj / wo / fc2\n",
        "   â†’ æ¨èè‡³å°‘æ”¾å…¶ä¸­ä¸€æ®µï¼Œæ•ˆæœæ˜¾è‘—æå‡\n",
        "\n",
        "3. æ‰©å±•é€‰æ‹©ï¼ˆè¿›é˜¶ï¼‰\n",
        "   - æƒ³æ›´å¼ºï¼šMQA/Grouped-Queryã€RMSNormå¯åŠ å…¥\n",
        "   - æƒ³æ›´è½»ï¼šåªæ’ Attentionï¼Œä¸åŠ¨ MLP\n",
        "```\n",
        "\n",
        "**ä¸€å¥è¯è®°ä½**ï¼š\n",
        "\n",
        "> æ— è®ºæ¨¡å‹å«ä»€ä¹ˆï¼ŒLoRAåªåŠ åœ¨ *Attention + MLPçš„çº¿æ€§å±‚*ï¼Œ\n",
        ">  æ¢åå­—åªæ˜¯æ¢çš®è‚¤ï¼Œä½œç”¨æ°¸è¿œä¸€è‡´ã€‚"
      ],
      "metadata": {
        "id": "EwT18Hag3Knd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KGarAk35kgN"
      },
      "source": [
        "# 6ï¸âƒ£ è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°\n",
        "\n",
        "## ğŸ¯ è®­ç»ƒæµç¨‹æ¦‚è¿°\n",
        "\n",
        "æ ‡å‡†çš„æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹åŒ…æ‹¬ï¼š\n",
        "1. **è®­ç»ƒå¾ªç¯**ï¼šå‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°\n",
        "2. **è¯„ä¼°å¾ªç¯**ï¼šåœ¨éªŒè¯é›†ä¸Šè®¡ç®—æ€§èƒ½æŒ‡æ ‡\n",
        "3. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡\n",
        "4. **æ—©åœæœºåˆ¶**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "> æ—©åœï¼ˆEarly Stoppingï¼‰å°±æ˜¯åœ¨è®­ç»ƒè¿˜æ²¡è¿‡åº¦æ‹Ÿåˆå‰æå‰åœæ­¢è®­ç»ƒï¼Œé¿å…æ¨¡å‹è¶Šå­¦è¶Šâ€œèƒŒç­”æ¡ˆâ€ã€æ€§èƒ½å˜å·®ã€‚ **å½“éªŒè¯é›†è¡¨ç°ä¸å†æå‡æ—¶ï¼Œå°±æå‰è®©æ¨¡å‹åœæ­¢è®­ç»ƒã€‚**\n",
        "\n",
        "\n",
        "## ğŸ“Š PEFT æ¨¡å‹çš„è®­ç»ƒç‰¹ç‚¹\n",
        "\n",
        "ä½¿ç”¨ PEFT åï¼Œè®­ç»ƒä»£ç ä¸æ™®é€š PyTorch æ¨¡å‹**å®Œå…¨ç›¸åŒ**ï¼\n",
        "- âœ… è‡ªåŠ¨åªä¼˜åŒ– LoRA å‚æ•°\n",
        "- âœ… è‡ªåŠ¨å†»ç»“åŸå§‹å‚æ•°\n",
        "- âœ… æ— éœ€æ‰‹åŠ¨ç­›é€‰å‚æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8ZBEAGB5kgN",
        "outputId": "638beada-b0e4-4b22-b279-a749c0290c87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°å®šä¹‰å®Œæˆ\n"
          ]
        }
      ],
      "source": [
        "# ğŸ“Š è¯„ä¼°å‡½æ•°å®šä¹‰\n",
        "# åŠŸèƒ½ï¼šè®¡ç®—æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡å’ŒæŸå¤±\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    \"\"\"\n",
        "    è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
        "\n",
        "    ğŸ¯ ç›®çš„ï¼šåœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè®¡ç®—å‡†ç¡®ç‡å’Œå¹³å‡æŸå¤±\n",
        "\n",
        "    å‚æ•°ï¼š\n",
        "        model: å¾…è¯„ä¼°çš„æ¨¡å‹\n",
        "        data_loader: DataLoader å¯¹è±¡ï¼ˆéªŒè¯é›†æˆ–æµ‹è¯•é›†ï¼‰\n",
        "\n",
        "    è¿”å›ï¼š\n",
        "        acc: å‡†ç¡®ç‡ (0-1 ä¹‹é—´)\n",
        "        avg_loss: å¹³å‡æŸå¤±å€¼\n",
        "\n",
        "    å·¥ä½œæµç¨‹ï¼š\n",
        "        1. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (model.eval())\n",
        "        2. ç¦ç”¨æ¢¯åº¦è®¡ç®— (torch.no_grad())\n",
        "        3. éå†æ‰€æœ‰æ‰¹æ¬¡è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡\n",
        "        4. è¿”å›å¹³å‡å€¼\n",
        "    \"\"\"\n",
        "    # ğŸ”§ åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
        "    # ä½œç”¨ï¼š\n",
        "    # 1. ç¦ç”¨ Dropoutï¼ˆæ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸è®¡ç®—ï¼‰\n",
        "    # 2. ç¦ç”¨ Batch Normalization çš„æ›´æ–°ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„ç»Ÿè®¡é‡ï¼‰\n",
        "    # 3. æŸäº›å±‚çš„è¡Œä¸ºä¼šæ”¹å˜\n",
        "    model.eval()\n",
        "\n",
        "    # ğŸ“Š åˆå§‹åŒ–ç»Ÿè®¡å˜é‡\n",
        "    total_loss = 0.0    # ç´¯ç§¯æŸå¤±\n",
        "    correct = 0         # æ­£ç¡®é¢„æµ‹æ•°\n",
        "    total = 0           # æ€»æ ·æœ¬æ•°\n",
        "\n",
        "    # ğŸš« ç¦ç”¨æ¢¯åº¦è®¡ç®—\n",
        "    # ä½œç”¨ï¼šèŠ‚çœæ˜¾å­˜ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦\n",
        "    # è¯„ä¼°æ—¶ä¸éœ€è¦æ¢¯åº¦ï¼Œå› ä¸ºä¸è¿›è¡Œå‚æ•°æ›´æ–°\n",
        "    with torch.no_grad():\n",
        "        # ğŸ”„ éå†æ•°æ®æ‰¹æ¬¡\n",
        "        for batch in data_loader:\n",
        "            # ğŸ® å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPU/CPUï¼‰\n",
        "            # å­—å…¸æ¨å¯¼å¼ï¼šå°† batch ä¸­çš„æ¯ä¸ªå¼ é‡éƒ½ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # ğŸ”® å‰å‘ä¼ æ’­\n",
        "            # model(**batch) ç­‰ä»·äº model(input_ids=..., attention_mask=..., labels=...)\n",
        "            # Hugging Face æ¨¡å‹è¿”å›ä¸€ä¸ªåŒ…å« loss å’Œ logits çš„å¯¹è±¡\n",
        "            outputs = model(**batch)\n",
        "\n",
        "            # ğŸ“‰ æå–æŸå¤±å€¼\n",
        "            # å½“ä¼ å…¥ labels æ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è®¡ç®—äº¤å‰ç†µæŸå¤±\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # ğŸ¯ è·å–é¢„æµ‹ç»“æœ\n",
        "            # logits: [batch_size, num_labels] å½¢çŠ¶çš„å¼ é‡\n",
        "            # argmax(dim=-1): æ²¿æœ€åä¸€ç»´å–æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå¾—åˆ°é¢„æµ‹ç±»åˆ«\n",
        "            preds = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "            # ğŸ“Š ç´¯ç§¯ç»Ÿè®¡é‡\n",
        "            total_loss += loss.item()  # ç´¯åŠ æŸå¤±ï¼ˆè½¬ä¸º Python æ•°å€¼ï¼‰\n",
        "            correct += (preds == batch[\"labels\"]).sum().item()  # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹æ•°\n",
        "            total += batch[\"labels\"].size(0)  # ç´¯åŠ æ ·æœ¬æ•°\n",
        "\n",
        "    # ğŸ“ è®¡ç®—å¹³å‡å€¼\n",
        "    avg_loss = total_loss / max(len(data_loader), 1)  # å¹³å‡æŸå¤±\n",
        "    acc = correct / total if total > 0 else 0.0        # å‡†ç¡®ç‡\n",
        "\n",
        "    return acc, avg_loss\n",
        "\n",
        "\n",
        "# ğŸš€ è®­ç»ƒå‡½æ•°å®šä¹‰\n",
        "# åŠŸèƒ½ï¼šæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=3, lr=5e-4):\n",
        "    \"\"\"\n",
        "    è®­ç»ƒ LoRA æ¨¡å‹\n",
        "\n",
        "    ğŸ¯ ç›®çš„ï¼šæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å’Œæ¢¯åº¦è£å‰ª\n",
        "\n",
        "    å‚æ•°ï¼š\n",
        "        model: PEFT æ¨¡å‹ï¼ˆå·²åº”ç”¨ LoRAï¼‰\n",
        "        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
        "        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨\n",
        "        epochs: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 3ï¼‰\n",
        "        lr: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 5e-4ï¼Œå³0.0005ï¼‰\n",
        "\n",
        "    è®­ç»ƒæµç¨‹ï¼š\n",
        "        æ¯ä¸ª epoch:\n",
        "            1. è®­ç»ƒé˜¶æ®µï¼šéå†è®­ç»ƒé›†ï¼Œæ›´æ–°å‚æ•°\n",
        "            2. è¯„ä¼°é˜¶æ®µï¼šåœ¨éªŒè¯é›†ä¸Šè®¡ç®—æ€§èƒ½\n",
        "            3. å­¦ä¹ ç‡è°ƒåº¦ï¼šæ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡\n",
        "\n",
        "    ğŸ’¡ LoRA å¾®è°ƒçš„å­¦ä¹ ç‡å»ºè®®ï¼š\n",
        "    - å…¨é‡å¾®è°ƒï¼š1e-5 ~ 5e-5ï¼ˆå°å­¦ä¹ ç‡ï¼‰\n",
        "    - LoRA å¾®è°ƒï¼š5e-4 ~ 1e-3ï¼ˆå¯ä»¥ç”¨è¾ƒå¤§å­¦ä¹ ç‡ï¼‰\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ å¼€å§‹è®­ç»ƒ...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # âš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨\n",
        "    # AdamW: Adam ä¼˜åŒ–å™¨çš„æ”¹è¿›ç‰ˆï¼Œå¸¦æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰\n",
        "    # model.parameters() ä¼šè‡ªåŠ¨åªè¿”å› requires_grad=True çš„å‚æ•°\n",
        "    # å› æ­¤è¿™é‡Œä¼šè‡ªåŠ¨åªä¼˜åŒ– LoRA å‚æ•°ï¼\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "    print(f\"âš™ï¸  ä¼˜åŒ–å™¨: AdamW (lr={lr})\")\n",
        "\n",
        "    # ğŸ“… åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨\n",
        "    # get_linear_schedule_with_warmup: å¸¦é¢„çƒ­çš„çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦\n",
        "    #\n",
        "    # ğŸ”¥ Warmupï¼ˆé¢„çƒ­ï¼‰çš„ä½œç”¨ï¼š\n",
        "    # - è®­ç»ƒåˆæœŸä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œé€æ¸å¢åŠ åˆ°ç›®æ ‡å­¦ä¹ ç‡\n",
        "    # - é˜²æ­¢è®­ç»ƒåˆæœŸæ¢¯åº¦è¿‡å¤§å¯¼è‡´çš„ä¸ç¨³å®š\n",
        "    # - å…¬å¼ï¼šlr = base_lr * (current_step / warmup_steps)\n",
        "    #\n",
        "    # ğŸ“‰ çº¿æ€§è¡°å‡ï¼š\n",
        "    # - Warmup ç»“æŸåï¼Œå­¦ä¹ ç‡çº¿æ€§é™ä½åˆ° 0\n",
        "    # - æœ‰åŠ©äºè®­ç»ƒåæœŸçš„ç²¾ç»†è°ƒæ•´\n",
        "    total_steps = epochs * len(train_loader)  # æ€»è®­ç»ƒæ­¥æ•°\n",
        "    warmup_steps = max(10, int(0.1 * total_steps))  # Warmup æ­¥æ•°ï¼ˆ10% çš„æ€»æ­¥æ•°ï¼‰\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "\n",
        "    print(f\"ğŸ“… å­¦ä¹ ç‡è°ƒåº¦å™¨: Linear with Warmup\")\n",
        "    print(f\"   - æ€»æ­¥æ•°: {total_steps}\")\n",
        "    print(f\"   - Warmup æ­¥æ•°: {warmup_steps}\")\n",
        "\n",
        "    # ğŸ”„ è®­ç»ƒå¾ªç¯\n",
        "    for epoch in range(epochs):\n",
        "        # ğŸ“š è®­ç»ƒé˜¶æ®µ\n",
        "        model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # ğŸ“Š ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        for batch in pbar:\n",
        "            # ğŸ® æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            # ğŸ§¹ æ¸…ç©ºæ¢¯åº¦\n",
        "            # PyTorch é»˜è®¤ç´¯ç§¯æ¢¯åº¦ï¼Œå¿…é¡»æ‰‹åŠ¨æ¸…é›¶\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # ğŸ”® å‰å‘ä¼ æ’­\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "\n",
        "            # ğŸ”„ åå‘ä¼ æ’­\n",
        "            # è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦\n",
        "            loss.backward()\n",
        "\n",
        "            # âœ‚ï¸ æ¢¯åº¦è£å‰ª\n",
        "            # é™åˆ¶æ¢¯åº¦èŒƒæ•°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
        "            # max_norm=1.0: å¦‚æœæ¢¯åº¦èŒƒæ•° > 1.0ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾åˆ° 1.0\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            # âš¡ å‚æ•°æ›´æ–°\n",
        "            optimizer.step()      # ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°\n",
        "            scheduler.step()      # æ›´æ–°å­¦ä¹ ç‡\n",
        "\n",
        "            # ğŸ“Š ç´¯ç§¯æŸå¤±\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # ğŸ“ˆ æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        # ğŸ“Š è®¡ç®—æœ¬è½®å¹³å‡æŸå¤±\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "\n",
        "        # ğŸ“ˆ éªŒè¯é˜¶æ®µ\n",
        "        val_acc, val_loss = evaluate(model, val_loader)\n",
        "\n",
        "        # ğŸ“¢ æ‰“å°ç»“æœ\n",
        "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"   - è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}\")\n",
        "        print(f\"   - éªŒè¯æŸå¤±: {val_loss:.4f}\")\n",
        "        print(f\"   - éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.1f}%\")\n",
        "        print(f\"   - å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "\n",
        "    print(\"\\nâœ… è®­ç»ƒå®Œæˆï¼\")\n",
        "\n",
        "\n",
        "# ğŸ“ åˆå­¦è€…æç¤ºï¼š\n",
        "# ä»¥ä¸Šè®­ç»ƒå‡½æ•°é›†æˆäº†å¤šä¸ªè®­ç»ƒæŠ€å·§ï¼š\n",
        "# 1. AdamW ä¼˜åŒ–å™¨ï¼šæ¯” SGD æ”¶æ•›æ›´å¿«ï¼Œæ¯” Adam æ³›åŒ–æ›´å¥½\n",
        "# 2. å­¦ä¹ ç‡é¢„çƒ­ï¼šè®­ç»ƒåˆæœŸç¨³å®šæ€§æ›´å¥½\n",
        "# 3. æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
        "# 4. è¿›åº¦æ¡æ˜¾ç¤ºï¼šå®æ—¶ç›‘æ§è®­ç»ƒçŠ¶æ€\n",
        "#\n",
        "# è¿™äº›éƒ½æ˜¯å·¥ä¸šç•Œçš„æ ‡å‡†åšæ³•ï¼Œå»ºè®®åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ä½¿ç”¨ã€‚\n",
        "\n",
        "print(\"âœ… è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaOHSKxX5kgN",
        "outputId": "042747af-da5f-460c-95f1-d4b1249a6da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸  è®­ç»ƒå‚æ•°:\n",
            "   - è®­ç»ƒè½®æ•°: 3\n",
            "   - å­¦ä¹ ç‡: 0.0005\n",
            "   - æ‰¹æ¬¡å¤§å°: 8\n",
            "   - ä¼˜åŒ–å™¨: AdamW\n",
            "   - å­¦ä¹ ç‡è°ƒåº¦: Linear with Warmup\n",
            "\n",
            "ğŸš€ å¼€å§‹è®­ç»ƒ...\n",
            "============================================================\n",
            "âš™ï¸  ä¼˜åŒ–å™¨: AdamW (lr=0.0005)\n",
            "ğŸ“… å­¦ä¹ ç‡è°ƒåº¦å™¨: Linear with Warmup\n",
            "   - æ€»æ­¥æ•°: 393\n",
            "   - Warmup æ­¥æ•°: 39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:22<00:00,  5.91it/s, loss=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3:\n",
            "   - è®­ç»ƒæŸå¤±: 0.7324\n",
            "   - éªŒè¯æŸå¤±: 0.0785\n",
            "   - éªŒè¯å‡†ç¡®ç‡: 98.0%\n",
            "   - å½“å‰å­¦ä¹ ç‡: 3.70e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:20<00:00,  6.54it/s, loss=0.0000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/3:\n",
            "   - è®­ç»ƒæŸå¤±: 0.1346\n",
            "   - éªŒè¯æŸå¤±: 0.0472\n",
            "   - éªŒè¯å‡†ç¡®ç‡: 98.0%\n",
            "   - å½“å‰å­¦ä¹ ç‡: 1.85e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:18<00:00,  7.03it/s, loss=0.0001]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/3:\n",
            "   - è®­ç»ƒæŸå¤±: 0.0675\n",
            "   - éªŒè¯æŸå¤±: 0.0406\n",
            "   - éªŒè¯å‡†ç¡®ç‡: 98.0%\n",
            "   - å½“å‰å­¦ä¹ ç‡: 0.00e+00\n",
            "\n",
            "âœ… è®­ç»ƒå®Œæˆï¼\n",
            "\n",
            "ğŸ“Š æœ€ç»ˆæµ‹è¯•...\n",
            "============================================================\n",
            "âœ… æµ‹è¯•ç»“æœ:\n",
            "   - æµ‹è¯•æŸå¤±: 0.0422\n",
            "   - æµ‹è¯•å‡†ç¡®ç‡: 99.00%\n",
            "\n",
            "ğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹è¾¾åˆ°äº† 95%+ çš„å‡†ç¡®ç‡ï¼\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ æ‰§è¡Œè®­ç»ƒ\n",
        "# åŠŸèƒ½ï¼šè°ƒç”¨è®­ç»ƒå‡½æ•°å¼€å§‹å¾®è°ƒ\n",
        "\n",
        "# âš™ï¸ è®­ç»ƒå‚æ•°è®¾ç½®\n",
        "EPOCHS = 3          # è®­ç»ƒè½®æ•°\n",
        "LEARNING_RATE = 5e-4  # å­¦ä¹ ç‡\n",
        "\n",
        "print(\"âš™ï¸  è®­ç»ƒå‚æ•°:\")\n",
        "print(f\"   - è®­ç»ƒè½®æ•°: {EPOCHS}\")\n",
        "print(f\"   - å­¦ä¹ ç‡: {LEARNING_RATE}\")\n",
        "print(f\"   - æ‰¹æ¬¡å¤§å°: 8\")\n",
        "print(f\"   - ä¼˜åŒ–å™¨: AdamW\")\n",
        "print(f\"   - å­¦ä¹ ç‡è°ƒåº¦: Linear with Warmup\")\n",
        "print(\"\")\n",
        "\n",
        "# ğŸš€ å¼€å§‹è®­ç»ƒ\n",
        "train(model, train_loader, val_loader, epochs=EPOCHS, lr=LEARNING_RATE)\n",
        "\n",
        "# ğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
        "print(\"\\nğŸ“Š æœ€ç»ˆæµ‹è¯•...\")\n",
        "print(\"=\" * 60)\n",
        "test_acc, test_loss = evaluate(model, test_loader)\n",
        "\n",
        "print(f\"âœ… æµ‹è¯•ç»“æœ:\")\n",
        "print(f\"   - æµ‹è¯•æŸå¤±: {test_loss:.4f}\")\n",
        "print(f\"   - æµ‹è¯•å‡†ç¡®ç‡: {test_acc*100:.2f}%\")\n",
        "\n",
        "# ğŸ¯ æ€§èƒ½åˆ†æ\n",
        "if test_acc >= 0.95:\n",
        "    print(\"\\nğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹è¾¾åˆ°äº† 95%+ çš„å‡†ç¡®ç‡ï¼\")\n",
        "elif test_acc >= 0.90:\n",
        "    print(\"\\nğŸ‘ å¾ˆå¥½ï¼æ¨¡å‹è¾¾åˆ°äº† 90%+ çš„å‡†ç¡®ç‡ï¼\")\n",
        "elif test_acc >= 0.85:\n",
        "    print(\"\\nâœ… ä¸é”™ï¼æ¨¡å‹è¾¾åˆ°äº† 85%+ çš„å‡†ç¡®ç‡ï¼\")\n",
        "else:\n",
        "    print(\"\\nğŸ’¡ æç¤ºï¼šå‡†ç¡®ç‡åä½ï¼Œå»ºè®®è°ƒæ•´è¶…å‚æ•°æˆ–å¢åŠ è®­ç»ƒè½®æ•°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGoDGjov5kgN"
      },
      "source": [
        "# 7ï¸âƒ£ å®é™…åº”ç”¨ï¼šæ–‡æœ¬åˆ†ç±»æ¨ç†\n",
        "\n",
        "## ğŸ¯ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
        "\n",
        "ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juft7yHg5kgN",
        "outputId": "41309ad4-304d-4485-e1ed-19d0c494aace"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”® æ¨ç†æ¼”ç¤º\n",
            "============================================================\n",
            "æµ‹è¯• 6 æ¡æ ·æœ¬...\n",
            "\n",
            "ã€æ ·æœ¬ 1ã€‘\n",
            "æ–‡æœ¬: Hey, want to grab lunch together?\n",
            "é¢„æµ‹: HAM\n",
            "ç½®ä¿¡åº¦: ham=1.000, spam=0.000\n",
            "âœ… åˆ†ç±»ç»“æœï¼šæ­£å¸¸çŸ­ä¿¡\n",
            "\n",
            "ã€æ ·æœ¬ 2ã€‘\n",
            "æ–‡æœ¬: URGENT! You've won $5000! Click now to claim your prize!\n",
            "é¢„æµ‹: SPAM\n",
            "ç½®ä¿¡åº¦: ham=0.000, spam=1.000\n",
            "ğŸš¨ åˆ†ç±»ç»“æœï¼šåƒåœ¾çŸ­ä¿¡\n",
            "\n",
            "ã€æ ·æœ¬ 3ã€‘\n",
            "æ–‡æœ¬: The meeting has been moved to 2pm\n",
            "é¢„æµ‹: SPAM\n",
            "ç½®ä¿¡åº¦: ham=0.228, spam=0.772\n",
            "ğŸš¨ åˆ†ç±»ç»“æœï¼šåƒåœ¾çŸ­ä¿¡\n",
            "\n",
            "ã€æ ·æœ¬ 4ã€‘\n",
            "æ–‡æœ¬: Free iPhone! Limited time offer! Call immediately!\n",
            "é¢„æµ‹: SPAM\n",
            "ç½®ä¿¡åº¦: ham=0.000, spam=1.000\n",
            "ğŸš¨ åˆ†ç±»ç»“æœï¼šåƒåœ¾çŸ­ä¿¡\n",
            "\n",
            "ã€æ ·æœ¬ 5ã€‘\n",
            "æ–‡æœ¬: Thanks for your help yesterday\n",
            "é¢„æµ‹: HAM\n",
            "ç½®ä¿¡åº¦: ham=0.982, spam=0.018\n",
            "âœ… åˆ†ç±»ç»“æœï¼šæ­£å¸¸çŸ­ä¿¡\n",
            "\n",
            "ã€æ ·æœ¬ 6ã€‘\n",
            "æ–‡æœ¬: Congratulations! You are selected as winner. Text WIN now!\n",
            "é¢„æµ‹: SPAM\n",
            "ç½®ä¿¡åº¦: ham=0.000, spam=1.000\n",
            "ğŸš¨ åˆ†ç±»ç»“æœï¼šåƒåœ¾çŸ­ä¿¡\n",
            "\n",
            "âœ… æ¨ç†æ¼”ç¤ºå®Œæˆï¼\n"
          ]
        }
      ],
      "source": [
        "# ğŸ”® æ¨ç†æ¼”ç¤º\n",
        "# åŠŸèƒ½ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†ç±»é¢„æµ‹\n",
        "\n",
        "# ğŸ“ å‡†å¤‡æµ‹è¯•æ ·æœ¬\n",
        "# åŒ…å«æ˜æ˜¾çš„æ­£å¸¸çŸ­ä¿¡å’Œåƒåœ¾çŸ­ä¿¡\n",
        "sample_texts = [\n",
        "    \"Hey, want to grab lunch together?\",                           # æ­£å¸¸çŸ­ä¿¡\n",
        "    \"URGENT! You've won $5000! Click now to claim your prize!\",   # åƒåœ¾çŸ­ä¿¡\n",
        "    \"The meeting has been moved to 2pm\",                          # æ­£å¸¸çŸ­ä¿¡\n",
        "    \"Free iPhone! Limited time offer! Call immediately!\",         # åƒåœ¾çŸ­ä¿¡\n",
        "    \"Thanks for your help yesterday\",                              # æ­£å¸¸çŸ­ä¿¡\n",
        "    \"Congratulations! You are selected as winner. Text WIN now!\",  # åƒåœ¾çŸ­ä¿¡\n",
        "]\n",
        "\n",
        "print(\"ğŸ”® æ¨ç†æ¼”ç¤º\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"æµ‹è¯• {len(sample_texts)} æ¡æ ·æœ¬...\\n\")\n",
        "\n",
        "# ğŸ”§ åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
        "model.eval()\n",
        "\n",
        "# ğŸ”„ éå†æ¯ä¸ªæµ‹è¯•æ ·æœ¬\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    # ğŸ”¤ ä½¿ç”¨ tokenizer ç¼–ç æ–‡æœ¬\n",
        "    # å‚æ•°è¯´æ˜ï¼š\n",
        "    # - return_tensors=\"pt\": è¿”å› PyTorch å¼ é‡\n",
        "    # - truncation=True: å¦‚æœè¶…é•¿åˆ™æˆªæ–­\n",
        "    # - padding=\"max_length\": å¡«å……åˆ°æœ€å¤§é•¿åº¦\n",
        "    # - max_length: ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "    # ğŸ® å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    # ğŸš« ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆæ¨ç†æ—¶ä¸éœ€è¦ï¼‰\n",
        "    with torch.no_grad():\n",
        "        # ğŸ”® å‰å‘ä¼ æ’­è·å– logits\n",
        "        logits = model(**encoded).logits\n",
        "\n",
        "        # ğŸ“Š è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ\n",
        "        # softmax å°† logits è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆå’Œä¸º 1ï¼‰\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "\n",
        "    # ğŸ¯ è·å–é¢„æµ‹ç»“æœ\n",
        "    pred = torch.argmax(probs).item()  # é¢„æµ‹ç±»åˆ«ï¼ˆ0 æˆ– 1ï¼‰\n",
        "    label = \"spam\" if pred == 1 else \"ham\"  # è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾\n",
        "\n",
        "    # ğŸ“Š æå–æ¦‚ç‡å€¼\n",
        "    ham_prob = probs[0].item()   # æ­£å¸¸çŸ­ä¿¡æ¦‚ç‡\n",
        "    spam_prob = probs[1].item()  # åƒåœ¾çŸ­ä¿¡æ¦‚ç‡\n",
        "\n",
        "    # ğŸ“¢ æ‰“å°ç»“æœ\n",
        "    print(f\"ã€æ ·æœ¬ {i}ã€‘\")\n",
        "    print(f\"æ–‡æœ¬: {text[:60]}{'...' if len(text) > 60 else ''}\")\n",
        "    print(f\"é¢„æµ‹: {label.upper()}\")\n",
        "    print(f\"ç½®ä¿¡åº¦: ham={ham_prob:.3f}, spam={spam_prob:.3f}\")\n",
        "\n",
        "    # ğŸ¨ æ·»åŠ è¡¨æƒ…ç¬¦å·å¢å¼ºå¯è¯»æ€§\n",
        "    if pred == 1:\n",
        "        print(\"ğŸš¨ åˆ†ç±»ç»“æœï¼šåƒåœ¾çŸ­ä¿¡\")\n",
        "    else:\n",
        "        print(\"âœ… åˆ†ç±»ç»“æœï¼šæ­£å¸¸çŸ­ä¿¡\")\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"âœ… æ¨ç†æ¼”ç¤ºå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtM7rWqo5kgN"
      },
      "source": [
        "# 8ï¸âƒ£9ï¸âƒ£ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½\n",
        "\n",
        "## ğŸ’¾ PEFT æ¨¡å‹çš„ä¿å­˜ç­–ç•¥\n",
        "\n",
        "PEFT åº“æä¾›äº†éå¸¸æ–¹ä¾¿çš„æ¨¡å‹ä¿å­˜åŠŸèƒ½ï¼š\n",
        "\n",
        "### ğŸ¯ ä¸¤ç§ä¿å­˜æ–¹å¼\n",
        "\n",
        "1. **åªä¿å­˜ LoRA å‚æ•°**ï¼ˆæ¨èï¼‰\n",
        "   - æ–‡ä»¶å¾ˆå°ï¼ˆå‡  MBï¼‰\n",
        "   - éœ€è¦é…åˆåŸå§‹æ¨¡å‹ä½¿ç”¨\n",
        "   - é€‚åˆæ¨¡å‹åˆ†å‘å’Œå¤šä»»åŠ¡åœºæ™¯\n",
        "\n",
        "2. **ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹**\n",
        "   - å°† LoRA å‚æ•°åˆå¹¶åˆ°åŸå§‹æƒé‡\n",
        "   - æ–‡ä»¶è¾ƒå¤§\n",
        "   - å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å‹ä½¿ç”¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxrpSq8M5kgN",
        "outputId": "c646ec5b-7024-49fe-faf0-2d45d5a87291"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ ä¿å­˜ LoRA å‚æ•°...\n",
            "âœ… LoRA å‚æ•°å·²ä¿å­˜åˆ°: ./lora_sms_spam_classifier\n",
            "ğŸ“‚ ä¿å­˜çš„æ–‡ä»¶: ['adapter_config.json', 'adapter_model.safetensors', 'README.md']\n",
            "ğŸ“Š æ€»å¤§å°: 4.52 MB\n",
            "\n",
            "ğŸ’¡ åŠ è½½ LoRA æ¨¡å‹çš„æ–¹æ³•:\n",
            "```python\n",
            "from peft import PeftModel\n",
            "from transformers import AutoModelForSequenceClassification\n",
            "\n",
            "# 1. åŠ è½½åŸºç¡€æ¨¡å‹\n",
            "base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)\n",
            "\n",
            "# 2. åŠ è½½ LoRA é€‚é…å™¨\n",
            "model = PeftModel.from_pretrained(base_model, \"./lora_sms_spam_classifier\")\n",
            "\n",
            "# 3. ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†\n",
            "model.eval()\n",
            "# ... æ¨ç†ä»£ç  ...\n",
            "```\n",
            "\n",
            "============================================================\n",
            "\n",
            "ğŸ”— æ–¹å¼ 2ï¼šåˆå¹¶ LoRA å‚æ•°åˆ°åŸºç¡€æ¨¡å‹\n",
            "ğŸ’¡ åˆå¹¶æ“ä½œä¼šå°† LoRA å‚æ•°åŠ åˆ°åŸå§‹æƒé‡ä¸Šï¼š\n",
            "   W_new = W_original + Î±/r * (A @ B)\n",
            "\n",
            "âš ï¸  æ³¨æ„ï¼šåˆå¹¶åå°†å¤±å»åˆ‡æ¢ LoRA é€‚é…å™¨çš„èƒ½åŠ›\n",
            "\n",
            "ğŸ”§ åˆå¹¶ä»£ç ç¤ºä¾‹ï¼ˆä¸æ‰§è¡Œï¼Œä»…å±•ç¤ºï¼‰:\n",
            "```python\n",
            "# åˆå¹¶ LoRA å‚æ•°åˆ°åŸºç¡€æ¨¡å‹\n",
            "merged_model = model.merge_and_unload()\n",
            "\n",
            "# ä¿å­˜å®Œæ•´æ¨¡å‹\n",
            "merged_model.save_pretrained(\"./merged_sms_spam_classifier\")\n",
            "tokenizer.save_pretrained(\"./merged_sms_spam_classifier\")\n",
            "\n",
            "# åŠ è½½å®Œæ•´æ¨¡å‹\n",
            "loaded_model = AutoModelForSequenceClassification.from_pretrained(\n",
            "    \"./merged_sms_spam_classifier\"\n",
            ")\n",
            "```\n",
            "\n",
            "âœ… æ¨¡å‹ä¿å­˜å®Œæˆï¼\n",
            "\n",
            "ğŸ“Š ä¸¤ç§ä¿å­˜æ–¹å¼å¯¹æ¯”:\n",
            "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
            "â”‚      ç‰¹æ€§       â”‚  LoRA å‚æ•°   â”‚  å®Œæ•´æ¨¡å‹    â”‚\n",
            "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
            "â”‚  æ–‡ä»¶å¤§å°       â”‚  å‡  MB       â”‚  æ•°ç™¾ MB     â”‚\n",
            "â”‚  åŠ è½½é€Ÿåº¦       â”‚  å¿«          â”‚  è¾ƒæ…¢        â”‚\n",
            "â”‚  å¤šä»»åŠ¡åˆ‡æ¢     â”‚  æ”¯æŒ        â”‚  ä¸æ”¯æŒ      â”‚\n",
            "â”‚  ç‹¬ç«‹ä½¿ç”¨       â”‚  éœ€åŸºç¡€æ¨¡å‹  â”‚  å¯ç‹¬ç«‹ä½¿ç”¨  â”‚\n",
            "â”‚  æ¨èåœºæ™¯       â”‚  ç”Ÿäº§ç¯å¢ƒ    â”‚  æ¼”ç¤º/éƒ¨ç½²   â”‚\n",
            "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
          ]
        }
      ],
      "source": [
        "# ğŸ’¾ æ–¹å¼ 1ï¼šåªä¿å­˜ LoRA å‚æ•°ï¼ˆæ¨èï¼‰\n",
        "# åŠŸèƒ½ï¼šä¿å­˜è½»é‡çº§çš„ LoRA é€‚é…å™¨æ–‡ä»¶\n",
        "\n",
        "print(\"ğŸ’¾ ä¿å­˜ LoRA å‚æ•°...\")\n",
        "\n",
        "# ğŸ“ æŒ‡å®šä¿å­˜è·¯å¾„\n",
        "lora_save_path = \"./lora_sms_spam_classifier\"\n",
        "\n",
        "# ğŸ’¾ ä¿å­˜ LoRA é€‚é…å™¨\n",
        "# save_pretrained() ä¼šä¿å­˜ï¼š\n",
        "# 1. adapter_config.json: LoRA é…ç½®\n",
        "# 2. adapter_model.safetensors: LoRA æƒé‡ï¼ˆæ¨èæ ¼å¼ï¼‰\n",
        "# æˆ– adapter_model.bin: LoRA æƒé‡ï¼ˆä¼ ç»Ÿæ ¼å¼ï¼‰\n",
        "model.save_pretrained(lora_save_path)\n",
        "\n",
        "print(f\"âœ… LoRA å‚æ•°å·²ä¿å­˜åˆ°: {lora_save_path}\")\n",
        "\n",
        "# ğŸ“Š æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶\n",
        "import os\n",
        "saved_files = os.listdir(lora_save_path)\n",
        "print(f\"ğŸ“‚ ä¿å­˜çš„æ–‡ä»¶: {saved_files}\")\n",
        "\n",
        "# ğŸ“ æ£€æŸ¥æ–‡ä»¶å¤§å°\n",
        "total_size = sum(os.path.getsize(os.path.join(lora_save_path, f)) for f in saved_files)\n",
        "print(f\"ğŸ“Š æ€»å¤§å°: {total_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "# ğŸ’¡ åŠ è½½æ–¹å¼ç¤ºä¾‹ï¼ˆä»£ç ä»…ä¾›å‚è€ƒï¼Œä¸å®é™…æ‰§è¡Œï¼‰\n",
        "print(\"\\nğŸ’¡ åŠ è½½ LoRA æ¨¡å‹çš„æ–¹æ³•:\")\n",
        "print(\"```python\")\n",
        "print(\"from peft import PeftModel\")\n",
        "print(\"from transformers import AutoModelForSequenceClassification\")\n",
        "print(\"\")\n",
        "print(\"# 1. åŠ è½½åŸºç¡€æ¨¡å‹\")\n",
        "print('base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)')\n",
        "print(\"\")\n",
        "print(\"# 2. åŠ è½½ LoRA é€‚é…å™¨\")\n",
        "print('model = PeftModel.from_pretrained(base_model, \"./lora_sms_spam_classifier\")')\n",
        "print(\"\")\n",
        "print(\"# 3. ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†\")\n",
        "print(\"model.eval()\")\n",
        "print(\"# ... æ¨ç†ä»£ç  ...\")\n",
        "print(\"```\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# ğŸ”— æ–¹å¼ 2ï¼šåˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
        "print(\"\\nğŸ”— æ–¹å¼ 2ï¼šåˆå¹¶ LoRA å‚æ•°åˆ°åŸºç¡€æ¨¡å‹\")\n",
        "\n",
        "# ğŸ“ åˆå¹¶è¯´æ˜\n",
        "print(\"ğŸ’¡ åˆå¹¶æ“ä½œä¼šå°† LoRA å‚æ•°åŠ åˆ°åŸå§‹æƒé‡ä¸Šï¼š\")\n",
        "print(\"   W_new = W_original + Î±/r * (A @ B)\")\n",
        "print(\"\")\n",
        "print(\"âš ï¸  æ³¨æ„ï¼šåˆå¹¶åå°†å¤±å»åˆ‡æ¢ LoRA é€‚é…å™¨çš„èƒ½åŠ›\")\n",
        "print(\"\")\n",
        "\n",
        "# ğŸ”§ åˆå¹¶æ¨¡å‹ï¼ˆç¤ºä¾‹ä»£ç ï¼‰\n",
        "print(\"ğŸ”§ åˆå¹¶ä»£ç ç¤ºä¾‹ï¼ˆä¸æ‰§è¡Œï¼Œä»…å±•ç¤ºï¼‰:\")\n",
        "print(\"```python\")\n",
        "print(\"# åˆå¹¶ LoRA å‚æ•°åˆ°åŸºç¡€æ¨¡å‹\")\n",
        "print(\"merged_model = model.merge_and_unload()\")\n",
        "print(\"\")\n",
        "print(\"# ä¿å­˜å®Œæ•´æ¨¡å‹\")\n",
        "print('merged_model.save_pretrained(\"./merged_sms_spam_classifier\")')\n",
        "print('tokenizer.save_pretrained(\"./merged_sms_spam_classifier\")')\n",
        "print(\"\")\n",
        "print(\"# åŠ è½½å®Œæ•´æ¨¡å‹\")\n",
        "print('loaded_model = AutoModelForSequenceClassification.from_pretrained(')\n",
        "print('    \"./merged_sms_spam_classifier\"')\n",
        "print(')')\n",
        "print(\"```\")\n",
        "\n",
        "print(\"\\nâœ… æ¨¡å‹ä¿å­˜å®Œæˆï¼\")\n",
        "\n",
        "# ğŸ“Š ä¿å­˜æ–¹å¼å¯¹æ¯”\n",
        "print(\"\\nğŸ“Š ä¸¤ç§ä¿å­˜æ–¹å¼å¯¹æ¯”:\")\n",
        "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(\"â”‚      ç‰¹æ€§       â”‚  LoRA å‚æ•°   â”‚  å®Œæ•´æ¨¡å‹    â”‚\")\n",
        "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
        "print(\"â”‚  æ–‡ä»¶å¤§å°       â”‚  å‡  MB       â”‚  æ•°ç™¾ MB     â”‚\")\n",
        "print(\"â”‚  åŠ è½½é€Ÿåº¦       â”‚  å¿«          â”‚  è¾ƒæ…¢        â”‚\")\n",
        "print(\"â”‚  å¤šä»»åŠ¡åˆ‡æ¢     â”‚  æ”¯æŒ        â”‚  ä¸æ”¯æŒ      â”‚\")\n",
        "print(\"â”‚  ç‹¬ç«‹ä½¿ç”¨       â”‚  éœ€åŸºç¡€æ¨¡å‹  â”‚  å¯ç‹¬ç«‹ä½¿ç”¨  â”‚\")\n",
        "print(\"â”‚  æ¨èåœºæ™¯       â”‚  ç”Ÿäº§ç¯å¢ƒ    â”‚  æ¼”ç¤º/éƒ¨ç½²   â”‚\")\n",
        "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKyoKzQm5kgO"
      },
      "source": [
        "# ğŸ“ æ•™ç¨‹æ€»ç»“\n",
        "\n",
        "## âœ¨ æ ¸å¿ƒè¦ç‚¹\n",
        "\n",
        "### PEFT åº“ä½¿ç”¨ä¸‰æ­¥æ›²\n",
        "1. åˆ›å»º LoRAConfig\n",
        "2. åŠ è½½åŸºç¡€æ¨¡å‹  \n",
        "3. è°ƒç”¨ get_peft_model()\n",
        "\n",
        "### å…³é”®ä¼˜åŠ¿\n",
        "- ä»£ç ç®€æ´ï¼ˆ~10è¡Œ vs ~200è¡Œï¼‰\n",
        "- å·¥ä¸šçº§å®ç°\n",
        "- å®˜æ–¹ç»´æŠ¤æ›´æ–°\n",
        "- ä¸ Transformers æ— ç¼é›†æˆ\n",
        "\n",
        "## ğŸ¯ è¶…å‚æ•°å»ºè®®\n",
        "\n",
        "- **rank**: 8-16ï¼ˆæ¨èèµ·ç‚¹ï¼‰\n",
        "- **alpha**: 2 Ã— rank\n",
        "- **dropout**: 0.05-0.1\n",
        "- **learning_rate**: 5e-4 åˆ° 1e-3\n",
        "\n",
        "## ğŸ’¡ æœ€ä½³å®è·µ\n",
        "\n",
        "1. ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰\n",
        "2. æ·»åŠ æ¢¯åº¦è£å‰ª\n",
        "3. ç›‘æ§éªŒè¯é›†æ€§èƒ½\n",
        "4. ä¿å­˜è½»é‡çº§ LoRA å‚æ•°\n",
        "\n",
        "## ğŸš€ è¿›é˜¶æ–¹å‘\n",
        "\n",
        "- QLoRAï¼šç»“åˆé‡åŒ–\n",
        "- å¤šä»»åŠ¡ LoRA\n",
        "- å¤§æ¨¡å‹å¾®è°ƒï¼ˆLLaMAã€Mistralï¼‰\n",
        "\n",
        "ğŸ‰ æ­å–œå®Œæˆå­¦ä¹ ï¼\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}