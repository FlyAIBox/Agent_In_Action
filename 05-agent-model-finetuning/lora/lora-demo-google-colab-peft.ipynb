{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸš€ ä½¿ç”¨ Hugging Face PEFT åº“å®ç° LoRA å¾®è°ƒ - å®Œæ•´æ•™ç¨‹\n",
        "\n",
        "## ğŸ“– æ•™ç¨‹æ¦‚è¿°\n",
        "\n",
        "æœ¬æ•™ç¨‹å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ **Hugging Face PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰** åº“å®ç° LoRA å¾®è°ƒï¼Œè¿™æ˜¯å·¥ä¸šç•Œæ¨èçš„æ ‡å‡†æ–¹æ³•ã€‚\n",
        "\n",
        "### ğŸ¯ ä¸ºä»€ä¹ˆä½¿ç”¨ PEFT åº“ï¼Ÿ\n",
        "\n",
        "#### âœ… PEFT åº“çš„ä¼˜åŠ¿\n",
        "\n",
        "1. **å·¥ä¸šçº§å®ç°**ï¼šç»è¿‡å¤§è§„æ¨¡éªŒè¯ï¼Œç¨³å®šå¯é \n",
        "2. **æ˜“äºä½¿ç”¨**ï¼šå‡ è¡Œä»£ç å³å¯åº”ç”¨ LoRA\n",
        "3. **åŠŸèƒ½ä¸°å¯Œ**ï¼šæ”¯æŒ LoRAã€LoHaã€LoKr ç­‰å¤šç§æ–¹æ³•\n",
        "4. **ç”Ÿæ€é›†æˆ**ï¼šä¸ Transformersã€Accelerate æ— ç¼é›†æˆ\n",
        "5. **æŒç»­ç»´æŠ¤**ï¼šHugging Face å®˜æ–¹æ”¯æŒ\n",
        "\n",
        "#### ğŸ“Š ä¸æ‰‹åŠ¨å®ç°çš„å¯¹æ¯”\n",
        "\n",
        "| ç‰¹æ€§ | æ‰‹åŠ¨å®ç° | PEFT åº“ |\n",
        "|------|---------|---------|\n",
        "| **ä»£ç é‡** | å¤šï¼ˆéœ€å®ç° LoRA å±‚ï¼‰ | å°‘ï¼ˆå‡ è¡Œé…ç½®ï¼‰ |\n",
        "| **å­¦ä¹ ä»·å€¼** | æ·±å…¥ç†è§£åŸç† | å¿«é€Ÿåº”ç”¨å®è·µ |\n",
        "| **ç»´æŠ¤æˆæœ¬** | é«˜ï¼ˆéœ€è‡ªå·±ç»´æŠ¤ï¼‰ | ä½ï¼ˆå®˜æ–¹ç»´æŠ¤ï¼‰ |\n",
        "| **åŠŸèƒ½å®Œæ•´æ€§** | åŸºç¡€åŠŸèƒ½ | ä¸°å¯ŒåŠŸèƒ½ |\n",
        "| **é”™è¯¯é£é™©** | è¾ƒé«˜ | è¾ƒä½ |\n",
        "| **é€‚ç”¨åœºæ™¯** | å­¦ä¹ ç ”ç©¶ | ç”Ÿäº§ç¯å¢ƒ |\n",
        "\n",
        "### ğŸ“ å­¦ä¹ è·¯å¾„å»ºè®®\n",
        "\n",
        "1. **ç¬¬ä¸€æ­¥**ï¼šå­¦ä¹ æ‰‹åŠ¨å®ç°ï¼ˆç†è§£åŸç†ï¼‰\n",
        "2. **ç¬¬äºŒæ­¥**ï¼šå­¦ä¹  PEFT åº“ï¼ˆæŒæ¡å·¥å…·ï¼‰\n",
        "3. **ç¬¬ä¸‰æ­¥**ï¼šå®é™…é¡¹ç›®ä¸­ä½¿ç”¨ PEFT\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“¦ æ ¸å¿ƒä¾èµ–è¯´æ˜\n",
        "\n",
        "æœ¬æ•™ç¨‹ä½¿ç”¨ä»¥ä¸‹ç»è¿‡æµ‹è¯•çš„ä¾èµ–ç‰ˆæœ¬ç»„åˆï¼š\n",
        "\n",
        "- **torch==2.8.0**ï¼šPyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
        "- **transformers==4.51.3**ï¼šHugging Face æ¨¡å‹åº“\n",
        "- **peft==0.13.2**ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒåº“ï¼ˆæ ¸å¿ƒï¼‰\n",
        "- **accelerate==1.0.1**ï¼šåˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿåº“\n",
        "- **pandas==2.2.2**ï¼šæ•°æ®å¤„ç†\n",
        "- **numpy==2.0.2**ï¼šæ•°å€¼è®¡ç®—\n",
        "\n",
        "ğŸ’¡ **æ³¨æ„**ï¼šè¿™äº›ç‰ˆæœ¬å·²ç»è¿‡å…¼å®¹æ€§æµ‹è¯•ï¼Œå»ºè®®ä½¿ç”¨ç›¸åŒç‰ˆæœ¬ã€‚\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1ï¸âƒ£ ç¯å¢ƒå‡†å¤‡å’Œä¾èµ–å®‰è£…\n",
        "\n",
        "## ğŸ“¦ ä¸€é”®å®‰è£…æ‰€æœ‰ä¾èµ–\n",
        "\n",
        "ä»¥ä¸‹å‘½ä»¤å°†å®‰è£…æ‰€æœ‰å¿…éœ€çš„ä¾èµ–åŒ…ã€‚æˆ‘ä»¬ä½¿ç”¨ `-q` å‚æ•°æ¥å‡å°‘è¾“å‡ºä¿¡æ¯ã€‚\n",
        "\n",
        "### ğŸ” ä¾èµ–åŒ…è¯´æ˜\n",
        "\n",
        "- **torch**ï¼šPyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼ˆæ”¯æŒ CUDA åŠ é€Ÿï¼‰\n",
        "- **transformers**ï¼šHugging Face çš„é¢„è®­ç»ƒæ¨¡å‹åº“\n",
        "- **peft**ï¼šå‚æ•°é«˜æ•ˆå¾®è°ƒåº“ï¼ˆæœ¬æ•™ç¨‹çš„æ ¸å¿ƒï¼‰\n",
        "- **accelerate**ï¼šåˆ†å¸ƒå¼è®­ç»ƒåŠ é€Ÿåº“\n",
        "- **pandas/numpy**ï¼šæ•°æ®å¤„ç†å’Œæ•°å€¼è®¡ç®—\n",
        "- **tqdm**ï¼šè¿›åº¦æ¡æ˜¾ç¤º\n",
        "- **matplotlib**ï¼šæ•°æ®å¯è§†åŒ–\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“¦ å®‰è£…ä¾èµ–åŒ…\n",
        "# è¯´æ˜ï¼šä½¿ç”¨ -q å‚æ•°å®‰é™æ¨¡å¼å®‰è£…ï¼Œ--upgrade ç¡®ä¿å®‰è£…æœ€æ–°ç‰ˆæœ¬\n",
        "# å¦‚æœåœ¨ Colab ä¸­è¿è¡Œï¼ŒæŸäº›åŒ…å¯èƒ½å·²é¢„è£…ï¼Œä¼šè‡ªåŠ¨è·³è¿‡\n",
        "\n",
        "%pip install -q --upgrade torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121 transformers==4.51.3 peft==0.13.2 accelerate==1.0.1 pandas==2.2.2 numpy==2.0.2 tqdm==4.67.1 matplotlib==3.10.7 requests==2.32.5 safetensors==0.6.2\n",
        "\n",
        "print(\"âœ… ä¾èµ–åŒ…å®‰è£…å®Œæˆï¼\")\n",
        "print(\"ğŸ’¡ å»ºè®®é‡å¯ Runtime ä»¥ç¡®ä¿æ–°åŒ…æ­£å¸¸å·¥ä½œï¼ˆRuntime -> Restart Runtimeï¼‰\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2ï¸âƒ£ å¯¼å…¥ä¾èµ–åº“å’Œç¯å¢ƒæ£€æŸ¥\n",
        "\n",
        "## ğŸ”§ æ ¸å¿ƒåº“å¯¼å…¥è¯´æ˜\n",
        "\n",
        "ä¸‹é¢æˆ‘ä»¬å°†å¯¼å…¥æ‰€æœ‰éœ€è¦çš„åº“ï¼Œå¹¶æ£€æŸ¥è¿è¡Œç¯å¢ƒã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”§ å¯¼å…¥æ‰€æœ‰å¿…éœ€çš„åº“\n",
        "# åŠŸèƒ½ï¼šå‡†å¤‡è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ä¾èµ–\n",
        "\n",
        "import io\n",
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,  # åºåˆ—åˆ†ç±»æ¨¡å‹ï¼ˆç”¨äºæ–‡æœ¬åˆ†ç±»ï¼‰\n",
        "    AutoTokenizer,                        # è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨\n",
        "    get_linear_schedule_with_warmup,     # å¸¦é¢„çƒ­çš„çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦å™¨\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,        # LoRA é…ç½®ç±»\n",
        "    TaskType,          # ä»»åŠ¡ç±»å‹æšä¸¾\n",
        "    get_peft_model,    # å°† LoRA åº”ç”¨åˆ°æ¨¡å‹çš„æ ¸å¿ƒå‡½æ•°\n",
        ")\n",
        "from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º\n",
        "import requests        # HTTP è¯·æ±‚ï¼ˆç”¨äºä¸‹è½½æ•°æ®ï¼‰\n",
        "\n",
        "# ğŸ¯ è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
        "# è¯´æ˜ï¼šåœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œå¾ˆå¤šæ“ä½œæ¶‰åŠéšæœºæ€§ï¼ˆå¦‚å‚æ•°åˆå§‹åŒ–ã€æ•°æ®æ‰“ä¹±ï¼‰\n",
        "#      å›ºå®šéšæœºç§å­å¯ä»¥è®©æ¯æ¬¡è¿è¡Œå¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œä¾¿äºè°ƒè¯•å’Œå¯¹æ¯”\n",
        "torch.manual_seed(42)   # PyTorch çš„éšæœºç§å­\n",
        "random.seed(42)         # Python æ ‡å‡†åº“çš„éšæœºç§å­\n",
        "\n",
        "# ğŸ® è®¾å¤‡é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨ GPUï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ CPU\n",
        "# è¯´æ˜ï¼šCUDA æ˜¯ NVIDIA GPU çš„å¹¶è¡Œè®¡ç®—å¹³å°\n",
        "#      torch.cuda.is_available() æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ğŸ“Š æ˜¾ç¤ºç¯å¢ƒä¿¡æ¯\n",
        "print(\"ğŸ” ç¯å¢ƒæ£€æŸ¥\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"ğŸ® ä½¿ç”¨è®¾å¤‡: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU å‹å·: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   GPU æ•°é‡: {torch.cuda.device_count()}\")\n",
        "    print(f\"   CUDA ç‰ˆæœ¬: {torch.version.cuda}\")\n",
        "else:\n",
        "    print(\"   âš ï¸  æœªæ£€æµ‹åˆ° GPUï¼Œå°†ä½¿ç”¨ CPUï¼ˆè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼‰\")\n",
        "\n",
        "print(f\"ğŸ Python åº“ç‰ˆæœ¬:\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "try:\n",
        "    import transformers\n",
        "    print(f\"   Transformers: {transformers.__version__}\")\n",
        "    import peft\n",
        "    print(f\"   PEFT: {peft.__version__}\")\n",
        "except ImportError as e:\n",
        "    print(f\"   âš ï¸ éƒ¨åˆ†åº“æœªå®‰è£…: {e}\")\n",
        "\n",
        "print(\"\\nâœ… ç¯å¢ƒå‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3ï¸âƒ£ æ•°æ®å‡†å¤‡å’Œé¢„å¤„ç†\n",
        "\n",
        "## ğŸ“Š æ•°æ®é›†ä»‹ç»\n",
        "\n",
        "æˆ‘ä»¬ä½¿ç”¨ **SMS Spam Collection** æ•°æ®é›†è¿›è¡Œåƒåœ¾çŸ­ä¿¡åˆ†ç±»ä»»åŠ¡ï¼š\n",
        "\n",
        "- **æ•°æ®æ¥æº**ï¼šUCI Machine Learning Repository\n",
        "- **æ•°æ®è§„æ¨¡**ï¼šçº¦ 5,572 æ¡çŸ­ä¿¡\n",
        "- **ä»»åŠ¡ç±»å‹**ï¼šäºŒåˆ†ç±»ï¼ˆham æ­£å¸¸çŸ­ä¿¡ vs spam åƒåœ¾çŸ­ä¿¡ï¼‰\n",
        "- **æ ‡ç­¾åˆ†å¸ƒ**ï¼šä¸å¹³è¡¡ï¼ˆham çº¦ 87%, spam çº¦ 13%ï¼‰\n",
        "\n",
        "## ğŸ”§ æ•°æ®å¤„ç†æµç¨‹\n",
        "\n",
        "```\n",
        "åŸå§‹æ•°æ®ä¸‹è½½ â†’ ç±»åˆ«å¹³è¡¡é‡‡æ · â†’ åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›† â†’ åˆ†è¯ç¼–ç  â†’ DataLoaderåŠ è½½\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š æ•°æ®å¤„ç†å·¥å…·å‡½æ•°\n",
        "# åŠŸèƒ½ï¼šæä¾›æ•°æ®ä¸‹è½½ã€å¹³è¡¡ã€åˆ’åˆ†å’ŒåŠ è½½çš„å®Œæ•´å·¥å…·é›†\n",
        "\n",
        "def create_balanced_dataset(df):\n",
        "    \"\"\"\n",
        "    åˆ›å»ºç±»åˆ«å¹³è¡¡çš„æ•°æ®é›†\n",
        "    \n",
        "    ğŸ¯ ç›®çš„ï¼šè§£å†³æ•°æ®ä¸å¹³è¡¡é—®é¢˜\n",
        "    åŸå§‹æ•°æ®ä¸­ hamï¼ˆæ­£å¸¸çŸ­ä¿¡ï¼‰è¿œå¤šäº spamï¼ˆåƒåœ¾çŸ­ä¿¡ï¼‰ï¼Œ\n",
        "    è¿™ä¼šå¯¼è‡´æ¨¡å‹åå‘é¢„æµ‹å¤šæ•°ç±»ã€‚é€šè¿‡ä¸‹é‡‡æ ·å¤šæ•°ç±»ï¼Œ\n",
        "    ä½¿ä¸¤ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ç›¸ç­‰ã€‚\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        df: pandas DataFrameï¼ŒåŒ…å« Label å’Œ Text åˆ—\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        balanced_df: ç±»åˆ«å¹³è¡¡åçš„ DataFrame\n",
        "        \n",
        "    ç¤ºä¾‹ï¼š\n",
        "        åŸå§‹ï¼šham(4825æ¡) + spam(747æ¡) = 5572æ¡\n",
        "        å¹³è¡¡åï¼šham(747æ¡) + spam(747æ¡) = 1494æ¡\n",
        "    \"\"\"\n",
        "    label_col = df[\"Label\"]\n",
        "    \n",
        "    # ğŸ” å…¼å®¹æ€§å¤„ç†ï¼šæ ‡ç­¾å¯èƒ½æ˜¯å­—ç¬¦ä¸²ï¼ˆ'spam'/'ham'ï¼‰æˆ–æ•°å­—ï¼ˆ0/1ï¼‰\n",
        "    spam_mask = (label_col == \"spam\") | (label_col == 1)\n",
        "    ham_mask = (label_col == \"ham\") | (label_col == 0)\n",
        "    \n",
        "    # ç»Ÿè®¡åƒåœ¾çŸ­ä¿¡æ•°é‡ï¼ˆå°‘æ•°ç±»ï¼‰\n",
        "    spam_count = int(spam_mask.sum())\n",
        "    \n",
        "    if spam_count == 0 or ham_mask.sum() == 0:\n",
        "        raise ValueError(\"æ•°æ®é›†ç¼ºå°‘ spam æˆ– ham ç±»åˆ«ï¼Œæ— æ³•å¹³è¡¡\")\n",
        "    \n",
        "    # ğŸ² ä»å¤šæ•°ç±»ï¼ˆhamï¼‰ä¸­éšæœºé‡‡æ ·ï¼Œæ•°é‡ä¸å°‘æ•°ç±»ç›¸åŒ\n",
        "    # random_state=123 ç¡®ä¿æ¯æ¬¡é‡‡æ ·ç»“æœç›¸åŒï¼Œä¾¿äºå¤ç°\n",
        "    ham_subset = df[ham_mask].sample(spam_count, random_state=123)\n",
        "    \n",
        "    # ğŸ“¦ åˆå¹¶ä¸¤ä¸ªç±»åˆ«çš„æ•°æ®\n",
        "    balanced = pd.concat([ham_subset, df[spam_mask]])\n",
        "    \n",
        "    # ğŸ”€ éšæœºæ‰“ä¹±æ•°æ®ï¼Œé¿å…æ•°æ®é¡ºåºåå·®\n",
        "    return balanced.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def random_split(df, train_frac=0.7, val_frac=0.1):\n",
        "    \"\"\"\n",
        "    éšæœºåˆ’åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†\n",
        "    \n",
        "    ğŸ¯ ç›®çš„ï¼šå°†æ•°æ®åˆ†ä¸ºä¸‰éƒ¨åˆ†\n",
        "    - è®­ç»ƒé›†ï¼šç”¨äºæ¨¡å‹å‚æ•°æ›´æ–°\n",
        "    - éªŒè¯é›†ï¼šç”¨äºè°ƒæ•´è¶…å‚æ•°å’Œæ—©åœ\n",
        "    - æµ‹è¯•é›†ï¼šç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        df: pandas DataFrame\n",
        "        train_frac: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.7 = 70%ï¼‰\n",
        "        val_frac: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.1 = 10%ï¼‰\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        train_df, val_df, test_df: ä¸‰ä¸ªæ•°æ®æ¡†\n",
        "        \n",
        "    ç¤ºä¾‹ï¼š\n",
        "        æ€»å…±1494æ¡ â†’ è®­ç»ƒ1046æ¡(70%) + éªŒè¯149æ¡(10%) + æµ‹è¯•299æ¡(20%)\n",
        "    \"\"\"\n",
        "    # ğŸ”€ éšæœºæ‰“ä¹±æ•°æ®\n",
        "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "    \n",
        "    # ğŸ“ è®¡ç®—åˆ’åˆ†ç‚¹\n",
        "    train_end = int(len(df) * train_frac)\n",
        "    val_end = train_end + int(len(df) * val_frac)\n",
        "    \n",
        "    # âœ‚ï¸ æ‰§è¡Œåˆ’åˆ†\n",
        "    train_df = df[:train_end]\n",
        "    val_df = df[train_end:val_end]\n",
        "    test_df = df[val_end:]\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "def ensure_sms_file(local_path=\"sms_spam_collection/SMSSpamCollection.tsv\"):\n",
        "    \"\"\"\n",
        "    ç¡®ä¿ SMS æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è‡ªåŠ¨ä¸‹è½½\n",
        "    \n",
        "    ğŸ¯ ç›®çš„ï¼šè‡ªåŠ¨åŒ–æ•°æ®å‡†å¤‡æµç¨‹\n",
        "    å¦‚æœæœ¬åœ°å·²æœ‰æ•°æ®æ–‡ä»¶ï¼Œç›´æ¥è¿”å›è·¯å¾„ï¼›\n",
        "    å¦‚æœæ²¡æœ‰ï¼Œåˆ™ä» UCI ä»“åº“ä¸‹è½½å¹¶è§£å‹ã€‚\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        str: æ•°æ®æ–‡ä»¶çš„å®é™…è·¯å¾„\n",
        "        \n",
        "    è¯´æ˜ï¼š\n",
        "        æ•°æ®æ–‡ä»¶æ ¼å¼ä¸º TSVï¼ˆTab-Separated Valuesï¼‰ï¼Œæ¯è¡ŒåŒ…å«æ ‡ç­¾å’Œæ–‡æœ¬\n",
        "    \"\"\"\n",
        "    # âœ… å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›\n",
        "    if os.path.exists(local_path):\n",
        "        return local_path\n",
        "    \n",
        "    # ğŸ“¥ ä» UCI æœºå™¨å­¦ä¹ ä»“åº“ä¸‹è½½æ•°æ®\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
        "    print(f\"ğŸ“¥ æ­£åœ¨ä» UCI ä»“åº“ä¸‹è½½æ•°æ®...\")\n",
        "    print(f\"   URL: {url}\")\n",
        "    \n",
        "    try:\n",
        "        # ğŸŒ å‘é€ HTTP è¯·æ±‚ä¸‹è½½ ZIP æ–‡ä»¶\n",
        "        resp = requests.get(url, timeout=30)\n",
        "        resp.raise_for_status()  # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸\n",
        "        \n",
        "        # ğŸ“¦ è§£å‹ ZIP æ–‡ä»¶ï¼Œè¯»å–å…¶ä¸­çš„ SMSSpamCollection æ–‡ä»¶\n",
        "        with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
        "            raw = zf.read(\"SMSSpamCollection\").decode(\"utf-8\")\n",
        "        \n",
        "        # ğŸ’¾ ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶\n",
        "        out_path = \"SMSSpamCollection.tsv\"\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(raw)\n",
        "        \n",
        "        print(f\"âœ… æ•°æ®ä¸‹è½½æˆåŠŸï¼Œä¿å­˜åˆ°: {out_path}\")\n",
        "        return out_path\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"âŒ æ•°æ®ä¸‹è½½å¤±è´¥: {e}\")\n",
        "        print(f\"ğŸ’¡ è¯·æ‰‹åŠ¨ä¸‹è½½å¹¶æ”¾ç½®åˆ°å½“å‰ç›®å½•\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_sms_dataframe():\n",
        "    \"\"\"\n",
        "    åŠ è½½å¹¶é¢„å¤„ç† SMS æ•°æ®é›†\n",
        "    \n",
        "    ğŸ¯ ç›®çš„ï¼šä¸€ç«™å¼æ•°æ®åŠ è½½å‡½æ•°\n",
        "    æ•´åˆäº†ä¸‹è½½ã€è¯»å–ã€å¹³è¡¡ã€æ ‡ç­¾è½¬æ¢å’Œåˆ’åˆ†çš„å®Œæ•´æµç¨‹\n",
        "    \n",
        "    è¿”å›ï¼š\n",
        "        train_df, val_df, test_df: ä¸‰ä¸ªå·²å¤„ç†çš„ DataFrame\n",
        "        \n",
        "    æ•°æ®å¤„ç†æ­¥éª¤ï¼š\n",
        "        1. ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨ï¼ˆè‡ªåŠ¨ä¸‹è½½ï¼‰\n",
        "        2. è¯»å– TSV æ–‡ä»¶\n",
        "        3. å¹³è¡¡ç±»åˆ«åˆ†å¸ƒ\n",
        "        4. æ ‡ç­¾è½¬æ¢ï¼ˆhamâ†’0, spamâ†’1ï¼‰\n",
        "        5. åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†\n",
        "    \"\"\"\n",
        "    # ğŸ“‚ è·å–æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚éœ€è¦ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰\n",
        "    data_path = ensure_sms_file()\n",
        "    \n",
        "    # ğŸ“– è¯»å– TSV æ–‡ä»¶\n",
        "    # sep=\"\\t\" æŒ‡å®šåˆ¶è¡¨ç¬¦åˆ†éš”\n",
        "    # names æŒ‡å®šåˆ—å\n",
        "    # header=None è¡¨ç¤ºæ–‡ä»¶æ²¡æœ‰è¡¨å¤´è¡Œ\n",
        "    df = pd.read_csv(data_path, sep=\"\\t\", names=[\"Label\", \"Text\"], header=None)\n",
        "    \n",
        "    print(f\"ğŸ“Š åŸå§‹æ•°æ®åŠ è½½å®Œæˆ: {len(df)} æ¡\")\n",
        "    print(f\"   - ham (æ­£å¸¸): {(df['Label']=='ham').sum()} æ¡\")\n",
        "    print(f\"   - spam (åƒåœ¾): {(df['Label']=='spam').sum()} æ¡\")\n",
        "    \n",
        "    # âš–ï¸ åˆ›å»ºç±»åˆ«å¹³è¡¡çš„æ•°æ®é›†\n",
        "    balanced = create_balanced_dataset(df)\n",
        "    print(f\"âš–ï¸  æ•°æ®å¹³è¡¡å: {len(balanced)} æ¡ï¼ˆæ¯ç±» {len(balanced)//2} æ¡ï¼‰\")\n",
        "    \n",
        "    # ğŸ”„ æ ‡ç­¾è½¬æ¢ï¼šå­—ç¬¦ä¸² â†’ æ•°å­—\n",
        "    # æœºå™¨å­¦ä¹ æ¨¡å‹éœ€è¦æ•°å­—æ ‡ç­¾\n",
        "    label_map = {\"ham\": 0, \"spam\": 1}\n",
        "    balanced[\"Label\"] = balanced[\"Label\"].apply(\n",
        "        lambda v: label_map[v] if v in label_map else int(v)\n",
        "    )\n",
        "    \n",
        "    # âœ‚ï¸ åˆ’åˆ†æ•°æ®é›†\n",
        "    train_df, val_df, test_df = random_split(balanced, train_frac=0.7, val_frac=0.1)\n",
        "    print(f\"âœ‚ï¸  æ•°æ®åˆ’åˆ†å®Œæˆ:\")\n",
        "    print(f\"   - è®­ç»ƒé›†: {len(train_df)} æ¡\")\n",
        "    print(f\"   - éªŒè¯é›†: {len(val_df)} æ¡\")\n",
        "    print(f\"   - æµ‹è¯•é›†: {len(test_df)} æ¡\")\n",
        "    \n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "# ğŸ“ åˆå­¦è€…æç¤ºï¼š\n",
        "# ä»¥ä¸Šå‡½æ•°æ„æˆäº†å®Œæ•´çš„æ•°æ®å¤„ç†æµæ°´çº¿ï¼Œæ¯ä¸ªå‡½æ•°èŒè´£å•ä¸€ï¼Œä¾¿äºç†è§£å’Œè°ƒè¯•ã€‚\n",
        "# åœ¨å®é™…é¡¹ç›®ä¸­ï¼Œå»ºè®®å°†æ•°æ®å¤„ç†ä»£ç æ¨¡å—åŒ–ï¼Œæé«˜ä»£ç å¤ç”¨æ€§ã€‚\n",
        "print(\"âœ… æ•°æ®å¤„ç†å‡½æ•°å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“¦ PyTorch Dataset ç±»å®šä¹‰\n",
        "# åŠŸèƒ½ï¼šå°† DataFrame è½¬æ¢ä¸º PyTorch å¯ç”¨çš„æ•°æ®é›†æ ¼å¼\n",
        "\n",
        "class SpamSequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    SMS åƒåœ¾çŸ­ä¿¡æ•°æ®é›†ç±»\n",
        "    \n",
        "    ğŸ¯ ç›®çš„ï¼šå°† pandas DataFrame è½¬æ¢ä¸º PyTorch Dataset\n",
        "    è¿™ä¸ªç±»ç»§æ‰¿è‡ª torch.utils.data.Datasetï¼Œå®ç°äº† PyTorch æ•°æ®åŠ è½½çš„æ ‡å‡†æ¥å£\n",
        "    \n",
        "    ä¸æ‰‹åŠ¨å®ç°çš„åŒºåˆ«ï¼š\n",
        "    - æ‰‹åŠ¨å®ç°ï¼šä½¿ç”¨ tiktoken åˆ†è¯å™¨ï¼Œæ‰‹åŠ¨å¤„ç† padding\n",
        "    - PEFTç‰ˆæœ¬ï¼šä½¿ç”¨ Hugging Face Tokenizerï¼Œè‡ªåŠ¨å¤„ç† padding å’Œ attention_mask\n",
        "    \n",
        "    å…³é”®ç‰¹æ€§ï¼š\n",
        "    1. ä½¿ç”¨ Hugging Face tokenizerï¼Œä¸æ¨¡å‹å®Œç¾é…åˆ\n",
        "    2. è‡ªåŠ¨ç”Ÿæˆ attention_maskï¼ˆæ ‡è®°å“ªäº›æ˜¯çœŸå® tokenï¼Œå“ªäº›æ˜¯ paddingï¼‰\n",
        "    3. æ‰¹é‡é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬ï¼Œæé«˜æ•ˆç‡\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, df, tokenizer, max_length=96):\n",
        "        \"\"\"\n",
        "        åˆå§‹åŒ–æ•°æ®é›†\n",
        "        \n",
        "        å‚æ•°ï¼š\n",
        "            df: pandas DataFrameï¼ŒåŒ…å« 'Text' å’Œ 'Label' åˆ—\n",
        "            tokenizer: Hugging Face tokenizer å¯¹è±¡\n",
        "            max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼Œè¶…è¿‡ä¼šæˆªæ–­ï¼Œä¸è¶³ä¼šå¡«å……\n",
        "            \n",
        "        å¤„ç†æµç¨‹ï¼š\n",
        "            1. æ‰¹é‡åˆ†è¯æ‰€æœ‰æ–‡æœ¬\n",
        "            2. ç»Ÿä¸€é•¿åº¦ï¼ˆæˆªæ–­/å¡«å……åˆ° max_lengthï¼‰\n",
        "            3. è½¬æ¢ä¸º PyTorch tensors\n",
        "            4. ä¿å­˜ labels ä¸ºæ•°å­—å¼ é‡\n",
        "        \"\"\"\n",
        "        # ğŸ”¤ æ‰¹é‡åˆ†è¯æ‰€æœ‰æ–‡æœ¬\n",
        "        # tokenizer() å‡½æ•°çš„å‚æ•°è¯´æ˜ï¼š\n",
        "        # - truncation=True: å¦‚æœæ–‡æœ¬è¶…è¿‡ max_lengthï¼Œè‡ªåŠ¨æˆªæ–­\n",
        "        # - padding=\"max_length\": å¡«å……åˆ° max_length é•¿åº¦\n",
        "        # - max_length=96: æœ€å¤§åºåˆ—é•¿åº¦\n",
        "        # - return_tensors=\"pt\": è¿”å› PyTorch tensors\n",
        "        encodings = tokenizer(\n",
        "            df[\"Text\"].tolist(),          # å°† pandas Series è½¬ä¸º list\n",
        "            truncation=True,               # å¯ç”¨æˆªæ–­\n",
        "            padding=\"max_length\",          # å¡«å……åˆ°æœ€å¤§é•¿åº¦\n",
        "            max_length=max_length,         # æœ€å¤§é•¿åº¦\n",
        "            return_tensors=\"pt\",           # è¿”å› PyTorch å¼ é‡\n",
        "        )\n",
        "        \n",
        "        # ğŸ“Š ä¿å­˜ç¼–ç ç»“æœ\n",
        "        # input_ids: token ID åºåˆ—ï¼Œå½¢çŠ¶ [N, max_length]\n",
        "        self.input_ids = encodings[\"input_ids\"]\n",
        "        \n",
        "        # attention_mask: æ³¨æ„åŠ›æ©ç ï¼Œå½¢çŠ¶ [N, max_length]\n",
        "        # 1 è¡¨ç¤ºçœŸå® tokenï¼Œ0 è¡¨ç¤º padding token\n",
        "        # æ¨¡å‹ä¼šå¿½ç•¥ attention_mask=0 çš„ä½ç½®\n",
        "        self.attention_mask = encodings[\"attention_mask\"]\n",
        "        \n",
        "        # ğŸ·ï¸ ä¿å­˜æ ‡ç­¾\n",
        "        # è½¬æ¢ä¸º long ç±»å‹ï¼ˆint64ï¼‰ï¼Œè¿™æ˜¯ PyTorch äº¤å‰ç†µæŸå¤±è¦æ±‚çš„ç±»å‹\n",
        "        self.labels = torch.tensor(df[\"Label\"].tolist(), dtype=torch.long)\n",
        "        \n",
        "        # ğŸ“ ä¿å­˜æ•°æ®é›†å¤§å°\n",
        "        self._len = len(self.labels)\n",
        "        \n",
        "        print(f\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ:\")\n",
        "        print(f\"   - æ ·æœ¬æ•°: {self._len}\")\n",
        "        print(f\"   - åºåˆ—é•¿åº¦: {max_length}\")\n",
        "        print(f\"   - è¾“å…¥å½¢çŠ¶: {self.input_ids.shape}\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        è¿”å›æ•°æ®é›†å¤§å°\n",
        "        \n",
        "        ğŸ¯ ç›®çš„ï¼šå‘Šè¯‰ DataLoader æ•°æ®é›†æœ‰å¤šå°‘æ ·æœ¬\n",
        "        è¿™æ˜¯ PyTorch Dataset å¿…é¡»å®ç°çš„æ–¹æ³•ä¹‹ä¸€\n",
        "        \"\"\"\n",
        "        return self._len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        è·å–å•ä¸ªæ ·æœ¬\n",
        "        \n",
        "        ğŸ¯ ç›®çš„ï¼šæ ¹æ®ç´¢å¼•è¿”å›ä¸€ä¸ªæ ·æœ¬\n",
        "        è¿™æ˜¯ PyTorch Dataset å¿…é¡»å®ç°çš„æ–¹æ³•ä¹‹ä¸€\n",
        "        \n",
        "        å‚æ•°ï¼š\n",
        "            idx: æ ·æœ¬ç´¢å¼•ï¼ˆ0 åˆ° len-1ï¼‰\n",
        "            \n",
        "        è¿”å›ï¼š\n",
        "            dict: åŒ…å« input_ids, attention_mask, labels çš„å­—å…¸\n",
        "            \n",
        "        è¯´æ˜ï¼š\n",
        "            è¿”å›å­—å…¸æ ¼å¼æ˜¯ Hugging Face æ¨¡å‹çš„æ ‡å‡†è¾“å…¥æ ¼å¼ï¼Œ\n",
        "            å¯ä»¥ç›´æ¥ä½¿ç”¨ model(**batch) è¿›è¡Œè§£åŒ…ä¼ å‚\n",
        "        \"\"\"\n",
        "        return {\n",
        "            \"input_ids\": self.input_ids[idx],           # Token ID åºåˆ—\n",
        "            \"attention_mask\": self.attention_mask[idx], # æ³¨æ„åŠ›æ©ç \n",
        "            \"labels\": self.labels[idx],                 # æ ‡ç­¾ï¼ˆ0 æˆ– 1ï¼‰\n",
        "        }\n",
        "\n",
        "\n",
        "# ğŸ“ åˆå­¦è€…æç¤ºï¼š\n",
        "# Dataset ç±»åªè´Ÿè´£æ•°æ®çš„\"ç»„ç»‡\"ï¼Œä¸è´Ÿè´£æ•°æ®çš„\"æ‰¹é‡åŠ è½½\"ã€‚\n",
        "# æ‰¹é‡åŠ è½½ç”± DataLoader å®Œæˆï¼Œå®ƒä¼šï¼š\n",
        "# 1. è°ƒç”¨ __getitem__ è·å–å¤šä¸ªæ ·æœ¬\n",
        "# 2. è‡ªåŠ¨å°†è¿™äº›æ ·æœ¬ç»„åˆæˆ batch\n",
        "# 3. å¯é€‰åœ°æ‰“ä¹±æ•°æ®é¡ºåºï¼ˆshuffle=Trueï¼‰\n",
        "# 4. å¯é€‰åœ°ä½¿ç”¨å¤šè¿›ç¨‹åŠ é€Ÿï¼ˆnum_workers>0ï¼‰\n",
        "\n",
        "print(\"âœ… Dataset ç±»å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4ï¸âƒ£ æ‰§è¡Œæ•°æ®åŠ è½½\n",
        "\n",
        "## ğŸš€ åˆ›å»º Tokenizer å’ŒåŠ è½½æ•°æ®\n",
        "\n",
        "ç°åœ¨æˆ‘ä»¬å°†ï¼š\n",
        "1. åŠ è½½ GPT-2 tokenizer\n",
        "2. åŠ è½½å¹¶å¤„ç† SMS æ•°æ®é›†\n",
        "3. åˆ›å»º PyTorch DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”¤ åŠ è½½ Tokenizer\n",
        "# åŠŸèƒ½ï¼šä» Hugging Face Hub åŠ è½½é¢„è®­ç»ƒçš„ GPT-2 åˆ†è¯å™¨\n",
        "\n",
        "print(\"ğŸ”¤ åŠ è½½ GPT-2 Tokenizer...\")\n",
        "\n",
        "# ğŸ“¥ ä» Hugging Face ä¸‹è½½å¹¶åŠ è½½ GPT-2 tokenizer\n",
        "# AutoTokenizer.from_pretrained() ä¼šï¼š\n",
        "# 1. æ£€æŸ¥æœ¬åœ°ç¼“å­˜ï¼ˆ~/.cache/huggingface/ï¼‰\n",
        "# 2. å¦‚æœæ²¡æœ‰ï¼Œä» Hub ä¸‹è½½\n",
        "# 3. åŠ è½½è¯è¡¨å’Œåˆ†è¯è§„åˆ™\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "# ğŸ”§ é…ç½® padding token\n",
        "# GPT-2 åŸå§‹è®¾è®¡ç”¨äºç”Ÿæˆä»»åŠ¡ï¼Œæ²¡æœ‰ padding token\n",
        "# ä½†åˆ†ç±»ä»»åŠ¡éœ€è¦å°†ä¸åŒé•¿åº¦çš„æ–‡æœ¬å¯¹é½åˆ°ç›¸åŒé•¿åº¦\n",
        "# è§£å†³æ–¹æ¡ˆï¼šå°† eos_tokenï¼ˆç»“æŸç¬¦ï¼‰å¤ç”¨ä¸º pad_token\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    print(f\"   âš™ï¸  è®¾ç½® pad_token = eos_token (ID: {tokenizer.eos_token_id})\")\n",
        "\n",
        "print(f\"âœ… Tokenizer åŠ è½½å®Œæˆ\")\n",
        "print(f\"   - è¯è¡¨å¤§å°: {tokenizer.vocab_size:,}\")\n",
        "print(f\"   - æ¨¡å‹ç±»å‹: {tokenizer.__class__.__name__}\")\n",
        "\n",
        "# ğŸ“Š åŠ è½½å¹¶åˆ’åˆ†æ•°æ®\n",
        "print(\"\\nğŸ“Š åŠ è½½ SMS æ•°æ®é›†...\")\n",
        "train_df, val_df, test_df = load_sms_dataframe()\n",
        "\n",
        "# ğŸ“ è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦\n",
        "# è¯´æ˜ï¼šè¿™ä¸ªå€¼éœ€è¦å¹³è¡¡ä»¥ä¸‹å› ç´ ï¼š\n",
        "# - å¤ªçŸ­ï¼šå¯èƒ½æˆªæ–­é‡è¦ä¿¡æ¯\n",
        "# - å¤ªé•¿ï¼šå¢åŠ è®¡ç®—æˆæœ¬å’Œæ˜¾å­˜å ç”¨\n",
        "# - 96 æ˜¯ç»è¿‡å®éªŒçš„åˆç†å€¼ï¼Œè¶³ä»¥è¦†ç›–å¤§éƒ¨åˆ†çŸ­ä¿¡\n",
        "max_length = 96\n",
        "print(f\"\\nğŸ“ è®¾ç½®æœ€å¤§åºåˆ—é•¿åº¦: {max_length}\")\n",
        "\n",
        "# ğŸ”¨ åˆ›å»º PyTorch Dataset\n",
        "print(\"\\nğŸ”¨ åˆ›å»º PyTorch Dataset...\")\n",
        "train_dataset = SpamSequenceDataset(train_df, tokenizer, max_length=max_length)\n",
        "val_dataset = SpamSequenceDataset(val_df, tokenizer, max_length=max_length)\n",
        "test_dataset = SpamSequenceDataset(test_df, tokenizer, max_length=max_length)\n",
        "\n",
        "# ğŸ“¦ åˆ›å»º DataLoader\n",
        "# åŠŸèƒ½ï¼šæ‰¹é‡åŠ è½½æ•°æ®ï¼Œæ”¯æŒè‡ªåŠ¨æ‰“ä¹±ã€å¤šè¿›ç¨‹ç­‰\n",
        "print(\"\\nğŸ“¦ åˆ›å»º DataLoader...\")\n",
        "\n",
        "# ğŸ¯ DataLoader å‚æ•°è¯´æ˜ï¼š\n",
        "# - batch_size: æ¯æ‰¹æ ·æœ¬æ•°é‡\n",
        "#   - è¾ƒå¤§ï¼šè®­ç»ƒå¿«ï¼Œä½†æ˜¾å­˜å ç”¨é«˜\n",
        "#   - è¾ƒå°ï¼šæ˜¾å­˜å‹å¥½ï¼Œä½†è®­ç»ƒæ…¢\n",
        "#   - 8 æ˜¯é€‚ä¸­çš„å€¼\n",
        "# - shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®\n",
        "#   - è®­ç»ƒé›†ï¼šTrueï¼ˆé¿å…é¡ºåºåå·®ï¼‰\n",
        "#   - éªŒè¯/æµ‹è¯•é›†ï¼šFalseï¼ˆä¿æŒé¡ºåºï¼Œä¾¿äºåˆ†æï¼‰\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"âœ… DataLoader åˆ›å»ºå®Œæˆ\")\n",
        "print(f\"   - è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}\")\n",
        "print(f\"   - éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}\")\n",
        "print(f\"   - æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}\")\n",
        "print(f\"   - æ¯æ‰¹å¤§å°: 8\")\n",
        "\n",
        "# ğŸ” æ•°æ®æ£€æŸ¥ï¼šæŸ¥çœ‹ä¸€ä¸ªæ ·æœ¬\n",
        "print(\"\\nğŸ” æ ·æœ¬æ£€æŸ¥:\")\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"   - input_ids å½¢çŠ¶: {sample_batch['input_ids'].shape}\")\n",
        "print(f\"   - attention_mask å½¢çŠ¶: {sample_batch['attention_mask'].shape}\")\n",
        "print(f\"   - labels å½¢çŠ¶: {sample_batch['labels'].shape}\")\n",
        "print(f\"   - æ ‡ç­¾åˆ†å¸ƒ: {sample_batch['labels'].tolist()}\")\n",
        "\n",
        "print(\"\\nâœ… æ•°æ®å‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5ï¸âƒ£ ä½¿ç”¨ PEFT åº“åº”ç”¨ LoRA\n",
        "\n",
        "## ğŸ¯ PEFT æ ¸å¿ƒæ¦‚å¿µ\n",
        "\n",
        "**PEFT (Parameter-Efficient Fine-Tuning)** æ˜¯ Hugging Face æä¾›çš„å‚æ•°é«˜æ•ˆå¾®è°ƒåº“ï¼Œæ”¯æŒå¤šç§æ–¹æ³•ï¼š\n",
        "\n",
        "- **LoRA**: Low-Rank Adaptationï¼ˆæœ¬æ•™ç¨‹ä½¿ç”¨ï¼‰\n",
        "- **LoHa**: Low-Rank Hadamard Product\n",
        "- **LoKr**: Low-Rank Kronecker Product\n",
        "- **Prefix Tuning**: å¯å­¦ä¹ çš„å‰ç¼€\n",
        "- **P-Tuning**: æç¤ºå¾®è°ƒ\n",
        "- **Adapter**: é€‚é…å™¨å±‚\n",
        "\n",
        "## ğŸ”§ LoRA é…ç½®è¯¦è§£\n",
        "\n",
        "ä½¿ç”¨ PEFT åº“åº”ç”¨ LoRA åªéœ€ä¸‰æ­¥ï¼š\n",
        "1. åˆ›å»º `LoraConfig` é…ç½®å¯¹è±¡\n",
        "2. åŠ è½½åŸºç¡€æ¨¡å‹\n",
        "3. ä½¿ç”¨ `get_peft_model()` åº”ç”¨ LoRA\n",
        "\n",
        "è®©æˆ‘ä»¬æ·±å…¥äº†è§£æ¯ä¸ªå‚æ•°çš„å«ä¹‰ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ¤– åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åº”ç”¨ LoRA\n",
        "# åŠŸèƒ½ï¼šè¿™æ˜¯ PEFT åº“ä½¿ç”¨çš„æ ¸å¿ƒæ­¥éª¤\n",
        "\n",
        "print(\"ğŸ¤– æ­¥éª¤ 1: åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ğŸ“¥ ä» Hugging Face Hub åŠ è½½ GPT-2 åˆ†ç±»æ¨¡å‹\n",
        "# AutoModelForSequenceClassification ä¼šï¼š\n",
        "# 1. åŠ è½½ GPT-2 çš„é¢„è®­ç»ƒæƒé‡\n",
        "# 2. åœ¨é¡¶éƒ¨æ·»åŠ ä¸€ä¸ªåˆ†ç±»å¤´ï¼ˆçº¿æ€§å±‚ï¼‰\n",
        "# 3. num_labels=2 è¡¨ç¤ºäºŒåˆ†ç±»ä»»åŠ¡\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"gpt2\",           # æ¨¡å‹åç§°\n",
        "    num_labels=2      # åˆ†ç±»ç±»åˆ«æ•°\n",
        ")\n",
        "\n",
        "# ğŸ”§ é…ç½®æ¨¡å‹çš„ padding token\n",
        "# å¿…é¡»ä¸ tokenizer çš„é…ç½®ä¿æŒä¸€è‡´\n",
        "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# ğŸ“ è°ƒæ•´ embedding å±‚å¤§å°\n",
        "# GPT-2 çš„è¯è¡¨å¤§å°æ˜¯ 50257ï¼Œå¦‚æœ tokenizer æ·»åŠ äº†æ–° tokenï¼Œéœ€è¦è°ƒæ•´\n",
        "base_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ\")\n",
        "print(f\"   - æ¨¡å‹ç±»å‹: {base_model.__class__.__name__}\")\n",
        "print(f\"   - åˆ†ç±»ç±»åˆ«: {base_model.config.num_labels}\")\n",
        "print(f\"   - è¯è¡¨å¤§å°: {base_model.config.vocab_size:,}\")\n",
        "\n",
        "# ğŸ”§ æ­¥éª¤ 2: åˆ›å»º LoRA é…ç½®\n",
        "print(\"\\nğŸ”§ æ­¥éª¤ 2: åˆ›å»º LoRA é…ç½®...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ğŸ“‹ LoRA é…ç½®å‚æ•°è¯¦è§£\n",
        "lora_config = LoraConfig(\n",
        "    # ğŸ¯ task_type: ä»»åŠ¡ç±»å‹\n",
        "    # - TaskType.SEQ_CLS: åºåˆ—åˆ†ç±»ä»»åŠ¡\n",
        "    # - TaskType.CAUSAL_LM: å› æœè¯­è¨€æ¨¡å‹ï¼ˆç”Ÿæˆä»»åŠ¡ï¼‰\n",
        "    # - TaskType.SEQ_2_SEQ_LM: åºåˆ—åˆ°åºåˆ—ä»»åŠ¡\n",
        "    # ä¸åŒä»»åŠ¡ç±»å‹ä¼šå½±å“ LoRA å±‚çš„æ’å…¥ä½ç½®\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        "    \n",
        "    # ğŸ“Š r: LoRA çš„ç§©ï¼ˆrankï¼‰\n",
        "    # è¿™æ˜¯ LoRA æœ€æ ¸å¿ƒçš„è¶…å‚æ•°ï¼\n",
        "    # - æ§åˆ¶ A å’Œ B çŸ©é˜µçš„ä¸­é—´ç»´åº¦\n",
        "    # - è¾ƒå°çš„å€¼ï¼ˆå¦‚ 4-8ï¼‰ï¼šå‚æ•°å°‘ï¼Œè®­ç»ƒå¿«ï¼Œä½†è¡¨è¾¾èƒ½åŠ›å¼±\n",
        "    # - è¾ƒå¤§çš„å€¼ï¼ˆå¦‚ 16-32ï¼‰ï¼šå‚æ•°å¤šï¼Œè¡¨è¾¾èƒ½åŠ›å¼ºï¼Œä½†æ¥è¿‘å…¨é‡å¾®è°ƒ\n",
        "    # - æ¨èèµ·å§‹å€¼ï¼š8\n",
        "    # - æœ¬æ•™ç¨‹ä½¿ç”¨ï¼š8ï¼ˆå¹³è¡¡æ•ˆç‡å’Œæ€§èƒ½ï¼‰\n",
        "    r=8,\n",
        "    \n",
        "    # ğŸšï¸ lora_alpha: LoRA ç¼©æ”¾å› å­\n",
        "    # - æ§åˆ¶ LoRA è¾“å‡ºçš„å½±å“å¼ºåº¦\n",
        "    # - å¸¸è§è®¾ç½®ï¼šalpha = 2Ã—r æˆ– alpha = r\n",
        "    # - æœ¬æ•™ç¨‹ï¼šalpha = 2Ã—r = 16\n",
        "    # - æ•°å­¦å…¬å¼ï¼šLoRA è¾“å‡ºä¼šä¹˜ä»¥ (alpha/r) çš„ç¼©æ”¾ç³»æ•°\n",
        "    lora_alpha=16,\n",
        "    \n",
        "    # ğŸ² lora_dropout: Dropout æ¯”ç‡\n",
        "    # - åœ¨ LoRA å±‚ä¸­åº”ç”¨ dropout æ­£åˆ™åŒ–\n",
        "    # - èŒƒå›´ï¼š0.0ï¼ˆä¸ä½¿ç”¨ï¼‰åˆ° 0.2ï¼ˆè¾ƒå¼ºæ­£åˆ™åŒ–ï¼‰\n",
        "    # - ä½œç”¨ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæé«˜æ³›åŒ–èƒ½åŠ›\n",
        "    # - æœ¬æ•™ç¨‹ï¼š0.05ï¼ˆè½»å¾®æ­£åˆ™åŒ–ï¼‰\n",
        "    lora_dropout=0.05,\n",
        "    \n",
        "    # ğŸ¯ target_modules: è¦æ›¿æ¢çš„æ¨¡å—åç§°åˆ—è¡¨\n",
        "    # è¿™æ˜¯ PEFT åº“çš„å¼ºå¤§åŠŸèƒ½ï¼šé€‰æ‹©æ€§åœ°åªå¯¹éƒ¨åˆ†å±‚åº”ç”¨ LoRA\n",
        "    # GPT-2 çš„æ¨¡å—åç§°ï¼š\n",
        "    # - \"c_attn\": æ³¨æ„åŠ›å±‚çš„ QKV æŠ•å½±ï¼ˆæœ€é‡è¦ï¼‰\n",
        "    # - \"c_proj\": æ³¨æ„åŠ›å±‚çš„è¾“å‡ºæŠ•å½±\n",
        "    # - \"c_fc\": å‰é¦ˆç½‘ç»œçš„ç¬¬ä¸€å±‚\n",
        "    # - \"c_proj\": å‰é¦ˆç½‘ç»œçš„ç¬¬äºŒå±‚ï¼ˆä¸æ³¨æ„åŠ›åŒåï¼ŒPEFT ä¼šåŒºåˆ†ï¼‰\n",
        "    # \n",
        "    # ğŸ’¡ é€‰æ‹©ç­–ç•¥ï¼š\n",
        "    # - åªæ›¿æ¢ [\"c_attn\"]: æœ€å°‘å‚æ•°ï¼Œé€‚åˆæ˜¾å­˜æå°åœºæ™¯\n",
        "    # - æ›¿æ¢ [\"c_attn\", \"c_proj\"]: å¹³è¡¡é€‰æ‹©\n",
        "    # - æ›¿æ¢ [\"c_attn\", \"c_fc\", \"c_proj\"]: æœ¬æ•™ç¨‹ä½¿ç”¨ï¼Œè¦†ç›–ä¸»è¦å±‚\n",
        "    # - ä¸å»ºè®®æ›¿æ¢ embedding å±‚å’Œåˆ†ç±»å¤´\n",
        "    target_modules=[\"c_attn\", \"c_fc\", \"c_proj\"],\n",
        "    \n",
        "    # âš™ï¸ å…¶ä»–å¯é€‰å‚æ•°ï¼ˆä½¿ç”¨é»˜è®¤å€¼ï¼‰ï¼š\n",
        "    # - bias=\"none\": ä¸è®­ç»ƒ bias å‚æ•°\n",
        "    # - fan_in_fan_out=False: æƒé‡çŸ©é˜µçš„ç»„ç»‡æ–¹å¼\n",
        "    # - modules_to_save=None: é™¤ LoRA å¤–éœ€è¦è®­ç»ƒçš„æ¨¡å—\n",
        ")\n",
        "\n",
        "print(\"âœ… LoRA é…ç½®åˆ›å»ºå®Œæˆ:\")\n",
        "print(f\"   - ç§© (r): {lora_config.r}\")\n",
        "print(f\"   - ç¼©æ”¾å› å­ (alpha): {lora_config.lora_alpha}\")\n",
        "print(f\"   - Dropout: {lora_config.lora_dropout}\")\n",
        "print(f\"   - ç›®æ ‡æ¨¡å—: {lora_config.target_modules}\")\n",
        "\n",
        "# ğŸš€ æ­¥éª¤ 3: åº”ç”¨ LoRA åˆ°æ¨¡å‹\n",
        "print(\"\\nğŸš€ æ­¥éª¤ 3: åº”ç”¨ LoRA...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ğŸ”‘ æ ¸å¿ƒå‡½æ•°ï¼šget_peft_model()\n",
        "# åŠŸèƒ½ï¼šå°† LoRA é…ç½®åº”ç”¨åˆ°åŸºç¡€æ¨¡å‹\n",
        "# å·¥ä½œåŸç†ï¼š\n",
        "# 1. éå†æ¨¡å‹çš„æ‰€æœ‰æ¨¡å—\n",
        "# 2. æ‰¾åˆ° target_modules ä¸­æŒ‡å®šçš„çº¿æ€§å±‚\n",
        "# 3. ç”¨ LoRA å¢å¼ºç‰ˆæœ¬æ›¿æ¢è¿™äº›å±‚\n",
        "# 4. å†»ç»“åŸå§‹å‚æ•°ï¼Œåªè®© LoRA å‚æ•°å¯è®­ç»ƒ\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "print(\"âœ… LoRA åº”ç”¨å®Œæˆï¼\")\n",
        "\n",
        "# ğŸ“Š æ‰“å°å‚æ•°ç»Ÿè®¡\n",
        "# print_trainable_parameters() æ˜¯ PEFT æ¨¡å‹çš„ä¾¿æ·æ–¹æ³•\n",
        "# ä¼šæ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°æ•°é‡å’Œå æ¯”\n",
        "print(\"\\nğŸ“Š å‚æ•°ç»Ÿè®¡:\")\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# ğŸ® å°†æ¨¡å‹ç§»åŠ¨åˆ° GPU/CPU\n",
        "model.to(device)\n",
        "print(f\"\\nğŸ® æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}\")\n",
        "\n",
        "# ğŸ” æ¨¡å‹ç»“æ„æ£€æŸ¥ï¼ˆå¯é€‰ï¼‰\n",
        "print(\"\\nğŸ” æ¨¡å‹ç»“æ„é¢„è§ˆ:\")\n",
        "print(f\"   - æ¨¡å‹ç±»å‹: {type(model).__name__}\")\n",
        "print(f\"   - åŸºç¡€æ¨¡å‹: {type(model.base_model).__name__}\")\n",
        "print(f\"   - LoRA é…ç½®: {model.peft_config}\")\n",
        "\n",
        "print(\"\\nâœ… æ¨¡å‹å‡†å¤‡å®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6ï¸âƒ£ è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°\n",
        "\n",
        "## ğŸ¯ è®­ç»ƒæµç¨‹æ¦‚è¿°\n",
        "\n",
        "æ ‡å‡†çš„æ·±åº¦å­¦ä¹ è®­ç»ƒæµç¨‹åŒ…æ‹¬ï¼š\n",
        "1. **è®­ç»ƒå¾ªç¯**ï¼šå‰å‘ä¼ æ’­ â†’ è®¡ç®—æŸå¤± â†’ åå‘ä¼ æ’­ â†’ æ›´æ–°å‚æ•°\n",
        "2. **è¯„ä¼°å¾ªç¯**ï¼šåœ¨éªŒè¯é›†ä¸Šè®¡ç®—æ€§èƒ½æŒ‡æ ‡\n",
        "3. **å­¦ä¹ ç‡è°ƒåº¦**ï¼šåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡\n",
        "4. **æ—©åœæœºåˆ¶**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "\n",
        "## ğŸ“Š PEFT æ¨¡å‹çš„è®­ç»ƒç‰¹ç‚¹\n",
        "\n",
        "ä½¿ç”¨ PEFT åï¼Œè®­ç»ƒä»£ç ä¸æ™®é€š PyTorch æ¨¡å‹**å®Œå…¨ç›¸åŒ**ï¼\n",
        "- âœ… è‡ªåŠ¨åªä¼˜åŒ– LoRA å‚æ•°\n",
        "- âœ… è‡ªåŠ¨å†»ç»“åŸå§‹å‚æ•°\n",
        "- âœ… æ— éœ€æ‰‹åŠ¨ç­›é€‰å‚æ•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ“Š è¯„ä¼°å‡½æ•°å®šä¹‰\n",
        "# åŠŸèƒ½ï¼šè®¡ç®—æ¨¡å‹åœ¨æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡å’ŒæŸå¤±\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    \"\"\"\n",
        "    è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
        "    \n",
        "    ğŸ¯ ç›®çš„ï¼šåœ¨éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸Šè®¡ç®—å‡†ç¡®ç‡å’Œå¹³å‡æŸå¤±\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        model: å¾…è¯„ä¼°çš„æ¨¡å‹\n",
        "        data_loader: DataLoader å¯¹è±¡ï¼ˆéªŒè¯é›†æˆ–æµ‹è¯•é›†ï¼‰\n",
        "        \n",
        "    è¿”å›ï¼š\n",
        "        acc: å‡†ç¡®ç‡ (0-1 ä¹‹é—´)\n",
        "        avg_loss: å¹³å‡æŸå¤±å€¼\n",
        "        \n",
        "    å·¥ä½œæµç¨‹ï¼š\n",
        "        1. åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ (model.eval())\n",
        "        2. ç¦ç”¨æ¢¯åº¦è®¡ç®— (torch.no_grad())\n",
        "        3. éå†æ‰€æœ‰æ‰¹æ¬¡è®¡ç®—æŸå¤±å’Œå‡†ç¡®ç‡\n",
        "        4. è¿”å›å¹³å‡å€¼\n",
        "    \"\"\"\n",
        "    # ğŸ”§ åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
        "    # ä½œç”¨ï¼š\n",
        "    # 1. ç¦ç”¨ Dropoutï¼ˆæ‰€æœ‰ç¥ç»å…ƒéƒ½å‚ä¸è®¡ç®—ï¼‰\n",
        "    # 2. ç¦ç”¨ Batch Normalization çš„æ›´æ–°ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„ç»Ÿè®¡é‡ï¼‰\n",
        "    # 3. æŸäº›å±‚çš„è¡Œä¸ºä¼šæ”¹å˜\n",
        "    model.eval()\n",
        "    \n",
        "    # ğŸ“Š åˆå§‹åŒ–ç»Ÿè®¡å˜é‡\n",
        "    total_loss = 0.0    # ç´¯ç§¯æŸå¤±\n",
        "    correct = 0         # æ­£ç¡®é¢„æµ‹æ•°\n",
        "    total = 0           # æ€»æ ·æœ¬æ•°\n",
        "    \n",
        "    # ğŸš« ç¦ç”¨æ¢¯åº¦è®¡ç®—\n",
        "    # ä½œç”¨ï¼šèŠ‚çœæ˜¾å­˜ï¼ŒåŠ å¿«è®¡ç®—é€Ÿåº¦\n",
        "    # è¯„ä¼°æ—¶ä¸éœ€è¦æ¢¯åº¦ï¼Œå› ä¸ºä¸è¿›è¡Œå‚æ•°æ›´æ–°\n",
        "    with torch.no_grad():\n",
        "        # ğŸ”„ éå†æ•°æ®æ‰¹æ¬¡\n",
        "        for batch in data_loader:\n",
        "            # ğŸ® å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡ï¼ˆGPU/CPUï¼‰\n",
        "            # å­—å…¸æ¨å¯¼å¼ï¼šå°† batch ä¸­çš„æ¯ä¸ªå¼ é‡éƒ½ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            \n",
        "            # ğŸ”® å‰å‘ä¼ æ’­\n",
        "            # model(**batch) ç­‰ä»·äº model(input_ids=..., attention_mask=..., labels=...)\n",
        "            # Hugging Face æ¨¡å‹è¿”å›ä¸€ä¸ªåŒ…å« loss å’Œ logits çš„å¯¹è±¡\n",
        "            outputs = model(**batch)\n",
        "            \n",
        "            # ğŸ“‰ æå–æŸå¤±å€¼\n",
        "            # å½“ä¼ å…¥ labels æ—¶ï¼Œæ¨¡å‹ä¼šè‡ªåŠ¨è®¡ç®—äº¤å‰ç†µæŸå¤±\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            # ğŸ¯ è·å–é¢„æµ‹ç»“æœ\n",
        "            # logits: [batch_size, num_labels] å½¢çŠ¶çš„å¼ é‡\n",
        "            # argmax(dim=-1): æ²¿æœ€åä¸€ç»´å–æœ€å¤§å€¼çš„ç´¢å¼•ï¼Œå¾—åˆ°é¢„æµ‹ç±»åˆ«\n",
        "            preds = outputs.logits.argmax(dim=-1)\n",
        "            \n",
        "            # ğŸ“Š ç´¯ç§¯ç»Ÿè®¡é‡\n",
        "            total_loss += loss.item()  # ç´¯åŠ æŸå¤±ï¼ˆè½¬ä¸º Python æ•°å€¼ï¼‰\n",
        "            correct += (preds == batch[\"labels\"]).sum().item()  # ç»Ÿè®¡æ­£ç¡®é¢„æµ‹æ•°\n",
        "            total += batch[\"labels\"].size(0)  # ç´¯åŠ æ ·æœ¬æ•°\n",
        "    \n",
        "    # ğŸ“ è®¡ç®—å¹³å‡å€¼\n",
        "    avg_loss = total_loss / max(len(data_loader), 1)  # å¹³å‡æŸå¤±\n",
        "    acc = correct / total if total > 0 else 0.0        # å‡†ç¡®ç‡\n",
        "    \n",
        "    return acc, avg_loss\n",
        "\n",
        "\n",
        "# ğŸš€ è®­ç»ƒå‡½æ•°å®šä¹‰\n",
        "# åŠŸèƒ½ï¼šæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹\n",
        "\n",
        "def train(model, train_loader, val_loader, epochs=3, lr=5e-4):\n",
        "    \"\"\"\n",
        "    è®­ç»ƒ LoRA æ¨¡å‹\n",
        "    \n",
        "    ğŸ¯ ç›®çš„ï¼šæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼ŒåŒ…æ‹¬ä¼˜åŒ–å™¨ã€å­¦ä¹ ç‡è°ƒåº¦å’Œæ¢¯åº¦è£å‰ª\n",
        "    \n",
        "    å‚æ•°ï¼š\n",
        "        model: PEFT æ¨¡å‹ï¼ˆå·²åº”ç”¨ LoRAï¼‰\n",
        "        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
        "        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨\n",
        "        epochs: è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 3ï¼‰\n",
        "        lr: å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 5e-4ï¼‰\n",
        "        \n",
        "    è®­ç»ƒæµç¨‹ï¼š\n",
        "        æ¯ä¸ª epoch:\n",
        "            1. è®­ç»ƒé˜¶æ®µï¼šéå†è®­ç»ƒé›†ï¼Œæ›´æ–°å‚æ•°\n",
        "            2. è¯„ä¼°é˜¶æ®µï¼šåœ¨éªŒè¯é›†ä¸Šè®¡ç®—æ€§èƒ½\n",
        "            3. å­¦ä¹ ç‡è°ƒåº¦ï¼šæ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´å­¦ä¹ ç‡\n",
        "    \n",
        "    ğŸ’¡ LoRA å¾®è°ƒçš„å­¦ä¹ ç‡å»ºè®®ï¼š\n",
        "    - å…¨é‡å¾®è°ƒï¼š1e-5 ~ 5e-5ï¼ˆå°å­¦ä¹ ç‡ï¼‰\n",
        "    - LoRA å¾®è°ƒï¼š5e-4 ~ 1e-3ï¼ˆå¯ä»¥ç”¨è¾ƒå¤§å­¦ä¹ ç‡ï¼‰\n",
        "    \"\"\"\n",
        "    print(\"ğŸš€ å¼€å§‹è®­ç»ƒ...\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # âš™ï¸ åˆ›å»ºä¼˜åŒ–å™¨\n",
        "    # AdamW: Adam ä¼˜åŒ–å™¨çš„æ”¹è¿›ç‰ˆï¼Œå¸¦æƒé‡è¡°å‡ï¼ˆWeight Decayï¼‰\n",
        "    # model.parameters() ä¼šè‡ªåŠ¨åªè¿”å› requires_grad=True çš„å‚æ•°\n",
        "    # å› æ­¤è¿™é‡Œä¼šè‡ªåŠ¨åªä¼˜åŒ– LoRA å‚æ•°ï¼\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    \n",
        "    print(f\"âš™ï¸  ä¼˜åŒ–å™¨: AdamW (lr={lr})\")\n",
        "    \n",
        "    # ğŸ“… åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨\n",
        "    # get_linear_schedule_with_warmup: å¸¦é¢„çƒ­çš„çº¿æ€§å­¦ä¹ ç‡è°ƒåº¦\n",
        "    # \n",
        "    # ğŸ”¥ Warmupï¼ˆé¢„çƒ­ï¼‰çš„ä½œç”¨ï¼š\n",
        "    # - è®­ç»ƒåˆæœŸä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡ï¼Œé€æ¸å¢åŠ åˆ°ç›®æ ‡å­¦ä¹ ç‡\n",
        "    # - é˜²æ­¢è®­ç»ƒåˆæœŸæ¢¯åº¦è¿‡å¤§å¯¼è‡´çš„ä¸ç¨³å®š\n",
        "    # - å…¬å¼ï¼šlr = base_lr * (current_step / warmup_steps)\n",
        "    # \n",
        "    # ğŸ“‰ çº¿æ€§è¡°å‡ï¼š\n",
        "    # - Warmup ç»“æŸåï¼Œå­¦ä¹ ç‡çº¿æ€§é™ä½åˆ° 0\n",
        "    # - æœ‰åŠ©äºè®­ç»ƒåæœŸçš„ç²¾ç»†è°ƒæ•´\n",
        "    total_steps = epochs * len(train_loader)  # æ€»è®­ç»ƒæ­¥æ•°\n",
        "    warmup_steps = max(10, int(0.1 * total_steps))  # Warmup æ­¥æ•°ï¼ˆ10% çš„æ€»æ­¥æ•°ï¼‰\n",
        "    \n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=warmup_steps,\n",
        "        num_training_steps=total_steps,\n",
        "    )\n",
        "    \n",
        "    print(f\"ğŸ“… å­¦ä¹ ç‡è°ƒåº¦å™¨: Linear with Warmup\")\n",
        "    print(f\"   - æ€»æ­¥æ•°: {total_steps}\")\n",
        "    print(f\"   - Warmup æ­¥æ•°: {warmup_steps}\")\n",
        "    \n",
        "    # ğŸ”„ è®­ç»ƒå¾ªç¯\n",
        "    for epoch in range(epochs):\n",
        "        # ğŸ“š è®­ç»ƒé˜¶æ®µ\n",
        "        model.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼\n",
        "        running_loss = 0.0\n",
        "        \n",
        "        # ğŸ“Š ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
        "        \n",
        "        for batch in pbar:\n",
        "            # ğŸ® æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            \n",
        "            # ğŸ§¹ æ¸…ç©ºæ¢¯åº¦\n",
        "            # PyTorch é»˜è®¤ç´¯ç§¯æ¢¯åº¦ï¼Œå¿…é¡»æ‰‹åŠ¨æ¸…é›¶\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # ğŸ”® å‰å‘ä¼ æ’­\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            \n",
        "            # ğŸ”„ åå‘ä¼ æ’­\n",
        "            # è®¡ç®—æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦\n",
        "            loss.backward()\n",
        "            \n",
        "            # âœ‚ï¸ æ¢¯åº¦è£å‰ª\n",
        "            # é™åˆ¶æ¢¯åº¦èŒƒæ•°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
        "            # max_norm=1.0: å¦‚æœæ¢¯åº¦èŒƒæ•° > 1.0ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾åˆ° 1.0\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            \n",
        "            # âš¡ å‚æ•°æ›´æ–°\n",
        "            optimizer.step()      # ä½¿ç”¨æ¢¯åº¦æ›´æ–°å‚æ•°\n",
        "            scheduler.step()      # æ›´æ–°å­¦ä¹ ç‡\n",
        "            \n",
        "            # ğŸ“Š ç´¯ç§¯æŸå¤±\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "            # ğŸ“ˆ æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º\n",
        "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "        \n",
        "        # ğŸ“Š è®¡ç®—æœ¬è½®å¹³å‡æŸå¤±\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        \n",
        "        # ğŸ“ˆ éªŒè¯é˜¶æ®µ\n",
        "        val_acc, val_loss = evaluate(model, val_loader)\n",
        "        \n",
        "        # ğŸ“¢ æ‰“å°ç»“æœ\n",
        "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
        "        print(f\"   - è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}\")\n",
        "        print(f\"   - éªŒè¯æŸå¤±: {val_loss:.4f}\")\n",
        "        print(f\"   - éªŒè¯å‡†ç¡®ç‡: {val_acc*100:.1f}%\")\n",
        "        print(f\"   - å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}\")\n",
        "    \n",
        "    print(\"\\nâœ… è®­ç»ƒå®Œæˆï¼\")\n",
        "\n",
        "\n",
        "# ğŸ“ åˆå­¦è€…æç¤ºï¼š\n",
        "# ä»¥ä¸Šè®­ç»ƒå‡½æ•°é›†æˆäº†å¤šä¸ªè®­ç»ƒæŠ€å·§ï¼š\n",
        "# 1. AdamW ä¼˜åŒ–å™¨ï¼šæ¯” SGD æ”¶æ•›æ›´å¿«ï¼Œæ¯” Adam æ³›åŒ–æ›´å¥½\n",
        "# 2. å­¦ä¹ ç‡é¢„çƒ­ï¼šè®­ç»ƒåˆæœŸç¨³å®šæ€§æ›´å¥½\n",
        "# 3. æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸\n",
        "# 4. è¿›åº¦æ¡æ˜¾ç¤ºï¼šå®æ—¶ç›‘æ§è®­ç»ƒçŠ¶æ€\n",
        "# \n",
        "# è¿™äº›éƒ½æ˜¯å·¥ä¸šç•Œçš„æ ‡å‡†åšæ³•ï¼Œå»ºè®®åœ¨è‡ªå·±çš„é¡¹ç›®ä¸­ä½¿ç”¨ã€‚\n",
        "\n",
        "print(\"âœ… è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°å®šä¹‰å®Œæˆ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸš€ æ‰§è¡Œè®­ç»ƒ\n",
        "# åŠŸèƒ½ï¼šè°ƒç”¨è®­ç»ƒå‡½æ•°å¼€å§‹å¾®è°ƒ\n",
        "\n",
        "# âš™ï¸ è®­ç»ƒå‚æ•°è®¾ç½®\n",
        "EPOCHS = 3          # è®­ç»ƒè½®æ•°\n",
        "LEARNING_RATE = 5e-4  # å­¦ä¹ ç‡\n",
        "\n",
        "print(\"âš™ï¸  è®­ç»ƒå‚æ•°:\")\n",
        "print(f\"   - è®­ç»ƒè½®æ•°: {EPOCHS}\")\n",
        "print(f\"   - å­¦ä¹ ç‡: {LEARNING_RATE}\")\n",
        "print(f\"   - æ‰¹æ¬¡å¤§å°: 8\")\n",
        "print(f\"   - ä¼˜åŒ–å™¨: AdamW\")\n",
        "print(f\"   - å­¦ä¹ ç‡è°ƒåº¦: Linear with Warmup\")\n",
        "print(\"\")\n",
        "\n",
        "# ğŸš€ å¼€å§‹è®­ç»ƒ\n",
        "train(model, train_loader, val_loader, epochs=EPOCHS, lr=LEARNING_RATE)\n",
        "\n",
        "# ğŸ“Š åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°\n",
        "print(\"\\nğŸ“Š æœ€ç»ˆæµ‹è¯•...\")\n",
        "print(\"=\" * 60)\n",
        "test_acc, test_loss = evaluate(model, test_loader)\n",
        "\n",
        "print(f\"âœ… æµ‹è¯•ç»“æœ:\")\n",
        "print(f\"   - æµ‹è¯•æŸå¤±: {test_loss:.4f}\")\n",
        "print(f\"   - æµ‹è¯•å‡†ç¡®ç‡: {test_acc*100:.2f}%\")\n",
        "\n",
        "# ğŸ¯ æ€§èƒ½åˆ†æ\n",
        "if test_acc >= 0.95:\n",
        "    print(\"\\nğŸ‰ ä¼˜ç§€ï¼æ¨¡å‹è¾¾åˆ°äº† 95%+ çš„å‡†ç¡®ç‡ï¼\")\n",
        "elif test_acc >= 0.90:\n",
        "    print(\"\\nğŸ‘ å¾ˆå¥½ï¼æ¨¡å‹è¾¾åˆ°äº† 90%+ çš„å‡†ç¡®ç‡ï¼\")\n",
        "elif test_acc >= 0.85:\n",
        "    print(\"\\nâœ… ä¸é”™ï¼æ¨¡å‹è¾¾åˆ°äº† 85%+ çš„å‡†ç¡®ç‡ï¼\")\n",
        "else:\n",
        "    print(\"\\nğŸ’¡ æç¤ºï¼šå‡†ç¡®ç‡åä½ï¼Œå»ºè®®è°ƒæ•´è¶…å‚æ•°æˆ–å¢åŠ è®­ç»ƒè½®æ•°\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8ï¸âƒ£ å®é™…åº”ç”¨ï¼šæ–‡æœ¬åˆ†ç±»æ¨ç†\n",
        "\n",
        "## ğŸ¯ ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
        "\n",
        "ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†ç±»ï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ã€‚\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”® æ¨ç†æ¼”ç¤º\n",
        "# åŠŸèƒ½ï¼šä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†ç±»é¢„æµ‹\n",
        "\n",
        "# ğŸ“ å‡†å¤‡æµ‹è¯•æ ·æœ¬\n",
        "# åŒ…å«æ˜æ˜¾çš„æ­£å¸¸çŸ­ä¿¡å’Œåƒåœ¾çŸ­ä¿¡\n",
        "sample_texts = [\n",
        "    \"Hey, want to grab lunch together?\",                           # æ­£å¸¸çŸ­ä¿¡\n",
        "    \"URGENT! You've won $5000! Click now to claim your prize!\",   # åƒåœ¾çŸ­ä¿¡\n",
        "    \"The meeting has been moved to 2pm\",                          # æ­£å¸¸çŸ­ä¿¡\n",
        "    \"Free iPhone! Limited time offer! Call immediately!\",         # åƒåœ¾çŸ­ä¿¡\n",
        "    \"Thanks for your help yesterday\",                              # æ­£å¸¸çŸ­ä¿¡\n",
        "    \"Congratulations! You are selected as winner. Text WIN now!\",  # åƒåœ¾çŸ­ä¿¡\n",
        "]\n",
        "\n",
        "print(\"ğŸ”® æ¨ç†æ¼”ç¤º\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"æµ‹è¯• {len(sample_texts)} æ¡æ ·æœ¬...\\n\")\n",
        "\n",
        "# ğŸ”§ åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
        "model.eval()\n",
        "\n",
        "# ğŸ”„ éå†æ¯ä¸ªæµ‹è¯•æ ·æœ¬\n",
        "for i, text in enumerate(sample_texts, 1):\n",
        "    # ğŸ”¤ ä½¿ç”¨ tokenizer ç¼–ç æ–‡æœ¬\n",
        "    # å‚æ•°è¯´æ˜ï¼š\n",
        "    # - return_tensors=\"pt\": è¿”å› PyTorch å¼ é‡\n",
        "    # - truncation=True: å¦‚æœè¶…é•¿åˆ™æˆªæ–­\n",
        "    # - padding=\"max_length\": å¡«å……åˆ°æœ€å¤§é•¿åº¦\n",
        "    # - max_length: ä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_length,\n",
        "    )\n",
        "    \n",
        "    # ğŸ® å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡\n",
        "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
        "    \n",
        "    # ğŸš« ç¦ç”¨æ¢¯åº¦è®¡ç®—ï¼ˆæ¨ç†æ—¶ä¸éœ€è¦ï¼‰\n",
        "    with torch.no_grad():\n",
        "        # ğŸ”® å‰å‘ä¼ æ’­è·å– logits\n",
        "        logits = model(**encoded).logits\n",
        "        \n",
        "        # ğŸ“Š è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ\n",
        "        # softmax å°† logits è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆå’Œä¸º 1ï¼‰\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "    \n",
        "    # ğŸ¯ è·å–é¢„æµ‹ç»“æœ\n",
        "    pred = torch.argmax(probs).item()  # é¢„æµ‹ç±»åˆ«ï¼ˆ0 æˆ– 1ï¼‰\n",
        "    label = \"spam\" if pred == 1 else \"ham\"  # è½¬æ¢ä¸ºæ–‡æœ¬æ ‡ç­¾\n",
        "    \n",
        "    # ğŸ“Š æå–æ¦‚ç‡å€¼\n",
        "    ham_prob = probs[0].item()   # æ­£å¸¸çŸ­ä¿¡æ¦‚ç‡\n",
        "    spam_prob = probs[1].item()  # åƒåœ¾çŸ­ä¿¡æ¦‚ç‡\n",
        "    \n",
        "    # ğŸ“¢ æ‰“å°ç»“æœ\n",
        "    print(f\"ã€æ ·æœ¬ {i}ã€‘\")\n",
        "    print(f\"æ–‡æœ¬: {text[:60]}{'...' if len(text) > 60 else ''}\")\n",
        "    print(f\"é¢„æµ‹: {label.upper()}\")\n",
        "    print(f\"ç½®ä¿¡åº¦: ham={ham_prob:.3f}, spam={spam_prob:.3f}\")\n",
        "    \n",
        "    # ğŸ¨ æ·»åŠ è¡¨æƒ…ç¬¦å·å¢å¼ºå¯è¯»æ€§\n",
        "    if pred == 1:\n",
        "        print(\"ğŸš¨ åˆ†ç±»ç»“æœï¼šåƒåœ¾çŸ­ä¿¡\")\n",
        "    else:\n",
        "        print(\"âœ… åˆ†ç±»ç»“æœï¼šæ­£å¸¸çŸ­ä¿¡\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"âœ… æ¨ç†æ¼”ç¤ºå®Œæˆï¼\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9ï¸âƒ£ æ¨¡å‹ä¿å­˜å’ŒåŠ è½½\n",
        "\n",
        "## ğŸ’¾ PEFT æ¨¡å‹çš„ä¿å­˜ç­–ç•¥\n",
        "\n",
        "PEFT åº“æä¾›äº†éå¸¸æ–¹ä¾¿çš„æ¨¡å‹ä¿å­˜åŠŸèƒ½ï¼š\n",
        "\n",
        "### ğŸ¯ ä¸¤ç§ä¿å­˜æ–¹å¼\n",
        "\n",
        "1. **åªä¿å­˜ LoRA å‚æ•°**ï¼ˆæ¨èï¼‰\n",
        "   - æ–‡ä»¶å¾ˆå°ï¼ˆå‡  MBï¼‰\n",
        "   - éœ€è¦é…åˆåŸå§‹æ¨¡å‹ä½¿ç”¨\n",
        "   - é€‚åˆæ¨¡å‹åˆ†å‘å’Œå¤šä»»åŠ¡åœºæ™¯\n",
        "\n",
        "2. **ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹**\n",
        "   - å°† LoRA å‚æ•°åˆå¹¶åˆ°åŸå§‹æƒé‡\n",
        "   - æ–‡ä»¶è¾ƒå¤§\n",
        "   - å¯ä»¥ä½œä¸ºç‹¬ç«‹æ¨¡å‹ä½¿ç”¨\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ’¾ æ–¹å¼ 1ï¼šåªä¿å­˜ LoRA å‚æ•°ï¼ˆæ¨èï¼‰\n",
        "# åŠŸèƒ½ï¼šä¿å­˜è½»é‡çº§çš„ LoRA é€‚é…å™¨æ–‡ä»¶\n",
        "\n",
        "print(\"ğŸ’¾ ä¿å­˜ LoRA å‚æ•°...\")\n",
        "\n",
        "# ğŸ“ æŒ‡å®šä¿å­˜è·¯å¾„\n",
        "lora_save_path = \"./lora_sms_spam_classifier\"\n",
        "\n",
        "# ğŸ’¾ ä¿å­˜ LoRA é€‚é…å™¨\n",
        "# save_pretrained() ä¼šä¿å­˜ï¼š\n",
        "# 1. adapter_config.json: LoRA é…ç½®\n",
        "# 2. adapter_model.safetensors: LoRA æƒé‡ï¼ˆæ¨èæ ¼å¼ï¼‰\n",
        "# æˆ– adapter_model.bin: LoRA æƒé‡ï¼ˆä¼ ç»Ÿæ ¼å¼ï¼‰\n",
        "model.save_pretrained(lora_save_path)\n",
        "\n",
        "print(f\"âœ… LoRA å‚æ•°å·²ä¿å­˜åˆ°: {lora_save_path}\")\n",
        "\n",
        "# ğŸ“Š æ£€æŸ¥ä¿å­˜çš„æ–‡ä»¶\n",
        "import os\n",
        "saved_files = os.listdir(lora_save_path)\n",
        "print(f\"ğŸ“‚ ä¿å­˜çš„æ–‡ä»¶: {saved_files}\")\n",
        "\n",
        "# ğŸ“ æ£€æŸ¥æ–‡ä»¶å¤§å°\n",
        "total_size = sum(os.path.getsize(os.path.join(lora_save_path, f)) for f in saved_files)\n",
        "print(f\"ğŸ“Š æ€»å¤§å°: {total_size / (1024*1024):.2f} MB\")\n",
        "\n",
        "# ğŸ’¡ åŠ è½½æ–¹å¼ç¤ºä¾‹ï¼ˆä»£ç ä»…ä¾›å‚è€ƒï¼Œä¸å®é™…æ‰§è¡Œï¼‰\n",
        "print(\"\\nğŸ’¡ åŠ è½½ LoRA æ¨¡å‹çš„æ–¹æ³•:\")\n",
        "print(\"```python\")\n",
        "print(\"from peft import PeftModel\")\n",
        "print(\"from transformers import AutoModelForSequenceClassification\")\n",
        "print(\"\")\n",
        "print(\"# 1. åŠ è½½åŸºç¡€æ¨¡å‹\")\n",
        "print('base_model = AutoModelForSequenceClassification.from_pretrained(\"gpt2\", num_labels=2)')\n",
        "print(\"\")\n",
        "print(\"# 2. åŠ è½½ LoRA é€‚é…å™¨\")\n",
        "print('model = PeftModel.from_pretrained(base_model, \"./lora_sms_spam_classifier\")')\n",
        "print(\"\")\n",
        "print(\"# 3. ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†\")\n",
        "print(\"model.eval()\")\n",
        "print(\"# ... æ¨ç†ä»£ç  ...\")\n",
        "print(\"```\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# ğŸ”— æ–¹å¼ 2ï¼šåˆå¹¶å¹¶ä¿å­˜å®Œæ•´æ¨¡å‹ï¼ˆå¯é€‰ï¼‰\n",
        "print(\"\\nğŸ”— æ–¹å¼ 2ï¼šåˆå¹¶ LoRA å‚æ•°åˆ°åŸºç¡€æ¨¡å‹\")\n",
        "\n",
        "# ğŸ“ åˆå¹¶è¯´æ˜\n",
        "print(\"ğŸ’¡ åˆå¹¶æ“ä½œä¼šå°† LoRA å‚æ•°åŠ åˆ°åŸå§‹æƒé‡ä¸Šï¼š\")\n",
        "print(\"   W_new = W_original + Î±/r * (A @ B)\")\n",
        "print(\"\")\n",
        "print(\"âš ï¸  æ³¨æ„ï¼šåˆå¹¶åå°†å¤±å»åˆ‡æ¢ LoRA é€‚é…å™¨çš„èƒ½åŠ›\")\n",
        "print(\"\")\n",
        "\n",
        "# ğŸ”§ åˆå¹¶æ¨¡å‹ï¼ˆç¤ºä¾‹ä»£ç ï¼‰\n",
        "print(\"ğŸ”§ åˆå¹¶ä»£ç ç¤ºä¾‹ï¼ˆä¸æ‰§è¡Œï¼Œä»…å±•ç¤ºï¼‰:\")\n",
        "print(\"```python\")\n",
        "print(\"# åˆå¹¶ LoRA å‚æ•°åˆ°åŸºç¡€æ¨¡å‹\")\n",
        "print(\"merged_model = model.merge_and_unload()\")\n",
        "print(\"\")\n",
        "print(\"# ä¿å­˜å®Œæ•´æ¨¡å‹\")\n",
        "print('merged_model.save_pretrained(\"./merged_sms_spam_classifier\")')\n",
        "print('tokenizer.save_pretrained(\"./merged_sms_spam_classifier\")')\n",
        "print(\"\")\n",
        "print(\"# åŠ è½½å®Œæ•´æ¨¡å‹\")\n",
        "print('loaded_model = AutoModelForSequenceClassification.from_pretrained(')\n",
        "print('    \"./merged_sms_spam_classifier\"')\n",
        "print(')')\n",
        "print(\"```\")\n",
        "\n",
        "print(\"\\nâœ… æ¨¡å‹ä¿å­˜å®Œæˆï¼\")\n",
        "\n",
        "# ğŸ“Š ä¿å­˜æ–¹å¼å¯¹æ¯”\n",
        "print(\"\\nğŸ“Š ä¸¤ç§ä¿å­˜æ–¹å¼å¯¹æ¯”:\")\n",
        "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
        "print(\"â”‚      ç‰¹æ€§       â”‚  LoRA å‚æ•°   â”‚  å®Œæ•´æ¨¡å‹    â”‚\")\n",
        "print(\"â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\")\n",
        "print(\"â”‚  æ–‡ä»¶å¤§å°       â”‚  å‡  MB       â”‚  æ•°ç™¾ MB     â”‚\")\n",
        "print(\"â”‚  åŠ è½½é€Ÿåº¦       â”‚  å¿«          â”‚  è¾ƒæ…¢        â”‚\")\n",
        "print(\"â”‚  å¤šä»»åŠ¡åˆ‡æ¢     â”‚  æ”¯æŒ        â”‚  ä¸æ”¯æŒ      â”‚\")\n",
        "print(\"â”‚  ç‹¬ç«‹ä½¿ç”¨       â”‚  éœ€åŸºç¡€æ¨¡å‹  â”‚  å¯ç‹¬ç«‹ä½¿ç”¨  â”‚\")\n",
        "print(\"â”‚  æ¨èåœºæ™¯       â”‚  ç”Ÿäº§ç¯å¢ƒ    â”‚  æ¼”ç¤º/éƒ¨ç½²   â”‚\")\n",
        "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“ æ•™ç¨‹æ€»ç»“\n",
        "\n",
        "## âœ¨ æ ¸å¿ƒè¦ç‚¹\n",
        "\n",
        "### PEFT åº“ä½¿ç”¨ä¸‰æ­¥æ›²\n",
        "1. åˆ›å»º LoRAConfig\n",
        "2. åŠ è½½åŸºç¡€æ¨¡å‹  \n",
        "3. è°ƒç”¨ get_peft_model()\n",
        "\n",
        "### å…³é”®ä¼˜åŠ¿\n",
        "- ä»£ç ç®€æ´ï¼ˆ~10è¡Œ vs ~200è¡Œï¼‰\n",
        "- å·¥ä¸šçº§å®ç°\n",
        "- å®˜æ–¹ç»´æŠ¤æ›´æ–°\n",
        "- ä¸ Transformers æ— ç¼é›†æˆ\n",
        "\n",
        "## ğŸ¯ è¶…å‚æ•°å»ºè®®\n",
        "\n",
        "- **rank**: 8-16ï¼ˆæ¨èèµ·ç‚¹ï¼‰\n",
        "- **alpha**: 2 Ã— rank\n",
        "- **dropout**: 0.05-0.1\n",
        "- **learning_rate**: 5e-4 åˆ° 1e-3\n",
        "\n",
        "## ğŸ’¡ æœ€ä½³å®è·µ\n",
        "\n",
        "1. ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­ï¼ˆWarmupï¼‰\n",
        "2. æ·»åŠ æ¢¯åº¦è£å‰ª\n",
        "3. ç›‘æ§éªŒè¯é›†æ€§èƒ½\n",
        "4. ä¿å­˜è½»é‡çº§ LoRA å‚æ•°\n",
        "\n",
        "## ğŸš€ è¿›é˜¶æ–¹å‘\n",
        "\n",
        "- QLoRAï¼šç»“åˆé‡åŒ–\n",
        "- å¤šä»»åŠ¡ LoRA\n",
        "- å¤§æ¨¡å‹å¾®è°ƒï¼ˆLLaMAã€Mistralï¼‰\n",
        "\n",
        "ğŸ‰ æ­å–œå®Œæˆå­¦ä¹ ï¼\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
