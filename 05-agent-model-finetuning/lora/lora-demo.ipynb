{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ”§ ç¯å¢ƒé…ç½®å’Œæ£€æŸ¥\n",
    "\n",
    "#### æ¦‚è¿°\n",
    "\n",
    "æœ¬æ•™ç¨‹éœ€è¦ç‰¹å®šçš„ç¯å¢ƒé…ç½®ä»¥ç¡®ä¿æœ€ä½³å­¦ä¹ ä½“éªŒã€‚ä»¥ä¸‹é…ç½®å°†å¸®åŠ©ä½ ï¼š\n",
    "\n",
    "- ä½¿ç”¨ç»Ÿä¸€çš„condaç¯å¢ƒï¼šæ¿€æ´»ç»Ÿä¸€çš„å­¦ä¹ ç¯å¢ƒ\n",
    "- é€šè¿‡å›½å†…é•œåƒæºå¿«é€Ÿå®‰è£…ä¾èµ–ï¼šé…ç½®pipä½¿ç”¨æ¸…åé•œåƒæº\n",
    "- åŠ é€Ÿæ¨¡å‹ä¸‹è½½ï¼šè®¾ç½®HuggingFaceé•œåƒä»£ç†\n",
    "- æ£€æŸ¥ç³»ç»Ÿé…ç½®ï¼šæ£€æŸ¥ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®\n",
    "\n",
    "#### é…ç½®\n",
    "\n",
    "- **æ‰€éœ€ç¯å¢ƒåŠå…¶ä¾èµ–å·²ç»éƒ¨ç½²å¥½**\n",
    "- åœ¨`Notebook`å³ä¸Šè§’é€‰æ‹©`jupyterå†…æ ¸`ä¸º`python(agent101)`ï¼Œå³å¯æ‰§è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
    "#\n",
    "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
    "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
    "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
    "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
    "#\n",
    "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©ä½ ï¼š\n",
    "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
    "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
    "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "\n",
    "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
    "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
    "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
    "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
    "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
    "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
    "        try:\n",
    "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
    "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "\n",
    "\n",
    "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    if platform.system() == \"Windows\":\n",
    "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
    "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
    "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
    "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
    "\n",
    "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
    "        else:\n",
    "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "\n",
    "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
    "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
    "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_python_version():\n",
    "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
    "\n",
    "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
    "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
    "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
    "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
    "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
    "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
    "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
    "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
    "                     # å°è¯•è·å– codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
    "                     except:\n",
    "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
    "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
    "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
    "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
    "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
    "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "\n",
    "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
    "env_data = {\n",
    "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
    "        \"æ“ä½œç³»ç»Ÿ\",\n",
    "        \"CPU ä¿¡æ¯\",\n",
    "        \"å†…å­˜ä¿¡æ¯\",\n",
    "        \"GPU ä¿¡æ¯\",\n",
    "        \"CUDA ä¿¡æ¯\",\n",
    "        \"Python ç‰ˆæœ¬\",\n",
    "        \"Conda ç‰ˆæœ¬\",\n",
    "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
    "    ],\n",
    "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# æ‰“å°è¡¨æ ¼\n",
    "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
    "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ğŸš€ LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒå®Œæ•´æ•™ç¨‹\n",
    "\n",
    "## ğŸ“– æ•™ç¨‹æ¦‚è¿°\n",
    "\n",
    "æœ¬æ•™ç¨‹é¢å‘å¤§æ¨¡å‹æŠ€æœ¯åˆå­¦è€…ï¼Œé€šè¿‡ä¸€ä¸ªå®Œæ•´çš„ SMS åƒåœ¾çŸ­ä¿¡åˆ†ç±»ä»»åŠ¡ï¼Œæ·±å…¥è®²è§£ LoRAï¼ˆLow-Rank Adaptationï¼‰å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯çš„åŸç†ã€å®ç°å’Œåº”ç”¨ã€‚\n",
    "\n",
    "### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½\n",
    "- **æ•°æ®å‡†å¤‡**ï¼šè‡ªåŠ¨ä¸‹è½½å¹¶å¤„ç† SMS Spam æ•°æ®é›†ï¼Œæ„å»ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®åŠ è½½å™¨\n",
    "- **æ¨¡å‹é€‚é…**ï¼šåŠ è½½ GPT-2 é¢„è®­ç»ƒæ¨¡å‹ï¼Œå°†è¯­è¨€å»ºæ¨¡å¤´æ›¿æ¢ä¸ºäºŒåˆ†ç±»å¤´\n",
    "- **LoRA æ›¿æ¢**ï¼šé€’å½’æ›¿æ¢æ‰€æœ‰ Linear å±‚ä¸º LoRA å±‚ï¼Œå†»ç»“åŸå§‹å‚æ•°ï¼Œä»…è®­ç»ƒ LoRA å‚æ•°\n",
    "- **è®­ç»ƒè¯„ä¼°**ï¼šå®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼ŒåŒ…å«æŸå¤±å¯è§†åŒ–ã€å‡†ç¡®ç‡è¯„ä¼°å’Œæ€§èƒ½åˆ†æ\n",
    "\n",
    "### ğŸ“ å­¦ä¹ ç›®æ ‡\n",
    "- **ç†è§£ LoRA åŸç†**ï¼šæŒæ¡ä½ç§©åˆ†è§£å¦‚ä½•å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ\n",
    "- **å®è·µå¾®è°ƒæµç¨‹**ï¼šä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´å·¥ä½œæµ\n",
    "- **æŒæ¡å…³é”®è¶…å‚**ï¼šrankã€alpha ç­‰å‚æ•°çš„é€‰æ‹©å’Œè°ƒä¼˜ç­–ç•¥\n",
    "- **è§£å†³å®é™…é—®é¢˜**ï¼šå°†é€šç”¨è¯­è¨€æ¨¡å‹é€‚é…ä¸ºç‰¹å®šä¸‹æ¸¸ä»»åŠ¡\n",
    "\n",
    "### ğŸ“Š é¢„æœŸæ•ˆæœ\n",
    "- **å‚æ•°æ•ˆç‡**ï¼šä»…è®­ç»ƒå°‘é‡ LoRA å‚æ•°ï¼ˆrank=16 æ—¶çº¦æ•°ç™¾ä¸‡çº§ï¼‰ï¼Œç›¸æ¯”å…¨é‡å¾®è°ƒæ˜¾è‘—é™ä½\n",
    "- **æ€§èƒ½æå‡**ï¼š3-5 ä¸ª epoch å³å¯å°†å‡†ç¡®ç‡ä» 45-50% æå‡è‡³ 95%+\n",
    "- **éƒ¨ç½²å‹å¥½**ï¼šLoRA å¢é‡å‚æ•°ä½“ç§¯å°ï¼Œä¾¿äºåˆ†å‘å’Œéƒ¨ç½²\n",
    "- **åˆå§‹ä¸€è‡´æ€§**ï¼šB çŸ©é˜µåˆå§‹åŒ–ä¸º 0ï¼Œç¡®ä¿æ›¿æ¢ååˆå§‹æ€§èƒ½ä¸å˜\n",
    "\n",
    "### ğŸ’¡ æŠ€æœ¯äº®ç‚¹\n",
    "- **ä½ç§©åˆ†è§£**ï¼šÎ”W â‰ˆ A Ã— Bï¼Œå…¶ä¸­ A âˆˆ â„^(dÃ—r)ï¼ŒB âˆˆ â„^(rÃ—k)ï¼Œr << min(d,k)\n",
    "- **å‚æ•°å†»ç»“**ï¼šåŸå§‹æ¨¡å‹å‚æ•°ä¿æŒå†»ç»“ï¼Œä»…ä¼˜åŒ– LoRA åˆ†æ”¯\n",
    "- **å¢é‡æ›´æ–°**ï¼šå‰å‘ä¼ æ’­ä¸º `x(W + Î±AB)`ï¼Œæ— éœ€ä¿®æ”¹åŸå§‹æƒé‡\n",
    "- **çµæ´»éƒ¨ç½²**ï¼šæ¨ç†æ—¶åŠ¨æ€å åŠ  LoRA å¢é‡ï¼Œæ”¯æŒå¤šä»»åŠ¡é€‚é…\n",
    "\n",
    "> ğŸ’» **ç¯å¢ƒè¦æ±‚**ï¼šå»ºè®®åœ¨æ”¯æŒ CUDA çš„ GPU ç¯å¢ƒä¸­æ‰§è¡Œï¼Œä»¥è·å¾—æœ€ä½³è®­ç»ƒæ•ˆæœ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pip in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (25.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: numpy<2.1,>=1.26 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.0.2)\n",
      "Requirement already satisfied: pandas>=2.2.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (3.10.7)\n",
      "Requirement already satisfied: tiktoken>=0.5.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas>=2.2.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas>=2.2.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from pandas>=2.2.1) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from matplotlib>=3.7.1) (3.2.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from tiktoken>=0.5.1) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from tiktoken>=0.5.1) (2.32.5)\n",
      "Requirement already satisfied: six>=1.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.1) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken>=0.5.1) (2025.8.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: torch>=2.3.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (1.14.0)\n",
      "Requirement already satisfied: networkx in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from torch>=2.3.0) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.3.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/envs/flyai_agent_in_action/lib/python3.12/site-packages (from jinja2->torch>=2.3.0) (3.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ ç¯å¢ƒä¾èµ–å®‰è£…\n",
    "# \n",
    "# æœ¬æ•™ç¨‹éœ€è¦ä»¥ä¸‹æ ¸å¿ƒä¾èµ–åŒ…ï¼Œè¯·æ ¹æ®æ‚¨çš„ç¯å¢ƒé€‰æ‹©åˆé€‚çš„å®‰è£…æ–¹å¼ï¼š\n",
    "# - PyTorchï¼šæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒ CPU/GPU è®­ç»ƒ\n",
    "# - tiktokenï¼šGPT-2 åˆ†è¯å™¨ï¼Œç”¨äºæ–‡æœ¬é¢„å¤„ç†\n",
    "# - pandasï¼šæ•°æ®å¤„ç†ï¼Œç”¨äºæ•°æ®é›†æ“ä½œ\n",
    "# - matplotlibï¼šå¯è§†åŒ–ï¼Œç”¨äºç»˜åˆ¶è®­ç»ƒæ›²çº¿\n",
    "# - numpyï¼šæ•°å€¼è®¡ç®—åŸºç¡€åº“\n",
    "print(\"ğŸš€ LoRA å¾®è°ƒé¡¹ç›®æ ¸å¿ƒä¾èµ–ä¸€é”®å®‰è£…\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "%pip install -U pip\n",
    "%pip install -U \"numpy>=1.26,<2.1\" \"pandas>=2.2.1\" \"matplotlib>=3.7.1\" \"tiktoken>=0.5.1\"\n",
    "\n",
    "# ğŸ”§ PyTorch å®‰è£…ï¼ˆæ ¹æ®æ‚¨çš„ç¡¬ä»¶ç¯å¢ƒé€‰æ‹©ï¼‰\n",
    "# CPU ç‰ˆæœ¬ï¼ˆé€‚åˆè½»é‡çº§å®éªŒï¼‰ï¼š\n",
    "# %pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cpu\n",
    "\n",
    "# GPU ç‰ˆæœ¬ï¼ˆæ¨èï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ï¼‰ï¼š\n",
    "# CUDA 12.1 ç‰ˆæœ¬ï¼ˆé€‚ç”¨äºå¤§å¤šæ•°ç°ä»£ GPUï¼‰\n",
    "%pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# CUDA 11.8 ç‰ˆæœ¬ï¼ˆé€‚ç”¨äºè¾ƒè€çš„ GPUï¼‰\n",
    "# %pip install \"torch>=2.3.0\" --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# ğŸ“š å®‰è£…é…å¥—å·¥å…·åº“\n",
    "# æä¾› GPT-2 æ¨¡å‹ä¸‹è½½ã€æƒé‡åŠ è½½ç­‰è¾…åŠ©åŠŸèƒ½\n",
    "# %pip install -U git+https://github.com/rasbt/LLMs-from-scratch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æ£€æŸ¥ä¾èµ–åŒ…ç‰ˆæœ¬...\n",
      "âœ… numpy: 2.0.2\n",
      "âœ… pandas: 2.3.3\n",
      "âœ… matplotlib: 3.10.7\n",
      "âœ… tiktoken: 0.12.0\n",
      "âœ… torch: 2.8.0\n",
      "\n",
      "ğŸ’¡ å¦‚æœç‰ˆæœ¬ä¸æ»¡è¶³è¦æ±‚ï¼Œè¯·é‡æ–°è¿è¡Œä¸Šæ–¹çš„å®‰è£…å‘½ä»¤\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ç¯å¢ƒç‰ˆæœ¬æ£€æŸ¥\n",
    "# \n",
    "# æ£€æŸ¥å…³é”®ä¾èµ–åŒ…çš„ç‰ˆæœ¬ï¼Œç¡®ä¿æ»¡è¶³æ•™ç¨‹è¦æ±‚\n",
    "# è¿™æœ‰åŠ©äºå¤ç°å®éªŒç»“æœå’Œæ’æŸ¥ç¯å¢ƒé—®é¢˜\n",
    "\n",
    "from importlib.metadata import version\n",
    "\n",
    "# æ ¸å¿ƒä¾èµ–åŒ…åŠå…¶ç‰ˆæœ¬è¦æ±‚\n",
    "pkgs = [\n",
    "    \"numpy\",      # æ•°å€¼è®¡ç®—åŸºç¡€åº“ï¼Œéœ€ >=1.26,<2.1ï¼ˆå…¼å®¹æ€§è¦æ±‚ï¼‰\n",
    "    \"pandas\",     # æ•°æ®å¤„ç†åº“ï¼Œéœ€ >=2.2.1ï¼ˆæ”¯æŒç°ä»£ DataFrame æ“ä½œï¼‰\n",
    "    \"matplotlib\", # å¯è§†åŒ–åº“ï¼Œéœ€ >=3.7.1ï¼ˆæ”¯æŒç°ä»£ç»˜å›¾åŠŸèƒ½ï¼‰\n",
    "    \"tiktoken\",   # GPT-2 åˆ†è¯å™¨ï¼Œéœ€ >=0.5.1ï¼ˆæ”¯æŒ BPE ç¼–ç ï¼‰\n",
    "    \"torch\"       # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œéœ€ >=2.3.0ï¼ˆæ”¯æŒç°ä»£ç‰¹æ€§ï¼‰\n",
    "]\n",
    "\n",
    "print(\"ğŸ” æ£€æŸ¥ä¾èµ–åŒ…ç‰ˆæœ¬...\")\n",
    "for pkg in pkgs:\n",
    "    try:\n",
    "        pkg_version = version(pkg)\n",
    "        print(f\"âœ… {pkg}: {pkg_version}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {pkg} æœªå®‰è£…: {e}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ å¦‚æœç‰ˆæœ¬ä¸æ»¡è¶³è¦æ±‚ï¼Œè¯·é‡æ–°è¿è¡Œä¸Šæ–¹çš„å®‰è£…å‘½ä»¤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## E.1 LoRA ç®€ä»‹ï¼ˆæ¦‚å¿µå±‚ï¼‰\n",
    "\n",
    "- ä¼ ç»Ÿå¾®è°ƒï¼šå­¦ä¹ å®Œæ•´æƒé‡æ›´æ–° Î”Wï¼Œä½¿å¾— `W_updated = W + Î”W`ã€‚\n",
    "- LoRAï¼šç”¨æ›´å°çš„çŸ©é˜µåˆ†è§£è¿‘ä¼¼ Î”Wï¼Œå³ `Î”W â‰ˆ A B`ï¼Œå› æ­¤ `W_updated = W + A B`ã€‚\n",
    "- å‰å‘æ—¶å¯å†™ä½œ `x(W + AB) = xW + xAB`ï¼Œæ— éœ€æ”¹å†™åŸæƒé‡ Wï¼ŒAB ä½œä¸ºâ€œå¢é‡åˆ†æ”¯â€åŠ¨æ€å åŠ ã€‚\n",
    "- è¶…å‚æ•°ï¼š\n",
    "  - rankï¼ˆç§©ï¼‰ï¼šæ§åˆ¶ Aã€B çš„å†…ç»´åº¦ï¼Œå†³å®šæ–°å¢å¯è®­ç»ƒå‚æ•°é‡ä¸é€‚é…èƒ½åŠ›ã€‚\n",
    "  - alphaï¼šç¼©æ”¾ç³»æ•°ï¼Œæ§åˆ¶ LoRA åˆ†æ”¯å¯¹åŸå±‚è¾“å‡ºçš„å½±å“å¼ºåº¦ã€‚\n",
    "- å®è·µæ„ä¹‰ï¼šåªè®­ç»ƒå°‘é‡å‚æ•°ï¼ˆAã€Bï¼‰ï¼Œæ˜¾è‘—é™ä½æ˜¾å­˜ä¸è®­ç»ƒæ—¶é—´ã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ“Š æ•°æ®é›†å‡†å¤‡ä¸é¢„å¤„ç†\n",
    "\n",
    "### ğŸ¯ æ•°æ®é›†é€‰æ‹©\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨ç»å…¸çš„ **SMS Spam Collection** æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¬å¼€çš„åƒåœ¾çŸ­ä¿¡åˆ†ç±»æ•°æ®é›†ï¼ŒåŒ…å«ï¼š\n",
    "- **æ•°æ®æ¥æº**ï¼šUCI Machine Learning Repository\n",
    "- **æ•°æ®è§„æ¨¡**ï¼šçº¦ 5,572 æ¡çŸ­ä¿¡\n",
    "- **æ ‡ç­¾åˆ†å¸ƒ**ï¼šhamï¼ˆæ­£å¸¸çŸ­ä¿¡ï¼‰vs spamï¼ˆåƒåœ¾çŸ­ä¿¡ï¼‰\n",
    "- **åº”ç”¨åœºæ™¯**ï¼šæ–‡æœ¬åˆ†ç±»ã€åƒåœ¾ä¿¡æ¯è¿‡æ»¤\n",
    "\n",
    "### ğŸ”§ æ•°æ®é¢„å¤„ç†æµç¨‹\n",
    "\n",
    "1. **æ•°æ®ä¸‹è½½**ï¼šè‡ªåŠ¨ä» UCI ä¸‹è½½åŸå§‹ TSV æ–‡ä»¶\n",
    "2. **æ ‡ç­¾æ˜ å°„**ï¼šå°†æ–‡æœ¬æ ‡ç­¾è½¬æ¢ä¸ºæ•°å€¼ï¼ˆhamâ†’0, spamâ†’1ï¼‰\n",
    "3. **ç±»åˆ«å¹³è¡¡**ï¼šç¡®ä¿è®­ç»ƒé›†æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹å‡è¡¡\n",
    "4. **æ•°æ®åˆ’åˆ†**ï¼šæŒ‰ 7:2:1 æ¯”ä¾‹åˆ†å‰²ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†\n",
    "5. **åºåˆ—å¤„ç†**ï¼šä½¿ç”¨ GPT-2 åˆ†è¯å™¨è¿›è¡Œæ–‡æœ¬ç¼–ç å’Œå¡«å……\n",
    "\n",
    "### ğŸ“ˆ æ•°æ®ç»Ÿè®¡ä¿¡æ¯\n",
    "\n",
    "- **è®­ç»ƒé›†**ï¼šçº¦ 70% çš„æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒ\n",
    "- **éªŒè¯é›†**ï¼šçº¦ 20% çš„æ•°æ®ç”¨äºè¶…å‚æ•°è°ƒä¼˜å’Œæ—©åœ\n",
    "- **æµ‹è¯•é›†**ï¼šçº¦ 10% çš„æ•°æ®ç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°\n",
    "- **åºåˆ—é•¿åº¦**ï¼šæ ¹æ®è®­ç»ƒé›†ç»Ÿè®¡æœ€é•¿åºåˆ—é•¿åº¦ï¼Œç¡®ä¿ä¸€è‡´æ€§\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“š æ•°æ®å¤„ç†æ ¸å¿ƒå‡½æ•°åº“\n",
    "# \n",
    "# æœ¬æ¨¡å—åŒ…å«æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°çš„æ ¸å¿ƒå‡½æ•°\n",
    "# ä¸º LoRA å¾®è°ƒæä¾›å®Œæ•´çš„æ•°æ®æµæ°´çº¿å’Œè®­ç»ƒå·¥å…·\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_balanced_dataset(df):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºç±»åˆ«å¹³è¡¡çš„æ•°æ®é›†\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - ç»Ÿè®¡åƒåœ¾çŸ­ä¿¡æ•°é‡ï¼Œéšæœºé‡‡æ ·ç­‰é‡çš„æ­£å¸¸çŸ­ä¿¡\n",
    "    - ç¡®ä¿è®­ç»ƒæ—¶æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹å‡è¡¡ï¼Œé¿å…æ¨¡å‹åå‘å¤šæ•°ç±»\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        df: åŸå§‹æ•°æ®æ¡†ï¼ŒåŒ…å« Label å’Œ Text åˆ—\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        balanced_df: ç±»åˆ«å¹³è¡¡åçš„æ•°æ®æ¡†\n",
    "    \"\"\"\n",
    "    # ç»Ÿè®¡åƒåœ¾çŸ­ä¿¡æ•°é‡\n",
    "    num_spam = df[df[\"Label\"] == \"spam\"].shape[0]\n",
    "    \n",
    "    # éšæœºé‡‡æ ·ç­‰é‡çš„æ­£å¸¸çŸ­ä¿¡ï¼ˆå›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°ï¼‰\n",
    "    ham_subset = df[df[\"Label\"] == \"ham\"].sample(num_spam, random_state=123)\n",
    "    \n",
    "    # åˆå¹¶æ­£å¸¸çŸ­ä¿¡å­é›†å’Œæ‰€æœ‰åƒåœ¾çŸ­ä¿¡\n",
    "    balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "\n",
    "def random_split(df, train_frac, validation_frac):\n",
    "    \"\"\"\n",
    "    éšæœºåˆ’åˆ†æ•°æ®é›†\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - æŒ‰æŒ‡å®šæ¯”ä¾‹å°†æ•°æ®åˆ’åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†\n",
    "    - ä½¿ç”¨å›ºå®šéšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        df: å¾…åˆ’åˆ†çš„æ•°æ®æ¡†\n",
    "        train_frac: è®­ç»ƒé›†æ¯”ä¾‹\n",
    "        validation_frac: éªŒè¯é›†æ¯”ä¾‹\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        train_df, validation_df, test_df: ä¸‰ä¸ªæ•°æ®æ¡†\n",
    "    \"\"\"\n",
    "    # éšæœºæ‰“ä¹±æ•°æ®ï¼ˆå›ºå®šéšæœºç§å­ï¼‰\n",
    "    df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "    \n",
    "    # è®¡ç®—åˆ’åˆ†ç´¢å¼•\n",
    "    train_end = int(len(df) * train_frac)\n",
    "    validation_end = train_end + int(len(df) * validation_frac)\n",
    "    \n",
    "    # æ‰§è¡Œåˆ’åˆ†\n",
    "    train_df = df[:train_end]\n",
    "    validation_df = df[train_end:validation_end]\n",
    "    test_df = df[validation_end:]\n",
    "    \n",
    "    return train_df, validation_df, test_df\n",
    "\n",
    "\n",
    "class SpamDataset(Dataset):\n",
    "    \"\"\"\n",
    "    SMS åƒåœ¾çŸ­ä¿¡åˆ†ç±»æ•°æ®é›†ç±»\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - ç»§æ‰¿ PyTorch Datasetï¼Œæ”¯æŒ DataLoader æ‰¹é‡åŠ è½½\n",
    "    - è‡ªåŠ¨å¤„ç†æ–‡æœ¬åˆ†è¯ã€åºåˆ—å¡«å……å’Œæ ‡ç­¾è½¬æ¢\n",
    "    - æ”¯æŒåŠ¨æ€åºåˆ—é•¿åº¦æˆ–å›ºå®šé•¿åº¦æˆªæ–­\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æ•°æ®é›†\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            csv_file: CSV æ–‡ä»¶è·¯å¾„\n",
    "            tokenizer: åˆ†è¯å™¨å¯¹è±¡ï¼ˆå¦‚ GPT-2 çš„ tiktokenï¼‰\n",
    "            max_length: æœ€å¤§åºåˆ—é•¿åº¦ï¼ŒNone è¡¨ç¤ºä½¿ç”¨è®­ç»ƒé›†æœ€é•¿é•¿åº¦\n",
    "            pad_token_id: å¡«å…… token çš„ IDï¼ˆGPT-2 é»˜è®¤ä¸º 50256ï¼‰\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        \n",
    "        # é¢„åˆ†è¯ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸º token ID åºåˆ—\n",
    "        self.encoded_texts = [\n",
    "            tokenizer.encode(text) for text in self.data[\"Text\"]\n",
    "        ]\n",
    "        \n",
    "        # ç¡®å®šåºåˆ—é•¿åº¦\n",
    "        if max_length is None:\n",
    "            self.max_length = self._longest_encoded_length()\n",
    "        else:\n",
    "            self.max_length = max_length\n",
    "            # æˆªæ–­è¿‡é•¿çš„åºåˆ—\n",
    "            self.encoded_texts = [\n",
    "                encoded_text[:self.max_length]\n",
    "                for encoded_text in self.encoded_texts\n",
    "            ]\n",
    "        \n",
    "        # å¡«å……åºåˆ—åˆ°ç»Ÿä¸€é•¿åº¦\n",
    "        self.encoded_texts = [\n",
    "            encoded_text + [pad_token_id] * (self.max_length - len(encoded_text))\n",
    "            for encoded_text in self.encoded_texts\n",
    "        ]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"è·å–å•ä¸ªæ ·æœ¬\"\"\"\n",
    "        encoded = self.encoded_texts[index]\n",
    "        label = self.data.iloc[index][\"Label\"]\n",
    "        return (\n",
    "            torch.tensor(encoded, dtype=torch.long),  # è¾“å…¥åºåˆ—\n",
    "            torch.tensor(label, dtype=torch.long)     # æ ‡ç­¾\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"è¿”å›æ•°æ®é›†å¤§å°\"\"\"\n",
    "        return len(self.data)\n",
    "    \n",
    "    def _longest_encoded_length(self):\n",
    "        \"\"\"è®¡ç®—æœ€é•¿ç¼–ç åºåˆ—é•¿åº¦\"\"\"\n",
    "        max_length = 0\n",
    "        for encoded_text in self.encoded_texts:\n",
    "            encoded_length = len(encoded_text)\n",
    "            if encoded_length > max_length:\n",
    "                max_length = encoded_length\n",
    "        return max_length\n",
    "\n",
    "\n",
    "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ•°æ®åŠ è½½å™¨çš„å‡†ç¡®ç‡\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - åœ¨æŒ‡å®šè®¾å¤‡ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
    "    - æ”¯æŒé™åˆ¶è¯„ä¼°æ‰¹æ¬¡æ•°ä»¥èŠ‚çœæ—¶é—´\n",
    "    - ä½¿ç”¨æœ€åä¸€ä¸ª token çš„ logits è¿›è¡Œåˆ†ç±»é¢„æµ‹\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        data_loader: æ•°æ®åŠ è½½å™¨\n",
    "        model: å¾…è¯„ä¼°çš„æ¨¡å‹\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        num_batches: è¯„ä¼°æ‰¹æ¬¡æ•°ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        accuracy: å‡†ç¡®ç‡ï¼ˆ0-1 ä¹‹é—´ï¼‰\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_predictions, num_examples = 0, 0\n",
    "    \n",
    "    if num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # ä½¿ç”¨æœ€åä¸€ä¸ª token çš„ logits è¿›è¡Œåˆ†ç±»\n",
    "                logits = model(input_batch)[:, -1, :]\n",
    "            predicted_labels = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            num_examples += predicted_labels.shape[0]\n",
    "            correct_predictions += (predicted_labels == target_batch).sum().item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return correct_predictions / num_examples\n",
    "\n",
    "\n",
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å•ä¸ªæ‰¹æ¬¡çš„æŸå¤±\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        input_batch: è¾“å…¥æ‰¹æ¬¡\n",
    "        target_batch: ç›®æ ‡æ‰¹æ¬¡\n",
    "        model: æ¨¡å‹\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        loss: äº¤å‰ç†µæŸå¤±\n",
    "    \"\"\"\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    \n",
    "    # 1. è¿è¡Œæ¨¡å‹ï¼Œå¾—åˆ°æ‰€æœ‰ token çš„ logits: [B, T, C]\n",
    "    full_logits = model(input_batch) \n",
    "    \n",
    "    # 2. ä»…åˆ‡ç‰‡è·å–åºåˆ—ä¸­æœ€åä¸€ä¸ª token çš„ logitsï¼Œç”¨äºåˆ†ç±»: [B, C]\n",
    "    logits = full_logits[:, -1, :] \n",
    "    \n",
    "    # 3. ä½¿ç”¨ [B, C] çš„ logits å’Œ [B] çš„æ ‡ç­¾è®¡ç®—æŸå¤±\n",
    "    loss = torch.nn.functional.cross_entropy(logits, target_batch) \n",
    "    return loss\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—æ•°æ®åŠ è½½å™¨çš„å¹³å‡æŸå¤±\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        data_loader: æ•°æ®åŠ è½½å™¨\n",
    "        model: æ¨¡å‹\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        num_batches: è¯„ä¼°æ‰¹æ¬¡æ•°\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        average_loss: å¹³å‡æŸå¤±\n",
    "    \"\"\"\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    \"\"\"\n",
    "    è¯„ä¼°æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„æŸå¤±\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        model: å¾…è¯„ä¼°çš„æ¨¡å‹\n",
    "        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
    "        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        eval_iter: è¯„ä¼°æ‰¹æ¬¡æ•°\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        train_loss, val_loss: è®­ç»ƒå’ŒéªŒè¯æŸå¤±\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def train_classifier_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                            eval_freq, eval_iter):\n",
    "    \"\"\"\n",
    "    ç®€åŒ–çš„åˆ†ç±»å™¨è®­ç»ƒå‡½æ•°\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - æ‰§è¡Œå®Œæ•´çš„è®­ç»ƒå¾ªç¯\n",
    "    - å‘¨æœŸæ€§è¯„ä¼°æ¨¡å‹æ€§èƒ½\n",
    "    - è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå‡†ç¡®ç‡\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        model: å¾…è®­ç»ƒçš„æ¨¡å‹\n",
    "        train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨\n",
    "        val_loader: éªŒè¯æ•°æ®åŠ è½½å™¨\n",
    "        optimizer: ä¼˜åŒ–å™¨\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        num_epochs: è®­ç»ƒè½®æ•°\n",
    "        eval_freq: è¯„ä¼°é¢‘ç‡ï¼ˆæ­¥æ•°ï¼‰\n",
    "        eval_iter: æ¯æ¬¡è¯„ä¼°çš„æ‰¹æ¬¡æ•°\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        train_losses, val_losses, train_accs, val_accs, examples_seen: è®­ç»ƒè®°å½•\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–è®°å½•åˆ—è¡¨\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    examples_seen, global_step = 0, -1\n",
    "    \n",
    "    # ä¸»è®­ç»ƒå¾ªç¯\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # æ¸…é›¶æ¢¯åº¦\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # åå‘ä¼ æ’­\n",
    "            optimizer.step()  # æ›´æ–°å‚æ•°\n",
    "            examples_seen += input_batch.shape[0]\n",
    "            global_step += 1\n",
    "            \n",
    "            # å‘¨æœŸæ€§è¯„ä¼°\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        \n",
    "        # æ¯ä¸ª epoch åè®¡ç®—å‡†ç¡®ç‡\n",
    "        train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "        print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
    "        print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "        train_accs.append(train_accuracy)\n",
    "        val_accs.append(val_accuracy)\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs, examples_seen\n",
    "\n",
    "\n",
    "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
    "    \"\"\"\n",
    "    ç»˜åˆ¶è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å›¾è¡¨\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡éšæ—¶é—´å’Œæ ·æœ¬æ•°çš„å˜åŒ–\n",
    "    - æ”¯æŒåŒ x è½´æ˜¾ç¤ºï¼ˆepochs å’Œ examplesï¼‰\n",
    "    - è‡ªåŠ¨ä¿å­˜å›¾è¡¨ä¸º PDF æ–‡ä»¶\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        epochs_seen: å·²è®­ç»ƒçš„è½®æ•°\n",
    "        examples_seen: å·²å¤„ç†çš„æ ·æœ¬æ•°\n",
    "        train_values: è®­ç»ƒé›†æŒ‡æ ‡å€¼\n",
    "        val_values: éªŒè¯é›†æŒ‡æ ‡å€¼\n",
    "        label: æŒ‡æ ‡åç§°ï¼ˆå¦‚ \"loss\", \"accuracy\"ï¼‰\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "    \n",
    "    # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŒ‡æ ‡\n",
    "    ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
    "    ax1.plot(epochs_seen, val_values, linestyle=\"-.\", label=f\"Validation {label}\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(label.capitalize())\n",
    "    ax1.legend()\n",
    "    \n",
    "    # åˆ›å»ºç¬¬äºŒä¸ª x è½´æ˜¾ç¤ºæ ·æœ¬æ•°\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(examples_seen, train_values, alpha=0)  # é€æ˜çº¿ç”¨äºå¯¹é½åˆ»åº¦\n",
    "    ax2.set_xlabel(\"Examples seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f\"{label}-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
    "    \"\"\"\n",
    "    å¯¹å•æ¡æ–‡æœ¬è¿›è¡Œåˆ†ç±»é¢„æµ‹\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - å¯¹è¾“å…¥çš„æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†å’Œåˆ†è¯\n",
    "    - ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œé¢„æµ‹\n",
    "    - è¿”å›åˆ†ç±»ç»“æœï¼ˆspam æˆ– not spamï¼‰\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        text: å¾…åˆ†ç±»çš„æ–‡æœ¬\n",
    "        model: è®­ç»ƒå¥½çš„æ¨¡å‹\n",
    "        tokenizer: åˆ†è¯å™¨\n",
    "        device: è®¡ç®—è®¾å¤‡\n",
    "        max_length: æœ€å¤§åºåˆ—é•¿åº¦\n",
    "        pad_token_id: å¡«å…… token ID\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        prediction: åˆ†ç±»ç»“æœå­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # é¢„å¤„ç†è¾“å…¥æ–‡æœ¬\n",
    "    input_ids = tokenizer.encode(text)\n",
    "    supported_context_length = model.pos_emb.weight.shape[0]\n",
    "    \n",
    "    # æˆªæ–­è¿‡é•¿çš„åºåˆ—\n",
    "    input_ids = input_ids[:min(max_length, supported_context_length)]\n",
    "    \n",
    "    # å¡«å……åºåˆ—\n",
    "    input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
    "    input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
    "    \n",
    "    # æ¨¡å‹æ¨ç†\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)[:, -1, :]  # æœ€åä¸€ä¸ª token çš„ logits\n",
    "    predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # è¿”å›åˆ†ç±»ç»“æœ\n",
    "    return \"spam\" if predicted_label == 1 else \"not spam\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®å·²å‡†å¤‡ï¼štrain.csv / validation.csv / test.csv\n"
     ]
    }
   ],
   "source": [
    "# æ•°æ®ä¸‹è½½ä¸æ‹†åˆ†ï¼ˆä¸åŸä¹¦ ch06 é€»è¾‘ä¸€è‡´ï¼‰\n",
    "# æ­¥éª¤è¯´æ˜ï¼š\n",
    "# 1) ä¸‹è½½å¹¶è§£å‹ SMS Spam æ•°æ®é›†ï¼›\n",
    "# 2) è¯»å– TSV æ–‡ä»¶ä¸º DataFrameï¼Œå¹¶å°†æ ‡ç­¾ ham/spam æ˜ å°„ä¸º 0/1ï¼›\n",
    "# 3) ä½¿ç”¨è¾…åŠ©å‡½æ•°åšç±»åˆ«å¹³è¡¡ä¸åˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼›\n",
    "# 4) å°†åˆ’åˆ†ç»“æœä¿å­˜ä¸º CSVï¼Œæ–¹ä¾¿åç»­ Dataset åŠ è½½ã€‚\n",
    "\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "data_file_path = \"./sms_spam_collection/SMSSpamCollection.tsv\"\n",
    "\n",
    "\n",
    "# è¯»å–ã€å¹³è¡¡ã€åˆ’åˆ†\n",
    "# åŸå§‹æ–‡ä»¶ä¸ºåˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œåˆ—ä¸º Label, Text\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "# ä¿è¯ç±»åˆ«åˆ†å¸ƒæ›´å‡è¡¡ï¼Œé¿å…è®­ç»ƒæ—¶åæ–œ\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "# æ ‡ç­¾æ˜ å°„ï¼šham->0, spam->1ï¼Œä¾¿äºåç»­è®¡ç®—æŸå¤±\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "# æŒ‰ 7:2:1ï¼ˆè®­ç»ƒ:æµ‹è¯•:éªŒè¯ï¼‰æ¯”ä¾‹åˆ’åˆ†\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "\n",
    "# è½ç›˜ï¼Œä¾¿äºå¤ç”¨ä¸è°ƒè¯•\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)\n",
    "\n",
    "print(\"æ•°æ®å·²å‡†å¤‡ï¼štrain.csv / validation.csv / test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®HuggingFaceä»£ç†\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# éªŒè¯ï¼šä½¿ç”¨shellå‘½ä»¤æ£€æŸ¥\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GPT-2 åˆ†è¯å™¨ä¿¡æ¯ ---\n",
      "åˆ†è¯å™¨åç§°: gpt2\n",
      "è¯æ±‡è¡¨å¤§å° (æ€» Token æ•°): 50257\n",
      "EOT (End-of-Text) Token ID: 50256\n",
      "--- æ‰¹æ¬¡æ•° ---\n",
      "è®­ç»ƒæ‰¹æ¬¡æ•°: 130 | éªŒè¯æ‰¹æ¬¡æ•°: 19 | æµ‹è¯•æ‰¹æ¬¡æ•°: 38\n"
     ]
    }
   ],
   "source": [
    "# æ„å»º Dataset ä¸ DataLoader\n",
    "# è¯´æ˜ï¼š\n",
    "# - ä½¿ç”¨ GPT-2 çš„ BPE åˆ†è¯å™¨ï¼ˆtiktoken.get_encoding(\"gpt2\")ï¼‰ã€‚\n",
    "# - è®­ç»ƒé›†åˆå§‹åŒ–æ—¶ä¸è®¾ max_lengthï¼ˆå†…éƒ¨ä¼šç»Ÿè®¡è®­ç»ƒé›†çš„æœ€é•¿é•¿åº¦ï¼‰ï¼Œ\n",
    "#   å°†è¯¥é•¿åº¦ä¼ ç»™éªŒè¯/æµ‹è¯•ï¼Œä»¥ä¿è¯ä¸‰è€…åºåˆ—é•¿åº¦ä¸€è‡´ï¼Œé¿å…è¯„ä¼°åå·®ã€‚\n",
    "\n",
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# åˆå§‹åŒ– GPT-2 åˆ†è¯å™¨\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"--- GPT-2 åˆ†è¯å™¨ä¿¡æ¯ ---\")\n",
    "print(f\"åˆ†è¯å™¨åç§°: {tokenizer.name}\")\n",
    "print(f\"è¯æ±‡è¡¨å¤§å° (æ€» Token æ•°): {tokenizer.max_token_value + 1}\")\n",
    "print(f\"EOT (End-of-Text) Token ID: {tokenizer.eot_token}\")\n",
    "\n",
    "# æ„å»ºä¸‰ä»½æ•°æ®é›†ï¼›éªŒè¯/æµ‹è¯•å…±ç”¨è®­ç»ƒé›†ç»Ÿè®¡å‡ºçš„ max_length\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "\n",
    "# DataLoader å‚æ•°ï¼š\n",
    "# - batch_size: æ¯æ‰¹æ ·æœ¬æ•°ï¼›\n",
    "# - shuffle: è®­ç»ƒé›†éœ€æ‰“ä¹±ï¼›\n",
    "# - drop_last: è®­ç»ƒæ—¶ä¸¢å¼ƒæœ€åä¸è¶³ä¸€æ‰¹çš„æ•°æ®ï¼Œä¾¿äºæ‰¹å½’ä¸€ï¼›\n",
    "# - num_workers: æ•°æ®åŠ è½½å­è¿›ç¨‹æ•°ï¼ˆWindows/ç¬”è®°æœ¬ç¯å¢ƒå¯è®¾ 0ï¼‰ã€‚\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)  # å›ºå®šéšæœºç§å­ï¼Œä¿è¯ç»“æœå¯å¤ç°\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "print(\"--- æ‰¹æ¬¡æ•° ---\")\n",
    "print(f\"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)} | éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)} | æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## E.3 åˆå§‹åŒ– GPT-2 å¹¶æ”¹ä¸ºåˆ†ç±»ä»»åŠ¡\n",
    "\n",
    "æ­¥éª¤ï¼š\n",
    "1. ä¸‹è½½å¹¶åŠ è½½ GPT-2 æƒé‡åˆ°è‡ªå®šä¹‰ `GPTModel`ã€‚\n",
    "2. å°†è¯­è¨€å»ºæ¨¡å¤´æ›¿æ¢ä¸ºäºŒåˆ†ç±»å¤´ï¼ˆè¾“å‡ºç»´åº¦=2ï¼‰ã€‚\n",
    "3. éªŒè¯åŠ è½½æ˜¯å¦æ­£å¸¸ï¼šè®©æ¨¡å‹ç”Ÿæˆä¸€æ®µæ–‡æœ¬è§‚å¯Ÿè¾“å‡ºæ˜¯å¦é€šé¡ºã€‚\n",
    "4. æŸ¥çœ‹æœªå¾®è°ƒå‰çš„åŸºçº¿å‡†ç¡®ç‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self, n_layers):\n",
    "        self.cache = [None] * n_layers\n",
    "\n",
    "    def get(self, layer_idx):\n",
    "        return self.cache[layer_idx]\n",
    "\n",
    "    def update(self, layer_idx, value):\n",
    "        self.cache[layer_idx] = value\n",
    "\n",
    "    def get_all(self):\n",
    "        return self.cache\n",
    "\n",
    "    def reset(self):\n",
    "        for i in range(len(self.cache)):\n",
    "            self.cache[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import requests\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path, backup_url)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\"), \"r\", encoding=\"utf-8\"))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        response = requests.get(download_url, stream=True, timeout=60)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "        # Check if file exists and has same size\n",
    "        if os.path.exists(destination):\n",
    "            file_size_local = os.path.getsize(destination)\n",
    "            if file_size and file_size == file_size_local:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return True\n",
    "\n",
    "        block_size = 1024  # 1 KB\n",
    "        desc = os.path.basename(download_url)\n",
    "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=desc) as progress_bar:\n",
    "            with open(destination, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=block_size):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "        return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except requests.exceptions.RequestException:\n",
    "        if backup_url is not None:\n",
    "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except requests.exceptions.RequestException:\n",
    "                pass\n",
    "\n",
    "        error_message = (\n",
    "            f\"Failed to download from both primary URL ({url})\"\n",
    "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
    "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
    "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
    "        )\n",
    "        print(error_message)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
    "# Source for \"Build a Large Language Model From Scratch\"\n",
    "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
    "#\n",
    "# This file collects all the relevant code that we covered thus far\n",
    "# throughout Chapters 2-6.\n",
    "# This file can be run as a standalone script.\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 2\n",
    "#####################################\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 3\n",
    "#####################################\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 4\n",
    "#####################################\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed-forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx is (B, T) array of indices in the current context\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "        # then only the last 5 tokens are used as context\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get the predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # logits åœ¨ GPU ä¸Š\n",
    "\n",
    "        # Focus only on the last time step\n",
    "        # (batch, n_token, vocab_size) becomes (batch, vocab_size)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # Get the idx of the vocab entry with the highest logits value\n",
    "        idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "#####################################\n",
    "# Chapter 5\n",
    "#####################################\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # New (not in book): numerical stability tip to get equivalent results on mps device\n",
    "            # subtract rowwise max before softmax\n",
    "            logits = logits - logits.max(dim=-1, keepdim=True).values\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    # Initialize lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # Reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # Calculate loss gradients\n",
    "            optimizer.step()  # Update model weights using loss gradients\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional evaluation step\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(\n",
    "                    model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "        # Print a sample text after each epoch\n",
    "        generate_and_print_sample(\n",
    "            model, tokenizer, device, start_context\n",
    "        )\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded,\n",
    "            max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias,\n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift,\n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift,\n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])\n",
    "\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "    # Plot training and validation loss against epochs\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "    # Create a second x-axis for tokens seen\n",
    "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "    fig.tight_layout()  # Adjust layout to make room\n",
    "    plt.savefig(\"loss-plot.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
      "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
      "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
      "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
      "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n",
      "Every effort moves you\"\"\"\"\"\"\"!\"\"\"\"\"\"!\n",
      "è¯„ä¼°æœªå¾®è°ƒåŸºçº¿ï¼ˆå„å– 10 æ‰¹ï¼‰...\n",
      "Train: 46.25% | Val: 45.00% | Test: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½ GPT-2 é¢„è®­ç»ƒæ¨¡å‹å¹¶æ”¹ä¸ºåˆ†ç±»ä»»åŠ¡\n",
    "# è¯´æ˜ï¼š\n",
    "# - ä½¿ç”¨é¡¹ç›®æä¾›çš„ä¸‹è½½ä¸è£…è½½å‡½æ•°ï¼Œå°†å…¬å¼€çš„ GPT-2 é¢„è®­ç»ƒæƒé‡å¯¼å…¥è‡ªå®šä¹‰ `GPTModel`ã€‚\n",
    "# - å°†åŸæœ¬çš„è¯­è¨€å»ºæ¨¡è¾“å‡ºå¤´æ›¿æ¢ä¸ºäºŒåˆ†ç±»å¤´ï¼ˆè¾“å‡ºç»´åº¦=2ï¼‰ã€‚\n",
    "# - é€šè¿‡ä¸€æ¬¡ç®€å•æ–‡æœ¬ç”Ÿæˆåš sanity checkï¼Œç¡®è®¤æ¨¡å‹åŠ è½½æ— è¯¯ã€‚\n",
    "# - è¯„ä¼° LoRA å‰çš„åŸºçº¿å‡†ç¡®ç‡ï¼ˆå°‘é‡æ‰¹æ¬¡æŠ½æ ·ï¼‰ï¼Œä½œä¸ºå¯¹æ¯”åŸºå‡†ã€‚\n",
    "\n",
    "# from gpt_download import download_and_load_gpt2\n",
    "# from previous_chapters import GPTModel, load_weights_into_gpt\n",
    "# from previous_chapters import (\n",
    "#     generate_text_simple,\n",
    "#     text_to_token_ids,\n",
    "#     token_ids_to_text,\n",
    "#     calc_accuracy_loader\n",
    "# )\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"  # å¯æ”¹ä¸º medium/large/xlï¼Œä½†æ˜¾å­˜ä¸ä¸‹è½½æ—¶é•¿ä¼šå¢åŠ \n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # GPT-2 è¯è¡¨å¤§å°\n",
    "    \"context_length\": 1024,  # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦\n",
    "    \"drop_rate\": 0.0,        # å…ˆä¸åŠ  dropoutï¼Œä¾¿äºå¯¹æ¯”\n",
    "    \"qkv_bias\": True         # æ³¨æ„åŠ› QKV æ˜¯å¦ä½¿ç”¨ bias\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "# ä¸‹è½½æƒé‡å¹¶è£…è½½åˆ°è‡ªå®šä¹‰ GPTModel\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()\n",
    "\n",
    "# æ›¿æ¢è¾“å‡ºå¤´ä¸ºäºŒåˆ†ç±»å¤´\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=BASE_CONFIG[\"emb_dim\"], out_features=num_classes)\n",
    "\n",
    "# è®¾å¤‡é€‰æ‹©ï¼ˆä¼˜å…ˆä½¿ç”¨ CUDAï¼Œå…¶æ¬¡ CPUï¼‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# æ–‡æœ¬ç”Ÿæˆ sanity checkï¼ˆåŠ è½½æ˜¯å¦æˆåŠŸï¼‰\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "# 1. ç”Ÿæˆ token ID å¼ é‡ï¼ˆé»˜è®¤åœ¨ CPU ä¸Šï¼‰\n",
    "input_ids_cpu = text_to_token_ids(text_1, tokenizer)\n",
    "# 2. ğŸŒŸ å…³é”®ä¿®æ­£ï¼šå°†è¾“å…¥å¼ é‡ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ (GPU/CUDA) ğŸŒŸ\n",
    "input_ids_device = input_ids_cpu.to(device)\n",
    "# 3. ä½¿ç”¨å·²åœ¨æ­£ç¡®è®¾å¤‡ä¸Šçš„å¼ é‡è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ\n",
    "ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=input_ids_device,  # ä½¿ç”¨å·²ç§»åŠ¨åˆ° device çš„å¼ é‡\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    ")\n",
    "print(token_ids_to_text(ids, tokenizer))\n",
    "\n",
    "# æ–‡æœ¬ç”Ÿæˆ sanity checkï¼ˆåŠ è½½æ˜¯å¦æˆåŠŸï¼‰\n",
    "# text_1 = \"Every effort moves you\"\n",
    "# ids = generate_text_simple(\n",
    "#     model=model,\n",
    "#     idx=text_to_token_ids(text_1, tokenizer),\n",
    "#     max_new_tokens=15,\n",
    "#     context_size=BASE_CONFIG[\"context_length\"],\n",
    "# )\n",
    "# print(token_ids_to_text(ids, tokenizer))\n",
    "\n",
    "# æœªå¾®è°ƒå‰çš„åŸºçº¿å‡†ç¡®ç‡ï¼ˆæŠ½æ ·è¯„ä¼° 10 æ‰¹ï¼‰\n",
    "torch.manual_seed(123)\n",
    "print(\"è¯„ä¼°æœªå¾®è°ƒåŸºçº¿ï¼ˆå„å– 10 æ‰¹ï¼‰...\")\n",
    "train_acc = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_acc = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_acc = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "print(f\"Train: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}% | Test: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## E.4 LoRA å®ç°ä¸å…¨æ¨¡å‹æ›¿æ¢\n",
    "\n",
    "æˆ‘ä»¬å®ç°ä¸¤ä¸ªç±»ï¼š\n",
    "- `LoRALayer`ï¼šæ„é€  Aã€B ä¸¤ä¸ªçŸ©é˜µï¼Œå‰å‘è¿”å› `alpha * (x @ A @ B)`ï¼›B åˆå§‹åŒ–ä¸º 0ï¼Œç¡®ä¿æ›¿æ¢ååˆå§‹è¾“å‡ºä¸å˜ã€‚\n",
    "- `LinearWithLoRA`ï¼šåŒ…è£…åŸ `Linear` å±‚ï¼Œè¾“å‡ºä¸º `linear(x) + lora(x)`ã€‚\n",
    "\n",
    "å¹¶æä¾› `replace_linear_with_lora(model, rank, alpha)` é€’å½’æ›¿æ¢å‡½æ•°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹ LoRA æ¨¡å‹æ›¿æ¢...\n",
      "ğŸ“Š æ›¿æ¢å‰å¯è®­ç»ƒå‚æ•°: 124,441,346\n",
      "ğŸ”’ å†»ç»“åå¯è®­ç»ƒå‚æ•°: 0\n",
      "ğŸ¯ LoRA å¯è®­ç»ƒå‚æ•°: 2,666,528\n",
      "âœ… LoRA æ›¿æ¢ä¸å‚æ•°å†»ç»“è®¾ç½®å®Œæˆï¼\n",
      "ğŸ’¡ å‚æ•°æ•ˆç‡æå‡ï¼šä»…è®­ç»ƒ 2,666,528 ä¸ªå‚æ•°ï¼Œç›¸æ¯”å…¨é‡å¾®è°ƒå¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ LoRA æ ¸å¿ƒå®ç°ï¼šä½ç§©é€‚é…å±‚ä¸æ¨¡å‹æ›¿æ¢\n",
    "# \n",
    "# æœ¬æ¨¡å—å®ç°äº† LoRA çš„æ ¸å¿ƒç»„ä»¶ï¼š\n",
    "# 1. LoRALayerï¼šä½ç§©åˆ†è§£å±‚ï¼Œå®ç° Î”W â‰ˆ A Ã— B çš„è¿‘ä¼¼\n",
    "# 2. LinearWithLoRAï¼šåŒ…è£…åŸå§‹çº¿æ€§å±‚ï¼Œå åŠ  LoRA å¢é‡\n",
    "# 3. replace_linear_with_loraï¼šé€’å½’æ›¿æ¢æ¨¡å‹ä¸­çš„æ‰€æœ‰çº¿æ€§å±‚\n",
    "\n",
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    LoRA ä½ç§©é€‚é…å±‚\n",
    "    \n",
    "    æ ¸å¿ƒåŸç†ï¼š\n",
    "    - ç”¨ä¸¤ä¸ªå°çŸ©é˜µ Aã€B è¿‘ä¼¼åŸå§‹æƒé‡æ›´æ–°ï¼šÎ”W â‰ˆ A Ã— B\n",
    "    - A âˆˆ â„^(dÃ—r)ï¼ŒB âˆˆ â„^(rÃ—k)ï¼Œå…¶ä¸­ r << min(d,k)\n",
    "    - å‰å‘ä¼ æ’­ï¼šy = Î± Ã— (x @ A @ B)\n",
    "    \n",
    "    å‚æ•°è¯´æ˜ï¼š\n",
    "        in_dim: è¾“å…¥ç»´åº¦ï¼ˆä¸åŸå§‹ Linear å±‚ä¸€è‡´ï¼‰\n",
    "        out_dim: è¾“å‡ºç»´åº¦ï¼ˆä¸åŸå§‹ Linear å±‚ä¸€è‡´ï¼‰\n",
    "        rank: ä½ç§©åˆ†è§£çš„ç§©ï¼Œæ§åˆ¶å‚æ•°é‡å’Œè¡¨è¾¾èƒ½åŠ›\n",
    "        alpha: ç¼©æ”¾ç³»æ•°ï¼Œå¹³è¡¡åŸå§‹æƒé‡å’Œ LoRA å¢é‡çš„è´¡çŒ®\n",
    "    \n",
    "    åˆå§‹åŒ–ç­–ç•¥ï¼š\n",
    "        - A çŸ©é˜µï¼šä½¿ç”¨ Kaiming Uniform åˆå§‹åŒ–ï¼Œä¿æŒæ¢¯åº¦ç¨³å®šæ€§\n",
    "        - B çŸ©é˜µï¼šåˆå§‹åŒ–ä¸º 0ï¼Œç¡®ä¿æ›¿æ¢ååˆå§‹è¾“å‡ºä¸å˜\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim: int, out_dim: int, rank: int, alpha: float):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­è®¡ç®— LoRA å¢é‡\n",
    "        \n",
    "        è®¡ç®—è¿‡ç¨‹ï¼š\n",
    "        1. x @ Aï¼šå°†è¾“å…¥æŠ•å½±åˆ°ä½ç»´ç©ºé—´\n",
    "        2. (x @ A) @ Bï¼šä»ä½ç»´ç©ºé—´æŠ•å½±åˆ°è¾“å‡ºç©ºé—´\n",
    "        3. Î± Ã— (x @ A @ B)ï¼šåº”ç”¨ç¼©æ”¾ç³»æ•°\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            LoRA å¢é‡è¾“å‡ºï¼Œå½¢çŠ¶ä¸åŸå§‹çº¿æ€§å±‚è¾“å‡ºä¸€è‡´\n",
    "        \"\"\"\n",
    "        return self.alpha * (x @ self.A @ self.B)\n",
    "\n",
    "\n",
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    å¸¦ LoRA å¢é‡çš„çº¿æ€§å±‚åŒ…è£…å™¨\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - åŒ…è£…åŸå§‹ Linear å±‚ï¼Œåœ¨ä¸ä¿®æ”¹åŸæƒé‡çš„å‰æä¸‹æ·»åŠ  LoRA å¢é‡\n",
    "    - å‰å‘ä¼ æ’­ï¼šoutput = original_linear(x) + lora_increment(x)\n",
    "    - è®­ç»ƒæ—¶åªä¼˜åŒ– LoRA å‚æ•°ï¼ŒåŸå§‹å‚æ•°ä¿æŒå†»ç»“\n",
    "    \n",
    "    è®¾è®¡ä¼˜åŠ¿ï¼š\n",
    "    - æ¨¡å—åŒ–ï¼šLoRA å¢é‡å¯ç‹¬ç«‹ä¿å­˜å’ŒåŠ è½½\n",
    "    - çµæ´»æ€§ï¼šæ”¯æŒåŠ¨æ€å¯ç”¨/ç¦ç”¨ LoRA åˆ†æ”¯\n",
    "    - å…¼å®¹æ€§ï¼šä¸åŸå§‹æ¨¡å‹å®Œå…¨å…¼å®¹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, linear: torch.nn.Linear, rank: int, alpha: float):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–åŒ…è£…å™¨\n",
    "        \n",
    "        å‚æ•°ï¼š\n",
    "            linear: åŸå§‹çº¿æ€§å±‚\n",
    "            rank: LoRA ç§©\n",
    "            alpha: ç¼©æ”¾ç³»æ•°\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        å‰å‘ä¼ æ’­ï¼šåŸå§‹è¾“å‡º + LoRA å¢é‡\n",
    "        \n",
    "        è®¡ç®—è¿‡ç¨‹ï¼š\n",
    "        1. åŸå§‹çº¿æ€§å˜æ¢ï¼šy_orig = x @ W\n",
    "        2. LoRA å¢é‡ï¼šy_lora = Î± Ã— (x @ A @ B)\n",
    "        3. æœ€ç»ˆè¾“å‡ºï¼šy = y_orig + y_lora\n",
    "        \n",
    "        è¿”å›ï¼š\n",
    "            ç»„åˆåçš„è¾“å‡ºå¼ é‡\n",
    "        \"\"\"\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "\n",
    "def replace_linear_with_lora(model: torch.nn.Module, rank: int, alpha: float):\n",
    "    \"\"\"\n",
    "    é€’å½’æ›¿æ¢æ¨¡å‹ä¸­çš„æ‰€æœ‰ Linear å±‚ä¸º LinearWithLoRA\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - æ·±åº¦éå†æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—\n",
    "    - å°† torch.nn.Linear æ›¿æ¢ä¸º LinearWithLoRA\n",
    "    - ä¿æŒæ¨¡å‹ç»“æ„ä¸å˜ï¼Œä»…æ›¿æ¢çº¿æ€§å±‚\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        model: å¾…æ›¿æ¢çš„æ¨¡å‹\n",
    "        rank: LoRA ç§©\n",
    "        alpha: ç¼©æ”¾ç³»æ•°\n",
    "    \n",
    "    æ³¨æ„äº‹é¡¹ï¼š\n",
    "    - æ›¿æ¢æ˜¯å°±åœ°è¿›è¡Œçš„ï¼Œä¼šä¿®æ”¹åŸå§‹æ¨¡å‹\n",
    "    - å»ºè®®åœ¨æ›¿æ¢å‰å¤‡ä»½æ¨¡å‹çŠ¶æ€\n",
    "    - æ›¿æ¢åéœ€è¦é‡æ–°è®¾ç½®ä¼˜åŒ–å™¨\n",
    "    \"\"\"\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # æ›¿æ¢ä¸º LoRA åŒ…è£…çš„çº¿æ€§å±‚\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # é€’å½’å¤„ç†å­æ¨¡å—\n",
    "            replace_linear_with_lora(module, rank, alpha)\n",
    "\n",
    "\n",
    "# ğŸ”„ æ‰§è¡Œ LoRA æ›¿æ¢æµç¨‹\n",
    "print(\"ğŸš€ å¼€å§‹ LoRA æ¨¡å‹æ›¿æ¢...\")\n",
    "\n",
    "# 1. ç»Ÿè®¡æ›¿æ¢å‰çš„å¯è®­ç»ƒå‚æ•°\n",
    "print(f\"ğŸ“Š æ›¿æ¢å‰å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# 2. å†»ç»“æ‰€æœ‰åŸå§‹å‚æ•°\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "print(f\"ğŸ”’ å†»ç»“åå¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# 3. æ‰§è¡Œ LoRA æ›¿æ¢ï¼ˆå…³é”®è¶…å‚æ•°è®¾ç½®ï¼‰\n",
    "RANK = 16      # ä½ç§©åˆ†è§£çš„ç§©ï¼Œæ§åˆ¶å‚æ•°é‡å’Œè¡¨è¾¾èƒ½åŠ›\n",
    "ALPHA = 16     # ç¼©æ”¾ç³»æ•°ï¼Œé€šå¸¸è®¾ç½®ä¸º rank çš„ 1-2 å€\n",
    "replace_linear_with_lora(model, rank=RANK, alpha=ALPHA)\n",
    "\n",
    "# 4. ç»Ÿè®¡ LoRA å¯è®­ç»ƒå‚æ•°\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"ğŸ¯ LoRA å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "\n",
    "# 5. å¯ç”¨ LoRA å‚æ•°è®­ç»ƒ\n",
    "for m in model.modules():\n",
    "    if isinstance(m, LoRALayer):\n",
    "        m.A.requires_grad = True\n",
    "        m.B.requires_grad = True\n",
    "\n",
    "# ğŸŒŸ åœ¨ LoRA æ¨¡å—æ·»åŠ åˆ°æ¨¡å‹åï¼Œå†æ¬¡å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡ ğŸŒŸ\n",
    "model.to(device)\n",
    "\n",
    "print(\"âœ… LoRA æ›¿æ¢ä¸å‚æ•°å†»ç»“è®¾ç½®å®Œæˆï¼\")\n",
    "print(f\"ğŸ’¡ å‚æ•°æ•ˆç‡æå‡ï¼šä»…è®­ç»ƒ {trainable_params:,} ä¸ªå‚æ•°ï¼Œç›¸æ¯”å…¨é‡å¾®è°ƒå¤§å¹…é™ä½æ˜¾å­˜éœ€æ±‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ›¿æ¢åã€è®­ç»ƒå‰çš„æŠ½æ ·å‡†ç¡®ç‡ï¼ˆå„å– 10 æ‰¹ï¼‰...\n",
      "Train: 46.25% | Val: 45.00% | Test: 48.75%\n"
     ]
    }
   ],
   "source": [
    "# LoRA æ›¿æ¢åï¼ŒéªŒè¯åŸºçº¿åº”åŸºæœ¬ä¸å˜\n",
    "# åŸç†ï¼šB åˆå§‹åŒ–ä¸º 0ï¼ŒLoRA å¢é‡åˆå§‹ä¸º 0ï¼Œå› æ­¤æ›¿æ¢åä½†æœªè®­ç»ƒå‰ï¼Œæ€§èƒ½åº”ä¸æ›¿æ¢å‰ä¸€è‡´ã€‚\n",
    "\n",
    "torch.manual_seed(123)\n",
    "print(\"æ›¿æ¢åã€è®­ç»ƒå‰çš„æŠ½æ ·å‡†ç¡®ç‡ï¼ˆå„å– 10 æ‰¹ï¼‰...\")\n",
    "train_acc = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_acc = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_acc = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "print(f\"Train: {train_acc*100:.2f}% | Val: {val_acc*100:.2f}% | Test: {test_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## è®­ç»ƒä¸è¯„ä¼°\n",
    "\n",
    "- ä»…ä¼˜åŒ– LoRA åˆ†æ”¯ï¼ˆAã€Bï¼‰ï¼ŒåŸæ¨¡å‹æƒé‡ä¿æŒå†»ç»“ã€‚\n",
    "- å­¦ä¹ ç‡å¯ä» 5e-5 èµ·æ­¥ï¼Œepoch å– 3-5 åšæ¼”ç¤ºå³å¯ã€‚\n",
    "- è®­ç»ƒä¸­å‘¨æœŸæ€§è¯„ä¼°è®­ç»ƒ/éªŒè¯æŸå¤±ä¸å‡†ç¡®ç‡ï¼Œç¡®è®¤æ”¶æ•›è¶‹åŠ¿ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ...\n",
      "ğŸ“‹ è®­ç»ƒé…ç½®ï¼š\n",
      "   - è®­ç»ƒè½®æ•°: 5\n",
      "   - å­¦ä¹ ç‡: 5e-05\n",
      "   - æƒé‡è¡°å‡: 0.1\n",
      "   - è¯„ä¼°é¢‘ç‡: æ¯ 50 æ­¥\n",
      "   - å¯è®­ç»ƒå‚æ•°: 2,666,528\n",
      "Ep 1 (Step 000000): Train loss 2.865, Val loss 2.584\n",
      "Ep 1 (Step 000050): Train loss 0.434, Val loss 0.390\n",
      "Ep 1 (Step 000100): Train loss 0.084, Val loss 0.242\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 2 (Step 000150): Train loss 0.238, Val loss 0.036\n",
      "Ep 2 (Step 000200): Train loss 0.092, Val loss 0.024\n",
      "Ep 2 (Step 000250): Train loss 0.129, Val loss 0.086\n",
      "Training accuracy: 95.00% | Validation accuracy: 97.50%\n",
      "Ep 3 (Step 000300): Train loss 0.289, Val loss 0.062\n",
      "Ep 3 (Step 000350): Train loss 0.107, Val loss 0.029\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "Ep 4 (Step 000400): Train loss 0.005, Val loss 0.024\n",
      "Ep 4 (Step 000450): Train loss 0.133, Val loss 0.173\n",
      "Ep 4 (Step 000500): Train loss 0.016, Val loss 0.017\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "Ep 5 (Step 000550): Train loss 0.001, Val loss 0.013\n",
      "Ep 5 (Step 000600): Train loss 0.001, Val loss 0.000\n",
      "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
      "âœ… è®­ç»ƒå®Œæˆï¼\n",
      "â±ï¸ è®­ç»ƒè€—æ—¶: 0.40 åˆ†é’Ÿ\n",
      "ğŸ“ˆ æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: 100.00%\n",
      "ğŸ“ˆ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ LoRA å¾®è°ƒè®­ç»ƒæµç¨‹\n",
    "# \n",
    "# æœ¬é˜¶æ®µæ‰§è¡Œ LoRA å‚æ•°çš„é«˜æ•ˆå¾®è°ƒè®­ç»ƒï¼š\n",
    "# 1. è®¾ç½®ä¼˜åŒ–å™¨ï¼Œä»…ä¼˜åŒ– LoRA å‚æ•°\n",
    "# 2. æ‰§è¡Œè®­ç»ƒå¾ªç¯ï¼Œç›‘æ§æŸå¤±å’Œå‡†ç¡®ç‡\n",
    "# 3. è®°å½•è®­ç»ƒæ—¶é—´å’Œæ€§èƒ½æŒ‡æ ‡\n",
    "\n",
    "import time\n",
    "\n",
    "# ğŸ”§ ä¼˜åŒ–å™¨é…ç½®\n",
    "# ä»…ä¼˜åŒ–éœ€è¦æ¢¯åº¦çš„å‚æ•°ï¼ˆæ­¤æ—¶åªåŒ…å« LoRA çš„ A å’Œ B çŸ©é˜µï¼‰\n",
    "optimizer = torch.optim.AdamW(\n",
    "    (p for p in model.parameters() if p.requires_grad), \n",
    "    lr=5e-5,           # å­¦ä¹ ç‡ï¼šLoRA å¾®è°ƒé€šå¸¸ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡\n",
    "    weight_decay=0.1   # æƒé‡è¡°å‡ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
    ")\n",
    "\n",
    "# ğŸ“Š è®­ç»ƒé…ç½®\n",
    "num_epochs = 5        # è®­ç»ƒè½®æ•°ï¼šLoRA å¾®è°ƒé€šå¸¸æ”¶æ•›è¾ƒå¿«\n",
    "eval_freq = 50        # è¯„ä¼°é¢‘ç‡ï¼šæ¯ 50 æ­¥è¯„ä¼°ä¸€æ¬¡\n",
    "eval_iter = 5         # è¯„ä¼°æ‰¹æ¬¡æ•°ï¼šé™åˆ¶è¯„ä¼°æ—¶é—´\n",
    "\n",
    "print(\"ğŸ¯ å¼€å§‹ LoRA å¾®è°ƒè®­ç»ƒ...\")\n",
    "print(f\"ğŸ“‹ è®­ç»ƒé…ç½®ï¼š\")\n",
    "print(f\"   - è®­ç»ƒè½®æ•°: {num_epochs}\")\n",
    "print(f\"   - å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"   - æƒé‡è¡°å‡: {optimizer.param_groups[0]['weight_decay']}\")\n",
    "print(f\"   - è¯„ä¼°é¢‘ç‡: æ¯ {eval_freq} æ­¥\")\n",
    "print(f\"   - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# â±ï¸ è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´\n",
    "start_time = time.time()\n",
    "\n",
    "# ğŸ”„ æ‰§è¡Œè®­ç»ƒå¾ªç¯\n",
    "torch.manual_seed(123)  # å›ºå®šéšæœºç§å­ç¡®ä¿å¯å¤ç°\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=eval_freq, eval_iter=eval_iter,\n",
    ")\n",
    "\n",
    "# â±ï¸ è®¡ç®—è®­ç»ƒè€—æ—¶\n",
    "end_time = time.time()\n",
    "training_time = (end_time - start_time) / 60\n",
    "\n",
    "print(f\"âœ… è®­ç»ƒå®Œæˆï¼\")\n",
    "print(f\"â±ï¸ è®­ç»ƒè€—æ—¶: {training_time:.2f} åˆ†é’Ÿ\")\n",
    "print(f\"ğŸ“ˆ æœ€ç»ˆè®­ç»ƒå‡†ç¡®ç‡: {train_accs[-1]*100:.2f}%\")\n",
    "print(f\"ğŸ“ˆ æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡: {val_accs[-1]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ç»˜åˆ¶è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "import torch\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 100.00% | Val: 99.33% | Test: 96.67%\n"
     ]
    }
   ],
   "source": [
    "# å…¨é‡è¯„ä¼°ï¼ˆå…¨æ•°æ®é›†ï¼‰\n",
    "\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Train: {train_accuracy*100:.2f}% | Val: {val_accuracy*100:.2f}% | Test: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ“ LoRA å¾®è°ƒæœ€ä½³å®è·µæŒ‡å—\n",
    "\n",
    "### ğŸ¯ ä½•æ—¶ä½¿ç”¨ LoRA\n",
    "\n",
    "**é€‚ç”¨åœºæ™¯ï¼š**\n",
    "- **å¤§æ¨¡å‹å¾®è°ƒ**ï¼šå‚æ•°é‡è¶…è¿‡ 1B çš„æ¨¡å‹ï¼Œå…¨é‡å¾®è°ƒæˆæœ¬è¿‡é«˜\n",
    "- **èµ„æºå—é™**ï¼šæ˜¾å­˜ä¸è¶³ã€è®­ç»ƒæ—¶é—´æœ‰é™çš„ç¯å¢ƒ\n",
    "- **å¤šä»»åŠ¡é€‚é…**ï¼šéœ€è¦ä¸ºåŒä¸€åŸºåº§æ¨¡å‹é€‚é…å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡\n",
    "- **å¿«é€ŸåŸå‹**ï¼šéœ€è¦å¿«é€ŸéªŒè¯æ¨¡å‹åœ¨ç‰¹å®šä»»åŠ¡ä¸Šçš„è¡¨ç°\n",
    "\n",
    "**ä¸é€‚ç”¨åœºæ™¯ï¼š**\n",
    "- **å°æ¨¡å‹**ï¼šå‚æ•°é‡ < 100M çš„æ¨¡å‹ï¼Œå…¨é‡å¾®è°ƒå¯èƒ½æ›´æœ‰æ•ˆ\n",
    "- **æ•°æ®å……è¶³**ï¼šæ‹¥æœ‰å¤§é‡é«˜è´¨é‡æ ‡æ³¨æ•°æ®ï¼Œå…¨é‡å¾®è°ƒæ•ˆæœæ›´å¥½\n",
    "- **è®¡ç®—èµ„æºå……è¶³**ï¼šæœ‰è¶³å¤Ÿçš„æ˜¾å­˜å’Œæ—¶é—´è¿›è¡Œå…¨é‡å¾®è°ƒ\n",
    "\n",
    "### âš™ï¸ è¶…å‚æ•°è°ƒä¼˜ç­–ç•¥\n",
    "\n",
    "#### Rankï¼ˆç§©ï¼‰é€‰æ‹©\n",
    "- **å°ä»»åŠ¡**ï¼šrank = 4-8ï¼Œé€‚åˆç®€å•åˆ†ç±»ä»»åŠ¡\n",
    "- **ä¸­ç­‰ä»»åŠ¡**ï¼šrank = 16-32ï¼Œé€‚åˆå¤§å¤šæ•° NLP ä»»åŠ¡\n",
    "- **å¤æ‚ä»»åŠ¡**ï¼šrank = 64-128ï¼Œé€‚åˆéœ€è¦å¼ºè¡¨è¾¾èƒ½åŠ›çš„ä»»åŠ¡\n",
    "- **ç»éªŒæ³•åˆ™**ï¼šrank è¶Šå¤§ï¼Œå‚æ•°é‡è¶Šå¤šï¼Œè¡¨è¾¾èƒ½åŠ›è¶Šå¼ºï¼Œä½†è¿‡æ‹Ÿåˆé£é™©ä¹Ÿè¶Šé«˜\n",
    "\n",
    "#### Alphaï¼ˆç¼©æ”¾ç³»æ•°ï¼‰è®¾ç½®\n",
    "- **å¸¸ç”¨è®¾ç½®**ï¼šalpha = rank æˆ– alpha = 2Ã—rank\n",
    "- **è°ƒä¼˜ç­–ç•¥**ï¼š\n",
    "  - ä» alpha = rank å¼€å§‹\n",
    "  - å¦‚æœ LoRA å½±å“è¿‡å°ï¼Œå¢åŠ  alpha\n",
    "  - å¦‚æœè®­ç»ƒä¸ç¨³å®šï¼Œå‡å°‘ alpha\n",
    "- **ç»éªŒèŒƒå›´**ï¼šalpha âˆˆ [rank/2, 2Ã—rank]\n",
    "\n",
    "#### å­¦ä¹ ç‡é…ç½®\n",
    "- **æ¨èèŒƒå›´**ï¼š1e-5 åˆ° 5e-4\n",
    "- **LoRA ä¸“ç”¨**ï¼šé€šå¸¸æ¯”å…¨é‡å¾®è°ƒä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡\n",
    "- **è‡ªé€‚åº”è°ƒæ•´**ï¼šæ ¹æ®è®­ç»ƒç¨³å®šæ€§åŠ¨æ€è°ƒæ•´\n",
    "\n",
    "### ğŸ” æ­£ç¡®æ€§éªŒè¯\n",
    "\n",
    "#### æ›¿æ¢åéªŒè¯\n",
    "```python\n",
    "# éªŒè¯ LoRA æ›¿æ¢ååˆå§‹æ€§èƒ½ä¸å˜\n",
    "baseline_acc = evaluate_before_lora()\n",
    "lora_acc = evaluate_after_lora_replacement()\n",
    "assert abs(baseline_acc - lora_acc) < 0.01  # è¯¯å·®åº” < 1%\n",
    "```\n",
    "\n",
    "#### è®­ç»ƒè¿‡ç¨‹ç›‘æ§\n",
    "- **æŸå¤±æ›²çº¿**ï¼šè®­ç»ƒæŸå¤±åº”ç¨³å®šä¸‹é™ï¼ŒéªŒè¯æŸå¤±ä¸åº”è¿‡åº¦ä¸Šå‡\n",
    "- **å‡†ç¡®ç‡**ï¼šéªŒè¯å‡†ç¡®ç‡åº”ç¨³æ­¥æå‡ï¼Œé¿å…è¿‡æ‹Ÿåˆ\n",
    "- **æ¢¯åº¦æ£€æŸ¥**ï¼šç¡®ä¿åªæœ‰ LoRA å‚æ•°æœ‰æ¢¯åº¦æ›´æ–°\n",
    "\n",
    "### ğŸš€ æ€§èƒ½ä¼˜åŒ–æŠ€å·§\n",
    "\n",
    "#### æ˜¾å­˜ä¼˜åŒ–\n",
    "- **æ¢¯åº¦æ£€æŸ¥ç‚¹**ï¼šåœ¨æ˜¾å­˜ä¸è¶³æ—¶å¯ç”¨\n",
    "- **æ··åˆç²¾åº¦è®­ç»ƒ**ï¼šä½¿ç”¨ FP16 å‡å°‘æ˜¾å­˜å ç”¨\n",
    "- **æ‰¹æ¬¡å¤§å°è°ƒæ•´**ï¼šæ ¹æ®æ˜¾å­˜åŠ¨æ€è°ƒæ•´ batch_size\n",
    "\n",
    "#### è®­ç»ƒåŠ é€Ÿ\n",
    "- **æ•°æ®å¹¶è¡Œ**ï¼šå¤š GPU è®­ç»ƒæ—¶ä½¿ç”¨\n",
    "- **æ¨¡å‹å¹¶è¡Œ**ï¼šè¶…å¤§æ¨¡å‹çš„åˆ†ç‰‡è®­ç»ƒ\n",
    "- **ä¼˜åŒ–å™¨é€‰æ‹©**ï¼šAdamW é€šå¸¸æ¯” SGD æ”¶æ•›æ›´å¿«\n",
    "\n",
    "### ğŸ“Š å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "#### è®­ç»ƒä¸æ”¶æ•›\n",
    "- **æ£€æŸ¥å­¦ä¹ ç‡**ï¼šå¯èƒ½è¿‡å¤§æˆ–è¿‡å°\n",
    "- **è°ƒæ•´ rank**ï¼šå¢åŠ è¡¨è¾¾èƒ½åŠ›\n",
    "- **æ•°æ®è´¨é‡**ï¼šæ£€æŸ¥æ ‡æ³¨æ•°æ®çš„å‡†ç¡®æ€§\n",
    "\n",
    "#### è¿‡æ‹Ÿåˆ\n",
    "- **å‡å°‘ rank**ï¼šé™ä½æ¨¡å‹å¤æ‚åº¦\n",
    "- **å¢åŠ æ­£åˆ™åŒ–**ï¼šæé«˜ weight_decay\n",
    "- **æ—©åœç­–ç•¥**ï¼šç›‘æ§éªŒè¯æŸå¤±ï¼ŒåŠæ—¶åœæ­¢\n",
    "\n",
    "#### æ€§èƒ½ä¸ä½³\n",
    "- **å¢åŠ  rank**ï¼šæå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›\n",
    "- **è°ƒæ•´ alpha**ï¼šå¹³è¡¡åŸå§‹æƒé‡å’Œ LoRA å¢é‡\n",
    "- **æ•°æ®å¢å¼º**ï¼šå¢åŠ è®­ç»ƒæ•°æ®çš„å¤šæ ·æ€§\n",
    "\n",
    "### ğŸ”§ éƒ¨ç½²å»ºè®®\n",
    "\n",
    "#### æ¨¡å‹ä¿å­˜\n",
    "- **åˆ†ç¦»ä¿å­˜**ï¼šåŸºåº§æ¨¡å‹å’Œ LoRA å‚æ•°åˆ†åˆ«ä¿å­˜\n",
    "- **ç‰ˆæœ¬ç®¡ç†**ï¼šä¸ºä¸åŒä»»åŠ¡ä¿å­˜ä¸åŒçš„ LoRA é€‚é…å™¨\n",
    "- **å‹ç¼©ä¼˜åŒ–**ï¼šä½¿ç”¨é‡åŒ–æŠ€æœ¯è¿›ä¸€æ­¥å‡å°æ–‡ä»¶å¤§å°\n",
    "\n",
    "#### æ¨ç†ä¼˜åŒ–\n",
    "- **æ¨¡å‹èåˆ**ï¼šå°† LoRA å‚æ•°åˆå¹¶åˆ°åŸºåº§æ¨¡å‹ä¸­\n",
    "- **æ‰¹å¤„ç†**ï¼šæ‰¹é‡å¤„ç†å¤šä¸ªè¯·æ±‚æé«˜æ•ˆç‡\n",
    "- **ç¼“å­˜ç­–ç•¥**ï¼šç¼“å­˜å¸¸ç”¨è¾“å…¥çš„å¤„ç†ç»“æœ\n",
    "\n",
    "### ğŸ’¡ è¿›é˜¶æŠ€å·§\n",
    "\n",
    "#### å¤šä»»åŠ¡ LoRA\n",
    "- **ä»»åŠ¡ç‰¹å®šé€‚é…å™¨**ï¼šä¸ºä¸åŒä»»åŠ¡è®­ç»ƒç‹¬ç«‹çš„ LoRA\n",
    "- **ä»»åŠ¡åˆ‡æ¢**ï¼šè¿è¡Œæ—¶åŠ¨æ€åŠ è½½ä¸åŒçš„ LoRA é€‚é…å™¨\n",
    "- **å‚æ•°å…±äº«**ï¼šåœ¨ç›¸å…³ä»»åŠ¡é—´å…±äº«éƒ¨åˆ† LoRA å‚æ•°\n",
    "\n",
    "#### æŒç»­å­¦ä¹ \n",
    "- **å¢é‡è®­ç»ƒ**ï¼šåœ¨æ–°æ•°æ®ä¸Šç»§ç»­è®­ç»ƒç°æœ‰ LoRA\n",
    "- **ç¾éš¾æ€§é—å¿˜**ï¼šä½¿ç”¨æ­£åˆ™åŒ–æŠ€æœ¯é˜²æ­¢é—å¿˜æ—§çŸ¥è¯†\n",
    "- **çŸ¥è¯†è’¸é¦**ï¼šä»å¤§æ¨¡å‹å‘å°æ¨¡å‹ä¼ é€’çŸ¥è¯†\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ä¿å­˜ä¸åŠ è½½ LoRA å‚æ•°ï¼ˆAdapterï¼‰\n",
    "\n",
    "- ä¿å­˜ï¼šä»…ä¿å­˜ LoRA åˆ†æ”¯ï¼ˆAã€Bã€alphaï¼‰ï¼Œä½“ç§¯å°ï¼Œä¾¿äºåˆ†å‘ä¸éƒ¨ç½²ã€‚\n",
    "- åŠ è½½ï¼šåœ¨åŒæ ·å®Œæˆ LoRA æ›¿æ¢åï¼Œå°†ä¿å­˜çš„å‚æ•°æ‹·å›å¯¹åº” `LoRALayer` ä¸­å³å¯ç”Ÿæ•ˆã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ ä¿å­˜ LoRA é€‚é…å™¨...\n",
      "ğŸ’¾ LoRA é€‚é…å™¨å·²ä¿å­˜åˆ°: lora_adapter.pt\n",
      "ğŸ“Š åŒ…å« 73 ä¸ª LoRA å±‚\n",
      "ğŸ“¦ æ–‡ä»¶å¤§å°: 10.17 MB\n",
      "\n",
      "ğŸ’¡ ä½¿ç”¨è¯´æ˜ï¼š\n",
      "   - ä¿å­˜çš„ LoRA å‚æ•°å¯ä»¥ç‹¬ç«‹åˆ†å‘å’Œéƒ¨ç½²\n",
      "   - åŠ è½½æ—¶éœ€è¦å…ˆå¯¹æ¨¡å‹æ‰§è¡Œç›¸åŒçš„ LoRA æ›¿æ¢\n",
      "   - æ”¯æŒåœ¨ä¸åŒç¯å¢ƒé—´è¿ç§» LoRA é€‚é…å™¨\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ LoRA é€‚é…å™¨ä¿å­˜ä¸åŠ è½½å·¥å…·\n",
    "# \n",
    "# æœ¬æ¨¡å—æä¾› LoRA å‚æ•°çš„ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½ï¼š\n",
    "# 1. ä¿å­˜ï¼šä»…ä¿å­˜ LoRA å¢é‡å‚æ•°ï¼Œä½“ç§¯å°ï¼Œä¾¿äºåˆ†å‘\n",
    "# 2. åŠ è½½ï¼šåœ¨ç›¸åŒæ¨¡å‹ç»“æ„ä¸Šæ¢å¤ LoRA å‚æ•°\n",
    "# 3. è·¨è®¾å¤‡ï¼šæ”¯æŒåœ¨ä¸åŒè®¾å¤‡é—´ä¼ è¾“ LoRA å‚æ•°\n",
    "\n",
    "import torch\n",
    "from typing import Dict, Tuple, Generator\n",
    "\n",
    "\n",
    "def iter_lora_named_modules(model: torch.nn.Module) -> Generator[Tuple[str, LoRALayer], None, None]:\n",
    "    \"\"\"\n",
    "    éå†æ¨¡å‹ä¸­æ‰€æœ‰ LoRALayer æ¨¡å—\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - æ·±åº¦éå†æ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å—\n",
    "    - ç­›é€‰å‡º LoRALayer ç±»å‹çš„æ¨¡å—\n",
    "    - è¿”å›æ¨¡å—çš„å®Œæ•´è·¯å¾„åå’Œæ¨¡å—å¯¹è±¡\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        model: å¾…éå†çš„æ¨¡å‹\n",
    "        \n",
    "    è¿”å›ï¼š\n",
    "        Generator: (æ¨¡å—å…¨å, LoRALayerå¯¹è±¡) çš„ç”Ÿæˆå™¨\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, LoRALayer):\n",
    "            yield name, module\n",
    "\n",
    "\n",
    "def save_lora_adapter(model: torch.nn.Module, filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    ä¿å­˜ LoRA é€‚é…å™¨å‚æ•°\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - ä»…ä¿å­˜ LoRA çš„ Aã€B çŸ©é˜µå’Œ alpha å‚æ•°\n",
    "    - ä½¿ç”¨æ¨¡å—å±‚çº§åä½œä¸ºé”®ï¼Œé¿å…åŠ è½½æ—¶é”™ä½\n",
    "    - è‡ªåŠ¨å°†å‚æ•°ç§»è‡³ CPUï¼ŒèŠ‚çœå­˜å‚¨ç©ºé—´\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        model: åŒ…å« LoRA å±‚çš„æ¨¡å‹\n",
    "        filepath: ä¿å­˜è·¯å¾„\n",
    "        \n",
    "    ä¿å­˜å†…å®¹ï¼š\n",
    "        - {module_name}.A: LoRA A çŸ©é˜µ\n",
    "        - {module_name}.B: LoRA B çŸ©é˜µ  \n",
    "        - {module_name}.alpha: ç¼©æ”¾ç³»æ•°\n",
    "    \"\"\"\n",
    "    state: Dict[str, torch.Tensor] = {}\n",
    "    \n",
    "    # éå†æ‰€æœ‰ LoRA å±‚ï¼Œæ”¶é›†å‚æ•°\n",
    "    for name, lora_module in iter_lora_named_modules(model):\n",
    "        # ä¿å­˜ A çŸ©é˜µï¼ˆç§»è‡³ CPUï¼‰\n",
    "        state[f\"{name}.A\"] = lora_module.A.detach().cpu()\n",
    "        # ä¿å­˜ B çŸ©é˜µï¼ˆç§»è‡³ CPUï¼‰\n",
    "        state[f\"{name}.B\"] = lora_module.B.detach().cpu()\n",
    "        # ä¿å­˜ alpha å‚æ•°ï¼ˆè½¬æ¢ä¸ºå¼ é‡ä¾¿äºè·¨è®¾å¤‡ï¼‰\n",
    "        state[f\"{name}.alpha\"] = torch.tensor(lora_module.alpha)\n",
    "    \n",
    "    # ä¿å­˜åˆ°æ–‡ä»¶\n",
    "    torch.save(state, filepath)\n",
    "    \n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    lora_layers = len([k for k in state if k.endswith('.A')])\n",
    "    file_size = sum(tensor.numel() * tensor.element_size() for tensor in state.values())\n",
    "    \n",
    "    print(f\"ğŸ’¾ LoRA é€‚é…å™¨å·²ä¿å­˜åˆ°: {filepath}\")\n",
    "    print(f\"ğŸ“Š åŒ…å« {lora_layers} ä¸ª LoRA å±‚\")\n",
    "    print(f\"ğŸ“¦ æ–‡ä»¶å¤§å°: {file_size / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "\n",
    "def load_lora_adapter(model: torch.nn.Module, filepath: str, map_location: str = \"cpu\") -> None:\n",
    "    \"\"\"\n",
    "    åŠ è½½ LoRA é€‚é…å™¨å‚æ•°\n",
    "    \n",
    "    åŠŸèƒ½è¯´æ˜ï¼š\n",
    "    - ä»æ–‡ä»¶åŠ è½½ LoRA å‚æ•°åˆ°å¯¹åº”æ¨¡å—\n",
    "    - è‡ªåŠ¨å¤„ç†è®¾å¤‡è½¬æ¢ï¼ˆCPU â†” GPUï¼‰\n",
    "    - æä¾›è¯¦ç»†çš„åŠ è½½çŠ¶æ€åé¦ˆ\n",
    "    \n",
    "    å‚æ•°ï¼š\n",
    "        model: ç›®æ ‡æ¨¡å‹ï¼ˆå¿…é¡»åŒ…å«ç›¸åŒç»“æ„çš„ LoRA å±‚ï¼‰\n",
    "        filepath: LoRA å‚æ•°æ–‡ä»¶è·¯å¾„\n",
    "        map_location: åŠ è½½æ—¶çš„è®¾å¤‡æ˜ å°„ï¼ˆ\"cpu\" æˆ– \"cuda\"ï¼‰\n",
    "        \n",
    "    æ³¨æ„äº‹é¡¹ï¼š\n",
    "    - æ¨¡å‹ç»“æ„å¿…é¡»ä¸ä¿å­˜æ—¶ä¸€è‡´\n",
    "    - åŠ è½½å‰éœ€è¦å…ˆæ‰§è¡Œ LoRA æ›¿æ¢\n",
    "    - æ”¯æŒè·¨è®¾å¤‡åŠ è½½ï¼ˆè‡ªåŠ¨è½¬æ¢åˆ°ç›®æ ‡è®¾å¤‡ï¼‰\n",
    "    \"\"\"\n",
    "    # åŠ è½½çŠ¶æ€å­—å…¸\n",
    "    state = torch.load(filepath, map_location=map_location)\n",
    "    \n",
    "    restored, missing = 0, []\n",
    "    \n",
    "    # éå†æ¨¡å‹ä¸­çš„ LoRA å±‚ï¼Œæ¢å¤å‚æ•°\n",
    "    for name, lora_module in iter_lora_named_modules(model):\n",
    "        keyA, keyB, keyAlpha = f\"{name}.A\", f\"{name}.B\", f\"{name}.alpha\"\n",
    "        \n",
    "        # æ£€æŸ¥æ‰€æœ‰å¿…éœ€çš„å‚æ•°æ˜¯å¦å­˜åœ¨\n",
    "        if keyA in state and keyB in state and keyAlpha in state:\n",
    "            with torch.no_grad():\n",
    "                # æ¢å¤ A çŸ©é˜µï¼ˆè‡ªåŠ¨è½¬æ¢åˆ°ç›®æ ‡è®¾å¤‡ï¼‰\n",
    "                lora_module.A.copy_(state[keyA].to(lora_module.A.device))\n",
    "                # æ¢å¤ B çŸ©é˜µï¼ˆè‡ªåŠ¨è½¬æ¢åˆ°ç›®æ ‡è®¾å¤‡ï¼‰\n",
    "                lora_module.B.copy_(state[keyB].to(lora_module.B.device))\n",
    "                # æ¢å¤ alpha å‚æ•°\n",
    "                lora_module.alpha = float(state[keyAlpha].item())\n",
    "            restored += 1\n",
    "        else:\n",
    "            missing.append(name)\n",
    "    \n",
    "    # è¾“å‡ºåŠ è½½ç»“æœ\n",
    "    print(f\"ğŸ”„ LoRA é€‚é…å™¨åŠ è½½å®Œæˆ\")\n",
    "    print(f\"âœ… æˆåŠŸæ¢å¤ {restored} ä¸ª LoRA å±‚\")\n",
    "    if missing:\n",
    "        print(f\"âš ï¸  ç¼ºå¤± {len(missing)} ä¸ª LoRA å±‚: {missing[:3]}{'...' if len(missing)>3 else ''}\")\n",
    "\n",
    "\n",
    "# ğŸ¯ ç¤ºä¾‹ï¼šä¿å­˜å½“å‰å¾®è°ƒå¾—åˆ°çš„ LoRA å‚æ•°\n",
    "print(\"ğŸ’¾ ä¿å­˜ LoRA é€‚é…å™¨...\")\n",
    "save_lora_adapter(model, \"lora_adapter.pt\")\n",
    "\n",
    "print(\"\\nğŸ’¡ ä½¿ç”¨è¯´æ˜ï¼š\")\n",
    "print(\"   - ä¿å­˜çš„ LoRA å‚æ•°å¯ä»¥ç‹¬ç«‹åˆ†å‘å’Œéƒ¨ç½²\")\n",
    "print(\"   - åŠ è½½æ—¶éœ€è¦å…ˆå¯¹æ¨¡å‹æ‰§è¡Œç›¸åŒçš„ LoRA æ›¿æ¢\")\n",
    "print(\"   - æ”¯æŒåœ¨ä¸åŒç¯å¢ƒé—´è¿ç§» LoRA é€‚é…å™¨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## æ¨ç†ç¤ºä¾‹ï¼ˆå¾®è°ƒåï¼‰\n",
    "\n",
    "- ä»¥æµ‹è¯•é›†æ ·æœ¬ä½œä¸ºæ¼”ç¤ºï¼Œæ‰“å°å‰è‹¥å¹²æ¡é¢„æµ‹ä¸çœŸå®æ ‡ç­¾ã€‚\n",
    "- æ³¨æ„ï¼šæ­¤å¤„ä»…ç”¨äºç›´è§‚æ„Ÿå—å¾®è°ƒæ•ˆæœï¼Œå®Œæ•´æŒ‡æ ‡è¯·å‚è€ƒä¸Šæ–¹è¯„ä¼°å•å…ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ·æœ¬0: é¢„æµ‹=1, çœŸå®=1\n",
      "æ ·æœ¬1: é¢„æµ‹=1, çœŸå®=1\n",
      "æ ·æœ¬2: é¢„æµ‹=1, çœŸå®=1\n",
      "æ ·æœ¬3: é¢„æµ‹=0, çœŸå®=0\n",
      "æ ·æœ¬4: é¢„æµ‹=1, çœŸå®=1\n"
     ]
    }
   ],
   "source": [
    "# æ¨ç†ç¤ºä¾‹\n",
    "# è¯´æ˜ï¼š\n",
    "# - è¿™é‡Œç¤ºèŒƒå¦‚ä½•ä» DataLoader ä¸­å–ä¸€ä¸ª batchï¼Œæ‰“å°é¢„æµ‹ç»“æœä¸çœŸå®æ ‡ç­¾å¯¹æ¯”ã€‚\n",
    "# - å…·ä½“å‰å‘é€»è¾‘ç”± `train_classifier_simple`/`calc_accuracy_loader` å†…éƒ¨å®šä¹‰çš„æ¨¡å‹è°ƒç”¨ä¿æŒä¸€è‡´ã€‚\n",
    "\n",
    "import torch\n",
    "\n",
    "# def predict_one_batch(data_loader, max_batches: int = 1):\n",
    "#     model.eval()\n",
    "#     shown = 0\n",
    "#     with torch.no_grad():\n",
    "#         for batch in data_loader:\n",
    "#             # å…¼å®¹ previous_chapters å†…éƒ¨çš„æ‰¹æ•°æ®ç»“æ„\n",
    "#             # å…¸å‹ä¸º (input_ids, labels) æˆ–å­—å…¸ï¼›å¦‚ç»“æ„ä¸åŒï¼Œè¯·å‚è€ƒåº“å®ç°åšç›¸åº”é€‚é…\n",
    "#             inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "#             logits = model(inputs)\n",
    "#             preds = torch.argmax(logits, dim=-1)\n",
    "#             for i in range(min(5, inputs.shape[0])):\n",
    "#                 print(f\"æ ·æœ¬{i}: é¢„æµ‹={int(preds[i].item())}, çœŸå®={int(labels[i].item())}\")\n",
    "#             shown += 1\n",
    "#             if shown >= max_batches:\n",
    "#                 break\n",
    "\n",
    "def predict_one_batch(data_loader, max_batches: int = 1):\n",
    "    model.eval()\n",
    "    shown = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "            \n",
    "            # ğŸŒŸ åˆ‡ç‰‡å–æœ€åä¸€ä¸ª token çš„ logits ğŸŒŸ\n",
    "            full_logits = model(inputs)\n",
    "            # Logits å½¢çŠ¶ä» [B, T, C] å˜ä¸º [B, C]\n",
    "            logits = full_logits[:, -1, :] \n",
    "            \n",
    "            # ğŸŒŸ argmax ç°åœ¨æ²¿ç€æ­£ç¡®çš„ç±»åˆ«ç»´åº¦ -1 è¿è¡Œ ğŸŒŸ\n",
    "            # preds å½¢çŠ¶å°†æ˜¯ [B]ï¼Œå…¶ä¸­ B æ˜¯æ‰¹æ¬¡å¤§å°ï¼ˆä¾‹å¦‚ 8ï¼‰\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            # æ ‡ç­¾ (labels) å·²ç»æ˜¯ [B] å½¢çŠ¶ï¼Œä¸éœ€è¦ä¿®æ”¹\n",
    "            \n",
    "            for i in range(min(5, inputs.shape[0])):\n",
    "                # ç°åœ¨ preds[i] æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [] çš„æ ‡é‡å¼ é‡ï¼Œå¯ä»¥å®‰å…¨åœ°ä½¿ç”¨ .item()\n",
    "                print(f\"æ ·æœ¬{i}: é¢„æµ‹={int(preds[i].item())}, çœŸå®={int(labels[i].item())}\")\n",
    "            shown += 1\n",
    "            if shown >= max_batches:\n",
    "                break\n",
    "\n",
    "predict_one_batch(test_loader, max_batches=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ‰ LoRA å¾®è°ƒæ•™ç¨‹æ€»ç»“\n",
    "\n",
    "### ğŸ† æ ¸å¿ƒæ”¶è·\n",
    "\n",
    "é€šè¿‡æœ¬æ•™ç¨‹ï¼Œæ‚¨å·²ç»æŒæ¡äº†ï¼š\n",
    "\n",
    "1. **LoRA åŸç†ç†è§£**ï¼šæ·±å…¥ç†è§£ä½ç§©åˆ†è§£å¦‚ä½•å®ç°å‚æ•°é«˜æ•ˆå¾®è°ƒ\n",
    "2. **å®Œæ•´å®ç°æµç¨‹**ï¼šä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²çš„ç«¯åˆ°ç«¯å·¥ä½œæµ\n",
    "3. **å…³é”®è¶…å‚è°ƒä¼˜**ï¼šæŒæ¡ rankã€alpha ç­‰å‚æ•°çš„é€‰æ‹©å’Œè°ƒä¼˜ç­–ç•¥\n",
    "4. **å®è·µæŠ€èƒ½æå‡**ï¼šå…·å¤‡ç‹¬ç«‹è¿›è¡Œ LoRA å¾®è°ƒçš„èƒ½åŠ›\n",
    "\n",
    "### ğŸ“Š æŠ€æœ¯ä¼˜åŠ¿æ€»ç»“\n",
    "\n",
    "#### å‚æ•°æ•ˆç‡\n",
    "- **æ˜¾è‘—é™ä½**ï¼šå¯è®­ç»ƒå‚æ•°ä» O(dk) é™ä½åˆ° O(r(d+k))\n",
    "- **æ˜¾å­˜å‹å¥½**ï¼šå¤§å¹…å‡å°‘è®­ç»ƒæ—¶çš„æ˜¾å­˜éœ€æ±‚\n",
    "- **è®­ç»ƒåŠ é€Ÿ**ï¼šå‡å°‘å‚æ•°æ›´æ–°ï¼Œæå‡è®­ç»ƒé€Ÿåº¦\n",
    "\n",
    "#### éƒ¨ç½²çµæ´»æ€§\n",
    "- **æ¨¡å—åŒ–è®¾è®¡**ï¼šLoRA å¢é‡å¯ç‹¬ç«‹ä¿å­˜å’ŒåŠ è½½\n",
    "- **å¤šä»»åŠ¡æ”¯æŒ**ï¼šåŒä¸€åŸºåº§æ¨¡å‹å¯é€‚é…å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡\n",
    "- **ç‰ˆæœ¬ç®¡ç†**ï¼šä¾¿äºç®¡ç†ä¸åŒä»»åŠ¡çš„é€‚é…å™¨\n",
    "\n",
    "#### æ€§èƒ½ä¿è¯\n",
    "- **åˆå§‹ä¸€è‡´æ€§**ï¼šB çŸ©é˜µåˆå§‹åŒ–ä¸º 0ï¼Œç¡®ä¿æ›¿æ¢åè¾“å‡ºä¸å˜\n",
    "- **æ”¶æ•›ç¨³å®š**ï¼šLoRA å¾®è°ƒé€šå¸¸æ¯”å…¨é‡å¾®è°ƒæ›´ç¨³å®š\n",
    "- **æ•ˆæœç›¸å½“**ï¼šåœ¨å¤§å¤šæ•°ä»»åŠ¡ä¸Šè¾¾åˆ°ä¸å…¨é‡å¾®è°ƒç›¸è¿‘çš„æ•ˆæœ\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®\n",
    "\n",
    "#### è¿›é˜¶æŠ€æœ¯\n",
    "- **QLoRA**ï¼šç»“åˆé‡åŒ–çš„ LoRAï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜éœ€æ±‚\n",
    "- **AdaLoRA**ï¼šè‡ªé€‚åº”è°ƒæ•´ä¸åŒå±‚çš„ rank\n",
    "- **LoRA+**ï¼šæ”¹è¿›çš„ LoRA å˜ä½“ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›\n",
    "\n",
    "#### å·¥ç¨‹å®è·µ\n",
    "- **HuggingFace é›†æˆ**ï¼šä½¿ç”¨ `peft` åº“è¿›è¡Œ LoRA å¾®è°ƒ\n",
    "- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šå¤š GPU/å¤šèŠ‚ç‚¹çš„å¤§è§„æ¨¡è®­ç»ƒ\n",
    "- **ç”Ÿäº§éƒ¨ç½²**ï¼šæ¨¡å‹æœåŠ¡åŒ–å’Œæ€§èƒ½ä¼˜åŒ–\n",
    "\n",
    "#### åº”ç”¨æ‹“å±•\n",
    "- **å¤šæ¨¡æ€ LoRA**ï¼šåœ¨è§†è§‰-è¯­è¨€æ¨¡å‹ä¸­çš„åº”ç”¨\n",
    "- **é¢†åŸŸé€‚é…**ï¼šç‰¹å®šé¢†åŸŸçš„ LoRA å¾®è°ƒ\n",
    "- **æŒç»­å­¦ä¹ **ï¼šå¢é‡å­¦ä¹ å’ŒçŸ¥è¯†ä¿æŒ\n",
    "\n",
    "### ğŸ’¡ å…³é”®è¦ç‚¹å›é¡¾\n",
    "\n",
    "1. **é€‰æ‹©åˆé€‚åœºæ™¯**ï¼šå¤§æ¨¡å‹ã€èµ„æºå—é™ã€å¤šä»»åŠ¡é€‚é…æ—¶ä¼˜å…ˆè€ƒè™‘ LoRA\n",
    "2. **è¶…å‚è°ƒä¼˜**ï¼šä» rank=16, alpha=16 å¼€å§‹ï¼Œæ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´\n",
    "3. **æ­£ç¡®æ€§éªŒè¯**ï¼šç¡®ä¿ LoRA æ›¿æ¢ååˆå§‹æ€§èƒ½ä¸å˜\n",
    "4. **æ€§èƒ½ç›‘æ§**ï¼šå¯†åˆ‡å…³æ³¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±å’Œå‡†ç¡®ç‡å˜åŒ–\n",
    "5. **éƒ¨ç½²ä¼˜åŒ–**ï¼šåˆç†ä¿å­˜å’ŒåŠ è½½ LoRA é€‚é…å™¨\n",
    "\n",
    "### ğŸ”— ç›¸å…³èµ„æº\n",
    "\n",
    "- **è®ºæ–‡åŸæ–‡**ï¼š[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)\n",
    "- **HuggingFace PEFT**ï¼š[Parameter-Efficient Fine-Tuning](https://huggingface.co/docs/peft/)\n",
    "- **å®è·µæ¡ˆä¾‹**ï¼šæ›´å¤š LoRA å¾®è°ƒçš„å®é™…åº”ç”¨æ¡ˆä¾‹\n",
    "- **ç¤¾åŒºæ”¯æŒ**ï¼šåŠ å…¥ç›¸å…³æŠ€æœ¯ç¤¾åŒºï¼Œè·å–æœ€æ–°è¿›å±•\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ¯ æ­å–œæ‚¨å®Œæˆäº† LoRA å¾®è°ƒçš„å®Œæ•´å­¦ä¹ ï¼** ç°åœ¨æ‚¨å·²ç»å…·å¤‡äº†ä½¿ç”¨ LoRA è¿›è¡Œå¤§æ¨¡å‹å¾®è°ƒçš„èƒ½åŠ›ï¼Œå¯ä»¥å¼€å§‹åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨è¿™äº›æŠ€æœ¯äº†ã€‚è®°ä½ï¼Œå®è·µæ˜¯æœ€å¥½çš„å­¦ä¹ æ–¹å¼ï¼Œå»ºè®®æ‚¨å°è¯•åœ¨ä¸åŒçš„æ•°æ®é›†å’Œä»»åŠ¡ä¸Šåº”ç”¨ LoRA å¾®è°ƒæŠ€æœ¯ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python(flyai_agent_in_action)",
   "language": "python",
   "name": "flyai_agent_in_action"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
