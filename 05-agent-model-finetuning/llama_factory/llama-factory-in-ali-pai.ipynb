{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f7fbb4-e992-4d5d-b7c7-c2309fc0bc31",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨LLaMA Factoryå¾®è°ƒQwen3æ¨¡å‹\n",
    "\n",
    "[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory)æ˜¯ä¸€æ¬¾å¼€æºä½ä»£ç å¤§æ¨¡å‹å¾®è°ƒæ¡†æ¶ï¼Œé›†æˆäº†ä¸šç•Œæœ€å¹¿æ³›ä½¿ç”¨çš„å¾®è°ƒæŠ€æœ¯ï¼Œæ”¯æŒé€šè¿‡Web UIç•Œé¢é›¶ä»£ç å¾®è°ƒå¤§æ¨¡å‹ï¼Œç›®å‰å·²ç»æˆä¸ºå¼€æºç¤¾åŒºå†…æœ€å—æ¬¢è¿çš„å¾®è°ƒæ¡†æ¶ï¼ŒGitHubæ˜Ÿæ ‡è¶…è¿‡2ä¸‡ã€‚\n",
    "æœ¬æ•™ç¨‹å°†åŸºäºå¼€æºçš„Qwen3 8Bæ¨¡å‹ï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨PAIå¹³å°åŠLLaMA Factoryè®­ç»ƒæ¡†æ¶å®Œæˆä¸­æ–‡åŒ»ç–—é¢†åŸŸçš„å¾®è°ƒå’Œè¯„ä¼°ã€‚ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe138c06-db8f-4422-b502-62bfcfca19e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## è¿è¡Œç¯å¢ƒè¦æ±‚\n",
    "\n",
    "- GPUæ¨èä½¿ç”¨24GBæ˜¾å­˜çš„A10ï¼ˆ`ecs.gn7i-c8g1.2xlarge`ï¼‰æˆ–æ›´é«˜é…ç½®\n",
    "- é•œåƒé€‰æ‹©DSWå®˜æ–¹é•œåƒ`pytorch:2.6.0-gpu-py311-cu124-ubuntu22.04`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9fdf00-1e2c-45b9-8d56-6302d943f0f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## ç³»ç»Ÿç¯å¢ƒæ£€æŸ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd803045-f131-448a-8023-de15d7574774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:26:49.728743Z",
     "iopub.status.busy": "2025-12-04T10:26:49.728585Z",
     "iopub.status.idle": "2025-12-04T10:26:50.849710Z",
     "shell.execute_reply": "2025-12-04T10:26:50.849055Z",
     "shell.execute_reply.started": "2025-12-04T10:26:49.728726Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/site-packages (from pandas==2.2.2) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### ç¯å¢ƒä¿¡æ¯\n",
      "| é¡¹ç›®         | ä¿¡æ¯                                                                              |\n",
      "|:-------------|:----------------------------------------------------------------------------------|\n",
      "| æ“ä½œç³»ç»Ÿ     | Linux 5.10.134-18.0.4.lifsea8.x86_64                                              |\n",
      "| CPU ä¿¡æ¯     | Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz (4 physical cores, 8 logical cores) |\n",
      "| å†…å­˜ä¿¡æ¯     | 29.37 GB (Available: 27.26 GB)                                                    |\n",
      "| GPU ä¿¡æ¯     | NVIDIA A10 (24564 MiB)                                                            |\n",
      "| CUDA ä¿¡æ¯    | 12.4                                                                              |\n",
      "| Python ç‰ˆæœ¬  | 3.11.14                                                                           |\n",
      "| Conda ç‰ˆæœ¬   | Conda not found                                                                   |\n",
      "| ç‰©ç†ç£ç›˜ç©ºé—´ | Total: 97.87 GB, Used: 17.77 GB, Free: 80.09 GB                                   |\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
    "#\n",
    "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
    "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
    "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
    "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
    "#\n",
    "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©ä½ ï¼š\n",
    "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
    "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
    "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "\n",
    "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
    "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
    "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
    "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
    "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
    "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
    "        try:\n",
    "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
    "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "\n",
    "\n",
    "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    if platform.system() == \"Windows\":\n",
    "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
    "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
    "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
    "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
    "\n",
    "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
    "        else:\n",
    "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "\n",
    "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
    "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
    "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_python_version():\n",
    "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
    "\n",
    "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
    "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
    "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
    "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
    "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
    "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
    "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
    "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
    "                     # å°è¯•è·å– codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
    "                     except:\n",
    "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
    "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
    "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
    "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
    "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
    "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "\n",
    "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
    "env_data = {\n",
    "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
    "        \"æ“ä½œç³»ç»Ÿ\",\n",
    "        \"CPU ä¿¡æ¯\",\n",
    "        \"å†…å­˜ä¿¡æ¯\",\n",
    "        \"GPU ä¿¡æ¯\",\n",
    "        \"CUDA ä¿¡æ¯\",\n",
    "        \"Python ç‰ˆæœ¬\",\n",
    "        \"Conda ç‰ˆæœ¬\",\n",
    "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
    "    ],\n",
    "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# æ‰“å°è¡¨æ ¼\n",
    "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
    "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50ba54e-fade-4d44-ad4b-6545643bc15c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## æ£€æŸ¥è½¯ç¡¬ä»¶ä¾èµ–\n",
    "\n",
    "| å¿…éœ€é¡¹       | è‡³å°‘   | æ¨è   |\n",
    "| ------------ | ------ | ------ |\n",
    "| python       | 3.9    | 3.10   |\n",
    "| torch        | 2.0.0  | 2.6.0  |\n",
    "| torchvision  | 0.15.0 | 0.21.0 |\n",
    "| transformers | 4.45.0 | 4.50.0 |\n",
    "| datasets     | 2.16.0 | 3.2.0  |\n",
    "| accelerate   | 0.34.0 | 1.2.1  |\n",
    "| peft         | 0.14.0 | 0.15.1 |\n",
    "| trl          | 0.8.6  | 0.9.6  |\n",
    "\n",
    "| å¯é€‰é¡¹       | è‡³å°‘   | æ¨è   |\n",
    "| ------------ | ------ | ------ |\n",
    "| CUDA         | 11.6   | 12.2   |\n",
    "| deepspeed    | 0.10.0 | 0.16.4 |\n",
    "| bitsandbytes | 0.39.0 | 0.43.1 |\n",
    "| vllm         | 0.4.3  | 0.8.2  |\n",
    "| flash-attn   | 2.5.6  | 2.7.2  |\n",
    "\n",
    "### ç¡¬ä»¶ä¾èµ–\n",
    "\n",
    "\\* *ä¼°ç®—å€¼*\n",
    "\n",
    "| æ–¹æ³•                            | ç²¾åº¦ | 7B    | 14B   | 30B   | 70B    | `x`B    |\n",
    "| ------------------------------- | ---- | ----- | ----- | ----- | ------ | ------- |\n",
    "| Full (`bf16` or `fp16`)         | 32   | 120GB | 240GB | 600GB | 1200GB | `18x`GB |\n",
    "| Full (`pure_bf16`)              | 16   | 60GB  | 120GB | 300GB | 600GB  | `8x`GB  |\n",
    "| Freeze/LoRA/GaLore/APOLLO/BAdam | 16   | 16GB  | 32GB  | 64GB  | 160GB  | `2x`GB  |\n",
    "| QLoRA                           | 8    | 10GB  | 20GB  | 40GB  | 80GB   | `x`GB   |\n",
    "| QLoRA                           | 4    | 6GB   | 12GB  | 24GB  | 48GB   | `x/2`GB |\n",
    "| QLoRA                           | 2    | 4GB   | 8GB   | 16GB  | 24GB   | `x/4`GB |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7aab387-96a7-4d4d-ae9d-e0395dfa0f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:26:50.850622Z",
     "iopub.status.busy": "2025-12-04T10:26:50.850383Z",
     "iopub.status.idle": "2025-12-04T10:26:52.004085Z",
     "shell.execute_reply": "2025-12-04T10:26:52.003343Z",
     "shell.execute_reply.started": "2025-12-04T10:26:50.850604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\n",
      "===========================================================================\n",
      "[ å¿…éœ€é¡¹ ]\n",
      "\u001b[92mâœ… python          | å½“å‰: å®Œç¾ (3.11.14)    | å¿…éœ€: >=3.9        | æ¨è: >=3.10\u001b[0m\n",
      "\u001b[92mâœ… torch           | å½“å‰: å®Œç¾ (2.6.0+cu124) | å¿…éœ€: >=2.0.0      | æ¨è: >=2.6.0\u001b[0m\n",
      "\u001b[92mâœ… torchvision     | å½“å‰: å®Œç¾ (0.21.0+cu124) | å¿…éœ€: >=0.15.0     | æ¨è: >=0.21.0\u001b[0m\n",
      "\u001b[92mâœ… transformers    | å½“å‰: å®Œç¾ (4.52.4)     | å¿…éœ€: >=4.45.0     | æ¨è: >=4.50.0\u001b[0m\n",
      "\u001b[92mâœ… datasets        | å½“å‰: å®Œç¾ (3.6.0)      | å¿…éœ€: >=2.16.0     | æ¨è: >=3.2.0\u001b[0m\n",
      "\u001b[92mâœ… accelerate      | å½“å‰: å®Œç¾ (1.7.0)      | å¿…éœ€: >=0.34.0     | æ¨è: >=1.2.1\u001b[0m\n",
      "\u001b[92mâœ… peft            | å½“å‰: å®Œç¾ (0.15.2)     | å¿…éœ€: >=0.14.0     | æ¨è: >=0.15.1\u001b[0m\n",
      "\u001b[92mâœ… trl             | å½“å‰: å®Œç¾ (0.9.6)      | å¿…éœ€: >=0.8.6      | æ¨è: >=0.9.6\u001b[0m\n",
      "\n",
      "[ å¯é€‰é¡¹ ]\n",
      "\u001b[92mâœ… cuda            | å½“å‰: å®Œç¾ (12.4)       | å¿…éœ€: >=11.6       | æ¨è: >=12.2\u001b[0m\n",
      "\u001b[90mâšª deepspeed       | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.10.0     | æ¨è: >=0.16.4\u001b[0m\n",
      "\u001b[92mâœ… bitsandbytes    | å½“å‰: å®Œç¾ (0.48.2)     | å¿…éœ€: >=0.39.0     | æ¨è: >=0.43.1\u001b[0m\n",
      "\u001b[90mâšª vllm            | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.4.3      | æ¨è: >=0.8.2\u001b[0m\n",
      "\u001b[90mâšª flash-attn      | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=2.5.6      | æ¨è: >=2.7.2\u001b[0m\n",
      "\n",
      "[ ç¡¬ä»¶ç®€æµ‹ ]\n",
      "âœ… æ£€æµ‹åˆ° 1 ä¸ª GPU è®¾å¤‡:\n",
      "   - GPU 0: NVIDIA A10 (23.5 GB VRAM)\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import importlib.metadata\n",
    "from packaging import version\n",
    "import torch\n",
    "\n",
    "# ================= æ‚¨çš„æœ€æ–°é…ç½®è¦æ±‚ (å·²å†æ¬¡æ›´æ–°) =================\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"python\":       (\"3.9\",    \"3.10\"),\n",
    "    \"torch\":        (\"2.0.0\",  \"2.6.0\"),\n",
    "    \"torchvision\":  (\"0.15.0\", \"0.21.0\"),\n",
    "    \"transformers\": (\"4.45.0\", \"4.50.0\"),\n",
    "    \"datasets\":     (\"2.16.0\", \"3.2.0\"),\n",
    "    \"accelerate\":   (\"0.34.0\", \"1.2.1\"),\n",
    "    \"peft\":         (\"0.14.0\", \"0.15.1\"),\n",
    "    \"trl\":          (\"0.8.6\",  \"0.9.6\"),\n",
    "}\n",
    "\n",
    "OPTIONAL_PACKAGES = {\n",
    "    \"cuda\":         (\"11.6\",   \"12.2\"),\n",
    "    \"deepspeed\":    (\"0.10.0\", \"0.16.4\"),\n",
    "    \"bitsandbytes\": (\"0.39.0\", \"0.43.1\"),\n",
    "    \"vllm\":         (\"0.4.3\",  \"0.8.2\"),\n",
    "    \"flash-attn\":   (\"2.5.6\",  \"2.7.2\"),\n",
    "}\n",
    "\n",
    "# ================= æ£€æµ‹é€»è¾‘ =================\n",
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        if package_name == \"python\": return platform.python_version()\n",
    "        elif package_name == \"cuda\": return torch.version.cuda if torch.cuda.is_available() else None\n",
    "        elif package_name == \"flash-attn\": return importlib.metadata.version(\"flash_attn\")\n",
    "        else: return importlib.metadata.version(package_name)\n",
    "    except importlib.metadata.PackageNotFoundError: return None\n",
    "\n",
    "def check_requirement(name, constraints, is_optional=False):\n",
    "    min_v, rec_v = constraints\n",
    "    current_v = get_package_version(name)\n",
    "    \n",
    "    # å®šä¹‰åˆ—å®½ä»¥ä¿æŒå¯¹é½\n",
    "    col_name = 15\n",
    "    col_curr = 15\n",
    "    col_req = 10\n",
    "    \n",
    "    if current_v is None:\n",
    "        status, msg, color = (\"âšª\", \"æœªå®‰è£… (å¯é€‰)\", \"\\033[90m\") if is_optional else (\"âŒ\", \"æœªå®‰è£… !!\", \"\\033[91m\")\n",
    "    else:\n",
    "        try:\n",
    "            curr_p, min_p, rec_p = version.parse(current_v), version.parse(min_v), version.parse(rec_v)\n",
    "            if curr_p >= rec_p:   status, msg, color = (\"âœ…\", f\"å®Œç¾ ({current_v})\", \"\\033[92m\") # Green\n",
    "            elif curr_p >= min_p: status, msg, color = (\"âš ï¸\", f\"è¾¾æ ‡ ({current_v})\", \"\\033[93m\") # Yellow\n",
    "            else:                 status, msg, color = (\"âŒ\", f\"è¿‡ä½ ({current_v})\", \"\\033[91m\") # Red\n",
    "        except Exception as e:\n",
    "            status, msg, color = (\"â“\", f\"è§£æé”™è¯¯\", \"\\033[93m\")\n",
    "\n",
    "    print(f\"{color}{status} {name:<{col_name}} | å½“å‰: {msg:<{col_curr}} | å¿…éœ€: >={min_v:<{col_req}} | æ¨è: >={rec_v}\\033[0m\")\n",
    "\n",
    "def check_gpu():\n",
    "    print(\"\\n[ ç¡¬ä»¶ç®€æµ‹ ]\")\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f\"âœ… æ£€æµ‹åˆ° {device_count} ä¸ª GPU è®¾å¤‡:\")\n",
    "        for i in range(device_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            # è½¬æ¢ä¸º GB\n",
    "            total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "            print(f\"   - GPU {i}: {gpu_name} ({total_mem:.1f} GB VRAM)\")\n",
    "    else:\n",
    "        print(\"âšª æœªæ£€æµ‹åˆ°å¯ç”¨ GPU (CUDAä¸å¯ç”¨)\")\n",
    "\n",
    "print(\"=\"*75)\n",
    "print(f\"ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\")\n",
    "print(\"=\"*75)\n",
    "print(\"[ å¿…éœ€é¡¹ ]\")\n",
    "for k, v in REQUIRED_PACKAGES.items(): check_requirement(k, v)\n",
    "print(\"\\n[ å¯é€‰é¡¹ ]\")\n",
    "for k, v in OPTIONAL_PACKAGES.items(): check_requirement(k, v, True)\n",
    "check_gpu()\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb110e1-2cf0-4ca4-98bd-d8c38322e944",
   "metadata": {},
   "source": [
    "## 1. å®‰è£…LLaMA Factory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e76f4d-15a3-4efe-9a32-190b63972e45",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œæ‹‰å–LLaMA-Factoryé¡¹ç›®åˆ°DSWå®ä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38481631-eeea-4d5f-abdc-f9aeeee439ae",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T10:26:52.004883Z",
     "iopub.status.busy": "2025-12-04T10:26:52.004626Z",
     "iopub.status.idle": "2025-12-04T10:26:56.010965Z",
     "shell.execute_reply": "2025-12-04T10:26:56.010375Z",
     "shell.execute_reply.started": "2025-12-04T10:26:52.004866Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "âš ï¸ æ£€æµ‹åˆ°æ—§ç›®å½• 'LLaMA-Factory'ï¼Œæ˜¯å¦åˆ é™¤ï¼Ÿ(è¾“å…¥ y ç¡®è®¤åˆ é™¤ï¼Œå…¶ä»–é”®å–æ¶ˆ):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš« æ“ä½œå·²å–æ¶ˆï¼Œä¿ç•™æ—§ç›®å½•ã€‚\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# å®šä¹‰ä»“åº“æ–‡ä»¶å¤¹åç§°\n",
    "repo_name = \"LLaMA-Factory\"\n",
    "\n",
    "# 1. åˆ¤æ–­æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨\n",
    "if os.path.exists(repo_name):\n",
    "    # è·å–ç”¨æˆ·è¾“å…¥\n",
    "    user_input = input(f\"âš ï¸ æ£€æµ‹åˆ°æ—§ç›®å½• '{repo_name}'ï¼Œæ˜¯å¦åˆ é™¤ï¼Ÿ(è¾“å…¥ y ç¡®è®¤åˆ é™¤ï¼Œå…¶ä»–é”®å–æ¶ˆ): \")\n",
    "    \n",
    "    # åˆ¤æ–­ç”¨æˆ·è¾“å…¥ (æ”¯æŒ y æˆ– Y)\n",
    "    if user_input.lower() == 'y':\n",
    "        print(f\"æ­£åœ¨åˆ é™¤ '{repo_name}'...\")\n",
    "        try:\n",
    "            shutil.rmtree(repo_name) # æ‰§è¡Œå®é™…åˆ é™¤\n",
    "            print(\"ğŸ—‘ï¸ æ—§ç›®å½•å·²æ¸…é™¤ï¼\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ åˆ é™¤å¤±è´¥: {e}\")\n",
    "    else:\n",
    "        print(\"ğŸš« æ“ä½œå·²å–æ¶ˆï¼Œä¿ç•™æ—§ç›®å½•ã€‚\")\n",
    "else:\n",
    "    print(f\"âœ… ç›®å½• '{repo_name}' ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä¸‹è½½ä»“åº“...\")\n",
    "    # å…‹éš†ä»“åº“\n",
    "    !git clone -b v0.9.3 https://github.com/hiyouga/LLaMA-Factory.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca0122b-7f5a-4f45-8e8f-5f3dbcd9ff0f",
   "metadata": {},
   "source": [
    "æ¥ç€ï¼Œæˆ‘ä»¬å®‰è£…LLaMA-Factoryä¾èµ–ç¯å¢ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a159cf2-7e19-44e5-8b86-7e8eb437a992",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T10:26:56.011690Z",
     "iopub.status.busy": "2025-12-04T10:26:56.011529Z",
     "iopub.status.idle": "2025-12-04T10:27:02.004635Z",
     "shell.execute_reply": "2025-12-04T10:27:02.003952Z",
     "shell.execute_reply.started": "2025-12-04T10:26:56.011674Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/workspace/LLaMA-Factory\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/site-packages (25.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Obtaining file:///mnt/workspace/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (4.52.4)\n",
      "Requirement already satisfied: datasets<=3.6.0,>=2.16.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (3.6.0)\n",
      "Requirement already satisfied: accelerate<=1.7.0,>=0.34.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (1.7.0)\n",
      "Requirement already satisfied: peft<=0.15.2,>=0.14.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.15.2)\n",
      "Requirement already satisfied: trl<=0.9.6,>=0.8.6 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.9.6)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.21.1)\n",
      "Requirement already satisfied: gradio<=5.31.0,>=4.38.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (5.31.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (1.16.3)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.8.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.2.1)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.12.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (6.33.1)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.38.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.123.5)\n",
      "Requirement already satisfied: sse-starlette in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (3.0.3)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (3.10.7)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.7.1)\n",
      "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (2.3.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (25.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (6.0.3)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (1.26.4)\n",
      "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (2.10.6)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (2.2.2)\n",
      "Requirement already satisfied: av in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (16.0.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.11.0)\n",
      "Requirement already satisfied: tyro<0.9.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.8.14)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (2.6.0+cu124)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.21.0+cu124)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (3.9.2)\n",
      "Requirement already satisfied: jieba in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (0.42.1)\n",
      "Requirement already satisfied: rouge-chinese in /usr/local/lib/python3.11/site-packages (from llamafactory==0.9.3) (1.0.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3) (7.1.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/site-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (22.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/site-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (3.13.2)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (4.12.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (1.10.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.28.1)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (3.11.4)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (11.3.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.14.7)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/site-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (4.15.0)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/site-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/site-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (3.11)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.11/site-packages (from fastapi->llamafactory==0.9.3) (0.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.9.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.9.3) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/site-packages (from pandas>=2.0.0->llamafactory==0.9.3) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/site-packages (from pydantic<=2.10.6->llamafactory==0.9.3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/site-packages (from pydantic<=2.10.6->llamafactory==0.9.3) (2.27.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3) (1.2.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.4,>=4.45.0->llamafactory==0.9.3) (2025.11.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/site-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (14.2.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/site-packages (from tyro<0.9.0->llamafactory==0.9.3) (0.17.0)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.11/site-packages (from tyro<0.9.0->llamafactory==0.9.3) (1.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (1.22.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/site-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/site-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/site-packages (from matplotlib>=3.7.0->llamafactory==0.9.3) (3.2.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->llamafactory==0.9.3) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3) (2.5.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3) (0.1.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/site-packages (from torch>=2.0.0->llamafactory==0.9.3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->llamafactory==0.9.3) (1.3.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/site-packages (from fire->llamafactory==0.9.3) (3.2.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (0.62.1)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (1.7.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (5.2.1)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/site-packages (from librosa->llamafactory==0.9.3) (1.1.2)\n",
      "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /usr/local/lib/python3.11/site-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3) (0.45.1)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/site-packages (from pooch>=1.1->librosa->llamafactory==0.9.3) (4.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/site-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/site-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3) (2.23)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/site-packages (from omegaconf->llamafactory==0.9.3) (4.9.3)\n",
      "Building wheels for collected packages: llamafactory\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3-0.editable-py3-none-any.whl size=27459 sha256=fa3429241150a8b0052e075455dcc92babda44c8ff52fc283ad590497c0a725a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ptpzmd_k/wheels/18/63/ec/7d8dd1bfb5f04a52ee923c20e91cc54ec06a170b202ec6a30d\n",
      "Successfully built llamafactory\n",
      "Installing collected packages: llamafactory\n",
      "  Attempting uninstall: llamafactory\n",
      "    Found existing installation: llamafactory 0.9.3\n",
      "    Uninstalling llamafactory-0.9.3:\n",
      "      Successfully uninstalled llamafactory-0.9.3\n",
      "Successfully installed llamafactory-0.9.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# 1. ã€å…³é”®ã€‘ä½¿ç”¨ %cd è¿›å…¥ç›®å½• (ä¸è¦ç”¨ !cd)\n",
    "%cd /mnt/workspace/LLaMA-Factory\n",
    "\n",
    "# 2. å»æ‰æ— å…³ä¾èµ–\n",
    "!pip install --upgrade pip\n",
    "!pip install modelscope>=1.11.0\n",
    "!pip install bitsandbytes>=0.39.0\n",
    "\n",
    "# 3. å®‰è£…é¡¹ç›®ä¾èµ–\n",
    "# è¿™ä¼šè‡ªåŠ¨å®‰è£… v0.9.3 æ‰€éœ€çš„å…¶ä»–åº“\n",
    "!pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204be19d-742d-4e1d-a3fd-a10932e21026",
   "metadata": {},
   "source": [
    "### æ£€éªŒä¾èµ–æ˜¯å¦æ»¡è¶³"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aadae0bb-b256-438e-aecd-b1d77773c7da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:27:02.005710Z",
     "iopub.status.busy": "2025-12-04T10:27:02.005513Z",
     "iopub.status.idle": "2025-12-04T10:27:02.025912Z",
     "shell.execute_reply": "2025-12-04T10:27:02.025188Z",
     "shell.execute_reply.started": "2025-12-04T10:27:02.005691Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\n",
      "===========================================================================\n",
      "[ å¿…éœ€é¡¹ ]\n",
      "\u001b[92mâœ… python          | å½“å‰: å®Œç¾ (3.11.14)    | å¿…éœ€: >=3.9        | æ¨è: >=3.10\u001b[0m\n",
      "\u001b[92mâœ… torch           | å½“å‰: å®Œç¾ (2.6.0+cu124) | å¿…éœ€: >=2.0.0      | æ¨è: >=2.6.0\u001b[0m\n",
      "\u001b[92mâœ… torchvision     | å½“å‰: å®Œç¾ (0.21.0+cu124) | å¿…éœ€: >=0.15.0     | æ¨è: >=0.21.0\u001b[0m\n",
      "\u001b[92mâœ… transformers    | å½“å‰: å®Œç¾ (4.52.4)     | å¿…éœ€: >=4.45.0     | æ¨è: >=4.50.0\u001b[0m\n",
      "\u001b[92mâœ… datasets        | å½“å‰: å®Œç¾ (3.6.0)      | å¿…éœ€: >=2.16.0     | æ¨è: >=3.2.0\u001b[0m\n",
      "\u001b[92mâœ… accelerate      | å½“å‰: å®Œç¾ (1.7.0)      | å¿…éœ€: >=0.34.0     | æ¨è: >=1.2.1\u001b[0m\n",
      "\u001b[92mâœ… peft            | å½“å‰: å®Œç¾ (0.15.2)     | å¿…éœ€: >=0.14.0     | æ¨è: >=0.15.1\u001b[0m\n",
      "\u001b[92mâœ… trl             | å½“å‰: å®Œç¾ (0.9.6)      | å¿…éœ€: >=0.8.6      | æ¨è: >=0.9.6\u001b[0m\n",
      "\n",
      "[ å¯é€‰é¡¹ ]\n",
      "\u001b[92mâœ… cuda            | å½“å‰: å®Œç¾ (12.4)       | å¿…éœ€: >=11.6       | æ¨è: >=12.2\u001b[0m\n",
      "\u001b[90mâšª deepspeed       | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.10.0     | æ¨è: >=0.16.4\u001b[0m\n",
      "\u001b[92mâœ… bitsandbytes    | å½“å‰: å®Œç¾ (0.48.2)     | å¿…éœ€: >=0.39.0     | æ¨è: >=0.43.1\u001b[0m\n",
      "\u001b[90mâšª vllm            | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=0.4.3      | æ¨è: >=0.8.2\u001b[0m\n",
      "\u001b[90mâšª flash-attn      | å½“å‰: æœªå®‰è£… (å¯é€‰)        | å¿…éœ€: >=2.5.6      | æ¨è: >=2.7.2\u001b[0m\n",
      "\n",
      "[ ç¡¬ä»¶ç®€æµ‹ ]\n",
      "âœ… æ£€æµ‹åˆ° 1 ä¸ª GPU è®¾å¤‡:\n",
      "   - GPU 0: NVIDIA A10 (23.5 GB VRAM)\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import platform\n",
    "import importlib.metadata\n",
    "from packaging import version\n",
    "import torch\n",
    "\n",
    "# ================= æ‚¨çš„æœ€æ–°é…ç½®è¦æ±‚ (å·²å†æ¬¡æ›´æ–°) =================\n",
    "REQUIRED_PACKAGES = {\n",
    "    \"python\":       (\"3.9\",    \"3.10\"),\n",
    "    \"torch\":        (\"2.0.0\",  \"2.6.0\"),\n",
    "    \"torchvision\":  (\"0.15.0\", \"0.21.0\"),\n",
    "    \"transformers\": (\"4.45.0\", \"4.50.0\"),\n",
    "    \"datasets\":     (\"2.16.0\", \"3.2.0\"),\n",
    "    \"accelerate\":   (\"0.34.0\", \"1.2.1\"),\n",
    "    \"peft\":         (\"0.14.0\", \"0.15.1\"),\n",
    "    \"trl\":          (\"0.8.6\",  \"0.9.6\"),\n",
    "}\n",
    "\n",
    "OPTIONAL_PACKAGES = {\n",
    "    \"cuda\":         (\"11.6\",   \"12.2\"),\n",
    "    \"deepspeed\":    (\"0.10.0\", \"0.16.4\"),\n",
    "    \"bitsandbytes\": (\"0.39.0\", \"0.43.1\"),\n",
    "    \"vllm\":         (\"0.4.3\",  \"0.8.2\"),\n",
    "    \"flash-attn\":   (\"2.5.6\",  \"2.7.2\"),\n",
    "}\n",
    "\n",
    "# ================= æ£€æµ‹é€»è¾‘ =================\n",
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        if package_name == \"python\": return platform.python_version()\n",
    "        elif package_name == \"cuda\": return torch.version.cuda if torch.cuda.is_available() else None\n",
    "        elif package_name == \"flash-attn\": return importlib.metadata.version(\"flash_attn\")\n",
    "        else: return importlib.metadata.version(package_name)\n",
    "    except importlib.metadata.PackageNotFoundError: return None\n",
    "\n",
    "def check_requirement(name, constraints, is_optional=False):\n",
    "    min_v, rec_v = constraints\n",
    "    current_v = get_package_version(name)\n",
    "    \n",
    "    # å®šä¹‰åˆ—å®½ä»¥ä¿æŒå¯¹é½\n",
    "    col_name = 15\n",
    "    col_curr = 15\n",
    "    col_req = 10\n",
    "    \n",
    "    if current_v is None:\n",
    "        status, msg, color = (\"âšª\", \"æœªå®‰è£… (å¯é€‰)\", \"\\033[90m\") if is_optional else (\"âŒ\", \"æœªå®‰è£… !!\", \"\\033[91m\")\n",
    "    else:\n",
    "        try:\n",
    "            curr_p, min_p, rec_p = version.parse(current_v), version.parse(min_v), version.parse(rec_v)\n",
    "            if curr_p >= rec_p:   status, msg, color = (\"âœ…\", f\"å®Œç¾ ({current_v})\", \"\\033[92m\") # Green\n",
    "            elif curr_p >= min_p: status, msg, color = (\"âš ï¸\", f\"è¾¾æ ‡ ({current_v})\", \"\\033[93m\") # Yellow\n",
    "            else:                 status, msg, color = (\"âŒ\", f\"è¿‡ä½ ({current_v})\", \"\\033[91m\") # Red\n",
    "        except Exception as e:\n",
    "            status, msg, color = (\"â“\", f\"è§£æé”™è¯¯\", \"\\033[93m\")\n",
    "\n",
    "    print(f\"{color}{status} {name:<{col_name}} | å½“å‰: {msg:<{col_curr}} | å¿…éœ€: >={min_v:<{col_req}} | æ¨è: >={rec_v}\\033[0m\")\n",
    "\n",
    "def check_gpu():\n",
    "    print(\"\\n[ ç¡¬ä»¶ç®€æµ‹ ]\")\n",
    "    if torch.cuda.is_available():\n",
    "        device_count = torch.cuda.device_count()\n",
    "        print(f\"âœ… æ£€æµ‹åˆ° {device_count} ä¸ª GPU è®¾å¤‡:\")\n",
    "        for i in range(device_count):\n",
    "            gpu_name = torch.cuda.get_device_name(i)\n",
    "            # è½¬æ¢ä¸º GB\n",
    "            total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)\n",
    "            print(f\"   - GPU {i}: {gpu_name} ({total_mem:.1f} GB VRAM)\")\n",
    "    else:\n",
    "        print(\"âšª æœªæ£€æµ‹åˆ°å¯ç”¨ GPU (CUDAä¸å¯ç”¨)\")\n",
    "\n",
    "print(\"=\"*75)\n",
    "print(f\"ğŸ› ï¸  ç¯å¢ƒä¾èµ–æ£€æµ‹æŠ¥å‘Š (æ ‡å‡†å·²å†æ¬¡æ›´æ–°)\")\n",
    "print(\"=\"*75)\n",
    "print(\"[ å¿…éœ€é¡¹ ]\")\n",
    "for k, v in REQUIRED_PACKAGES.items(): check_requirement(k, v)\n",
    "print(\"\\n[ å¯é€‰é¡¹ ]\")\n",
    "for k, v in OPTIONAL_PACKAGES.items(): check_requirement(k, v, True)\n",
    "check_gpu()\n",
    "print(\"=\"*75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14da143a-1457-4b45-8847-3ad80b98e3b7",
   "metadata": {},
   "source": [
    "è¿è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œå¦‚æœæ˜¾ç¤ºllamafactory-cliçš„ç‰ˆæœ¬ï¼Œåˆ™è¡¨ç¤ºå®‰è£…æˆåŠŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9865b7f4-016c-441b-94fe-0436a65e398a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:27:02.026572Z",
     "iopub.status.busy": "2025-12-04T10:27:02.026418Z",
     "iopub.status.idle": "2025-12-04T10:27:10.063740Z",
     "shell.execute_reply": "2025-12-04T10:27:10.063063Z",
     "shell.execute_reply.started": "2025-12-04T10:27:02.026556Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "| Welcome to LLaMA Factory, version 0.9.3                |\n",
      "|                                                        |\n",
      "| Project page: https://github.com/hiyouga/LLaMA-Factory |\n",
      "----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!llamafactory-cli version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507f4d9-5cef-45a3-96bd-a2bbd4d4a90d",
   "metadata": {},
   "source": [
    "## 2. ä¸‹è½½æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f2e63-865a-47ec-90a8-7e5af46b4d33",
   "metadata": {},
   "source": [
    "LLaMA-Factoryé¡¹ç›®å†…ç½®äº†ä¸°å¯Œçš„æ•°æ®é›†ï¼Œæ”¾åœ¨äº†`data`ç›®å½•ä¸‹ã€‚æ‚¨å¯ä»¥è·³è¿‡æœ¬æ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å†…ç½®æ•°æ®é›†ã€‚æ‚¨ä¹Ÿå¯ä»¥å‡†å¤‡è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå°†æ•°æ®å¤„ç†ä¸ºæ¡†æ¶ç‰¹å®šçš„æ ¼å¼ï¼Œæ”¾åœ¨`data`ä¸‹ï¼Œå¹¶ä¸”ä¿®æ”¹`dataset_info.json`æ–‡ä»¶ã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹å‡†å¤‡äº†ä¸€ä»½å¤šè½®å¯¹è¯æ•°æ®é›†ï¼Œè¿è¡Œä¸‹è¿°å‘½ä»¤ä¸‹è½½æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "748944fd-0545-4cdc-877a-1a5abcf50298",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T10:27:10.064522Z",
     "iopub.status.busy": "2025-12-04T10:27:10.064338Z",
     "iopub.status.idle": "2025-12-04T10:27:10.707467Z",
     "shell.execute_reply": "2025-12-04T10:27:10.706858Z",
     "shell.execute_reply.started": "2025-12-04T10:27:10.064503Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-12-04 18:27:10--  https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/llama_factory/data.zip\n",
      "Resolving atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)... 47.101.88.43\n",
      "Connecting to atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com (atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com)|47.101.88.43|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 402043 (393K) [application/zip]\n",
      "Saving to: â€˜data.zip.2â€™\n",
      "\n",
      "data.zip.2          100%[===================>] 392.62K  --.-KB/s    in 0.1s    \n",
      "\n",
      "2025-12-04 18:27:10 (3.73 MB/s) - â€˜data.zip.2â€™ saved [402043/402043]\n",
      "\n",
      "mv: cannot move 'data' to 'rawdata/data': Directory not empty\n"
     ]
    }
   ],
   "source": [
    "!wget https://atp-modelzoo-sh.oss-cn-shanghai.aliyuncs.com/release/tutorials/llama_factory/data.zip\n",
    "!mv data rawdata && unzip data.zip -d data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002d0f1-d726-414e-b8df-114b43791379",
   "metadata": {},
   "source": [
    "ä¸Šä¼ åŒ»ç–—æ•°æ®é›†ï¼Œå°†jsonlæ–‡ä»¶æ‹–æ‹½åˆ°`/mnt/workspace`æ–‡ä»¶ä¸‹ã€‚\n",
    "```\n",
    "root@dsw-523480-fd7f95d6d-cgjwf:/mnt/workspace# pwd\n",
    "/mnt/workspace\n",
    "root@dsw-523480-fd7f95d6d-cgjwf:/mnt/workspace# ll\n",
    "total 63916\n",
    "drwxr-xr-x  7 root root     4096 Dec  3 16:32  ./\n",
    "drwxr-xr-x  1 root root     4096 Dec  3 15:06  ../\n",
    "drwxr-xr-x  2 root root     4096 Dec  3 15:19  _html/\n",
    "drwxr-xr-x 16 root root     4096 Dec  3 15:59  LLaMA-Factory/\n",
    "-rw-r--r--  1 root root    87581 Dec  3 16:32  llama-factory-in-ali-pai.ipynb\n",
    "drwxr-xr-x  3 root root     4096 Dec  3 15:06  .local/\n",
    "-rw-r--r--  1 root root 65325060 Dec  3 16:21  medical_o1_sft_Chinese_alpaca_cot.jsonl\n",
    "drwxr-xr-x  2 root root     4096 Dec  3 15:10  .virtual_documents/\n",
    "```\n",
    "\n",
    "\n",
    "ä¿®æ”¹`/mnt/workspace`æ–‡ä»¶å¤¹ä¸‹â€œdataset_json.infoâ€æ–‡ä»¶ï¼Œåœ¨jsonæ–‡ä»¶å¼€å§‹ä½ç½®æ·»åŠ å¦‚ä¸‹æ‰€ç¤ºè„šæœ¬ã€‚\n",
    "```\n",
    "\"medical_sft\": {\n",
    "    \"file_name\": \"/workspace/LLaMA-Factory/data/medical_o1_sft_Chinese_alpaca_cot.jsonl\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"prompt\",\n",
    "      \"response\": \"response\"\n",
    "    }\n",
    "  }\n",
    "```\n",
    "\n",
    "â€œdataset_json.infoâ€æ–‡ä»¶æœ€ç»ˆå¦‚ä¸‹\n",
    "```\n",
    "{\n",
    "  \"medical_sft\": {\n",
    "    \"file_name\": \"/mnt/workspace/medical_o1_sft_Chinese_alpaca_cot.jsonl\",\n",
    "    \"columns\": {\n",
    "      \"prompt\": \"prompt\",\n",
    "      \"response\": \"response\"\n",
    "    }\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"file_name\": \"train.json\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  },\n",
    "  \"eval\": {\n",
    "    \"file_name\": \"eval.json\",\n",
    "    \"formatting\": \"sharegpt\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8241a6b7-cf72-4014-bd45-a2e5f434b70a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:27:10.708444Z",
     "iopub.status.busy": "2025-12-04T10:27:10.708259Z",
     "iopub.status.idle": "2025-12-04T10:27:10.713935Z",
     "shell.execute_reply": "2025-12-04T10:27:10.713446Z",
     "shell.execute_reply.started": "2025-12-04T10:27:10.708426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æˆåŠŸï¼å·²è‡ªåŠ¨æ·»åŠ é€—å·å¹¶æ³¨å†Œ 'medical_sft' åˆ°ï¼š\n",
      "/mnt/workspace/LLaMA-Factory/data/dataset_info.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 1. ä½ çš„æ•°æ®é›†è·¯å¾„\n",
    "jsonl_file = \"/mnt/workspace/medical_o1_sft_Chinese_alpaca_cot.jsonl\"\n",
    "# 2. LLaMA-Factory çš„é…ç½®æ–‡ä»¶è·¯å¾„\n",
    "config_file = \"/mnt/workspace/LLaMA-Factory/data/dataset_info.json\"\n",
    "\n",
    "# 3. è¦æ·»åŠ çš„é…ç½®å†…å®¹ (Pythonå­—å…¸æ ¼å¼ï¼Œä¸éœ€è¦æ‰‹åŠ¨ç®¡é€—å·)\n",
    "new_data = {\n",
    "    \"medical_sft\": {\n",
    "        \"file_name\": jsonl_file,\n",
    "        \"columns\": {\n",
    "            \"prompt\": \"prompt\",\n",
    "            \"response\": \"response\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# 4. æ‰§è¡Œè‡ªåŠ¨å†™å…¥\n",
    "try:\n",
    "    with open(config_file, 'r', encoding='utf-8') as f:\n",
    "        content = json.load(f)\n",
    "        \n",
    "    # æ›´æ–°æ•°æ®ï¼ˆç¨‹åºä¼šè‡ªåŠ¨å¤„ç†é€—å·åˆ†å‰²ï¼‰\n",
    "    content.update(new_data)\n",
    "    \n",
    "    with open(config_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "    print(f\"âœ… æˆåŠŸï¼å·²è‡ªåŠ¨æ·»åŠ é€—å·å¹¶æ³¨å†Œ 'medical_sft' åˆ°ï¼š\\n{config_file}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° LLaMA-Factory çš„é…ç½®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥ç›®å½•ç»“æ„ã€‚\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"âŒ é”™è¯¯ï¼šåŸé…ç½®æ–‡ä»¶æ ¼å¼æŸåï¼ˆå¯èƒ½ä¹‹å‰æ‰‹åŠ¨æ”¹åäº†ï¼‰ï¼Œè¯·è¿˜åŸæ–‡ä»¶ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c2c80e-4828-4c00-baf2-4d79e056263e",
   "metadata": {},
   "source": [
    "## 3. æ¨¡å‹å¾®è°ƒ\n",
    "### 3.1 å¯åŠ¨Web UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bcd446-eb8d-4a28-9909-98e08e5e57e9",
   "metadata": {},
   "source": [
    "åšå¥½å‰åºå‡†å¤‡å·¥ä½œåï¼Œç›´æ¥è¿è¡Œä¸‹è¿°å‘½ä»¤å°±å¯ä»¥å¯åŠ¨Web UIã€‚è¿™é‡Œç”¨åˆ°çš„ç¯å¢ƒå˜é‡è§£é‡Šå¦‚ä¸‹ï¼š\n",
    "- `USE_MODELSCOPE_HUB`è®¾ä¸º1ï¼Œè¡¨ç¤ºæ¨¡å‹æ¥æºæ˜¯ModelScopeã€‚ä½¿ç”¨HuggingFaceæ¨¡å‹å¯èƒ½ä¼šæœ‰ç½‘ç»œé—®é¢˜ã€‚\n",
    "\n",
    "ç‚¹å‡»è¿”å›çš„URLåœ°å€`http://0.0.0.0:7860`ï¼Œè¿›å…¥Web UIé¡µé¢ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3d0bf-668b-43e9-a146-33369c69a86a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-12-04T11:04:09.960111Z",
     "iopub.status.busy": "2025-12-04T11:04:09.959864Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n",
      "[WARNING|2025-12-04 19:05:54] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-12-04 19:05:54] llamafactory.hparams.parser:406 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,370 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-12-04 19:05:54,685 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:05:54,686 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:05:54,689 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,689 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2021] 2025-12-04 19:05:54,690 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2299] 2025-12-04 19:05:55,018 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-12-04 19:05:55] llamafactory.data.loader:143 >> Loading dataset /mnt/workspace/medical_o1_sft_Chinese_alpaca_cot.jsonl...\n",
      "training example:\n",
      "input_ids:\n",
      "[151644, 872, 198, 100345, 53481, 3837, 46944, 16, 104252, 99657, 18493, 106084, 117405, 100347, 42140, 44290, 30709, 36885, 55502, 3837, 101930, 16530, 100939, 39762, 3837, 100136, 99601, 116066, 26288, 29524, 100711, 3837, 101776, 99577, 88653, 115623, 3837, 39426, 16530, 118609, 3837, 117405, 16872, 18830, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 1773, 100137, 116067, 18493, 104823, 15946, 105262, 100678, 99252, 11319, 220, 14880, 104137, 113272, 62926, 107485, 102349, 8997, 99487, 109661, 18493, 104797, 117405, 17447, 45861, 112312, 30709, 36885, 55502, 3837, 99725, 105562, 52801, 3837, 103957, 105231, 115623, 67279, 3837, 88653, 117633, 115623, 1773, 105839, 104797, 100624, 99259, 3837, 87267, 33108, 100102, 99259, 101063, 1773, 99306, 14777, 92015, 104006, 99406, 3837, 112575, 102347, 102441, 99193, 3837, 104797, 9370, 100102, 99259, 70927, 99308, 80158, 99873, 100585, 34187, 101099, 3407, 11622, 104823, 106413, 100192, 3837, 100347, 30709, 36885, 55502, 5373, 105566, 101930, 16530, 100939, 39762, 3837, 100001, 101368, 104029, 114316, 64355, 116066, 1773, 109661, 111335, 49828, 100001, 116771, 3837, 99558, 99519, 100102, 99259, 18493, 31914, 20742, 100601, 36885, 3407, 77288, 87256, 101997, 3837, 117405, 16872, 100626, 34794, 100743, 3837, 43288, 87267, 106015, 20412, 105172, 64355, 116066, 1773, 104544, 106141, 101174, 105806, 3837, 110090, 115623, 101561, 70927, 117083, 1773, 101893, 99559, 104823, 15946, 104854, 105792, 53556, 123, 116066, 100631, 100102, 116066, 3837, 74763, 104560, 109851, 99559, 3407, 49567, 100158, 3837, 117405, 101913, 34794, 100743, 33108, 102010, 49185, 99696, 118329, 101160, 99461, 100403, 26939, 117405, 16872, 3837, 100346, 99520, 66394, 102410, 20412, 88653, 25074, 57191, 121813, 120201, 11319, 100001, 101419, 38953, 53481, 107200, 57191, 117591, 9370, 101120, 101304, 3837, 104050, 18830, 32108, 115623, 16530, 100939, 39762, 3837, 99518, 101894, 102257, 57191, 34794, 100743, 102072, 3407, 104857, 105839, 3837, 35946, 99494, 100681, 100001, 101368, 33126, 111892, 121813, 120201, 105437, 11319, 101150, 106350, 104661, 106021, 33108, 104797, 106806, 105419, 33071, 100741, 3837, 100102, 99259, 104560, 35568, 62112, 3837, 77288, 87267, 103943, 79599, 99586, 100631, 110776, 100102, 105998, 101742, 99337, 3407, 102112, 99797, 111492, 53481, 108734, 3837, 100137, 101930, 16530, 100939, 39762, 99518, 106888, 104215, 3837, 62244, 100374, 104823, 33126, 99835, 29258, 9370, 99252, 13072, 3837, 104074, 102410, 20412, 102031, 33126, 113879, 9370, 101304, 26850, 87256, 101118, 100158, 3837, 43288, 99730, 99520, 112507, 121813, 120201, 3837, 49828, 104857, 101042, 117405, 49185, 99696, 62926, 100347, 34794, 100743, 101893, 101120, 101368, 1773, 104823, 69249, 64355, 3837, 101893, 101107, 87267, 33126, 101137, 14009, 101782, 116066, 527, 57191, 14009, 64355, 120471, 527, 1773, 100001, 99252, 13072, 102119, 53481, 107200, 101120, 101304, 104813, 101776, 102581, 33108, 99877, 100250, 99561, 3407, 101997, 105419, 33108, 104661, 108372, 3837, 104797, 99518, 100102, 99518, 99259, 3837, 47815, 102494, 104892, 99873, 17254, 107200, 3837, 111309, 99899, 99932, 9370, 102778, 72448, 115781, 104036, 1773, 64355, 120471, 99487, 99252, 13072, 115371, 101228, 99322, 39762, 3837, 108884, 53481, 9370, 101304, 101120, 3837, 101776, 102581, 26939, 100347, 34794, 100743, 3407, 100632, 3837, 104857, 115104, 33447, 99879, 3837, 100626, 18947, 99252, 13072, 101994, 104652, 100968, 3837, 105792, 14009, 121617, 120657, 119740, 527, 3837, 43288, 99252, 18493, 104823, 69249, 95411, 63367, 65101, 100137, 101120, 101304, 62926, 115563, 99194, 32948, 34794, 100743, 102072, 1773, 99652, 74763, 114369, 32108, 115623, 33108, 102010, 49185, 99696, 100001, 101368, 3407, 104170, 3837, 75882, 99670, 20412, 106084, 100102, 99259, 3837, 100673, 100102, 99586, 110540, 3837, 104661, 108372, 53153, 101515, 3837, 41146, 106141, 99185, 12857, 101893, 101304, 11319, 99799, 101042, 33447, 104288, 14009, 121617, 120657, 119740, 527, 99487, 99252, 13072, 101228, 102084, 101137, 1773, 151645, 198, 151644, 77091, 198, 151667, 198, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 8997, 151668, 271, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 1773, 151645, 198]\n",
      "inputs:\n",
      "<|im_start|>user\n",
      "æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ è¯·é€æ­¥æ¨ç†å¹¶ç»™å‡ºç­”æ¡ˆã€‚\n",
      "è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\n",
      "\n",
      "ç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\n",
      "\n",
      "ä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\n",
      "\n",
      "ç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\n",
      "\n",
      "ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\n",
      "\n",
      "å›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\n",
      "\n",
      "å†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\n",
      "\n",
      "çœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\n",
      "\n",
      "å“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
      "</think>\n",
      "\n",
      "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<|im_end|>\n",
      "\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 198, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 8997, 151668, 271, 45181, 104823, 106413, 100192, 3837, 56568, 31838, 53481, 111492, 101137, 2073, 121617, 120657, 119740, 97907, 116067, 1773, 100137, 116067, 102119, 108132, 117405, 3837, 110966, 42140, 44290, 36885, 55502, 3837, 101776, 99577, 88653, 115623, 3837, 101894, 34794, 100743, 3837, 99811, 44290, 102010, 49185, 99696, 100136, 101930, 16530, 100939, 39762, 1773, 100102, 99259, 99260, 29258, 9370, 106084, 108478, 100673, 100137, 116067, 103949, 3837, 111625, 112575, 99260, 99932, 9370, 101939, 103996, 1773, 101898, 100374, 104823, 9370, 79766, 99259, 49238, 99586, 5373, 103458, 100102, 32320, 101561, 9370, 114464, 71817, 54542, 90395, 102035, 104715, 100182, 101898, 71817, 100700, 105262, 33108, 101899, 1773, 151645, 198]\n",
      "labels:\n",
      "<think>\n",
      "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
      "</think>\n",
      "\n",
      "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚<|im_end|>\n",
      "\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:05:56,156 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:05:56,157 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|2025-12-04 19:05:56] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-12-04 19:05:56] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "[INFO|modeling_utils.py:1148] 2025-12-04 19:05:57,751 >> loading weights file /mnt/workspace/Qwen3-8B/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2241] 2025-12-04 19:05:57,752 >> Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1135] 2025-12-04 19:05:57,753 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:10<00:00,  2.15s/it]\n",
      "[INFO|modeling_utils.py:5131] 2025-12-04 19:06:08,665 >> All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:5139] 2025-12-04 19:06:08,665 >> All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at /mnt/workspace/Qwen3-8B.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1088] 2025-12-04 19:06:08,669 >> loading configuration file /mnt/workspace/Qwen3-8B/generation_config.json\n",
      "[INFO|configuration_utils.py:1135] 2025-12-04 19:06:08,669 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "[INFO|2025-12-04 19:06:08] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-12-04 19:06:08] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-12-04 19:06:08] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-12-04 19:06:08] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-12-04 19:06:08] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,down_proj,o_proj,k_proj,v_proj,gate_proj,up_proj\n",
      "[INFO|2025-12-04 19:06:09] llamafactory.model.loader:143 >> trainable params: 21,823,488 || all params: 8,212,558,848 || trainable%: 0.2657\n",
      "[INFO|trainer.py:756] 2025-12-04 19:06:09,247 >> Using auto half precision backend\n",
      "[INFO|2025-12-04 19:06:09] llamafactory.train.trainer_utils:143 >> Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "[INFO|trainer.py:2409] 2025-12-04 19:06:09,529 >> ***** Running training *****\n",
      "[INFO|trainer.py:2410] 2025-12-04 19:06:09,529 >>   Num examples = 850\n",
      "[INFO|trainer.py:2411] 2025-12-04 19:06:09,529 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:2412] 2025-12-04 19:06:09,529 >>   Instantaneous batch size per device = 1\n",
      "[INFO|trainer.py:2415] 2025-12-04 19:06:09,529 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2416] 2025-12-04 19:06:09,529 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2417] 2025-12-04 19:06:09,529 >>   Total optimization steps = 639\n",
      "[INFO|trainer.py:2418] 2025-12-04 19:06:09,535 >>   Number of trainable parameters = 21,823,488\n",
      "  1%|â–                                          | 5/639 [00:20<42:31,  4.02s/it][INFO|2025-12-04 19:06:30] llamafactory.train.callbacks:143 >> {'loss': 0.6960, 'learning_rate': 9.9990e-05, 'epoch': 0.02, 'throughput': 697.13}\n",
      "{'loss': 0.696, 'grad_norm': 0.23502613604068756, 'learning_rate': 9.999033183566353e-05, 'epoch': 0.02, 'num_input_tokens_seen': 14376, 'train_runtime': 20.6278, 'train_tokens_per_second': 696.923}\n",
      "  2%|â–‹                                         | 10/639 [00:40<42:08,  4.02s/it][INFO|2025-12-04 19:06:50] llamafactory.train.callbacks:143 >> {'loss': 0.5438, 'learning_rate': 9.9951e-05, 'epoch': 0.05, 'throughput': 699.14}\n",
      "{'loss': 0.5438, 'grad_norm': 0.3318701684474945, 'learning_rate': 9.995106132599869e-05, 'epoch': 0.05, 'num_input_tokens_seen': 28296, 'train_runtime': 40.4784, 'train_tokens_per_second': 699.039}\n",
      "  2%|â–‰                                         | 15/639 [01:04<48:16,  4.64s/it][INFO|2025-12-04 19:07:13] llamafactory.train.callbacks:143 >> {'loss': 0.5772, 'learning_rate': 9.9882e-05, 'epoch': 0.07, 'throughput': 701.83}\n",
      "{'loss': 0.5772, 'grad_norm': 0.2540542483329773, 'learning_rate': 9.988160792165562e-05, 'epoch': 0.07, 'num_input_tokens_seen': 44992, 'train_runtime': 64.1129, 'train_tokens_per_second': 701.762}\n",
      "  3%|â–ˆâ–                                        | 20/639 [01:25<43:45,  4.24s/it][INFO|2025-12-04 19:07:34] llamafactory.train.callbacks:143 >> {'loss': 0.6002, 'learning_rate': 9.9782e-05, 'epoch': 0.09, 'throughput': 700.39}\n",
      "{'loss': 0.6002, 'grad_norm': 0.38846877217292786, 'learning_rate': 9.978201358980645e-05, 'epoch': 0.09, 'num_input_tokens_seen': 59576, 'train_runtime': 85.0673, 'train_tokens_per_second': 700.34}\n",
      "  4%|â–ˆâ–‹                                        | 25/639 [01:46<42:56,  4.20s/it][INFO|2025-12-04 19:07:55] llamafactory.train.callbacks:143 >> {'loss': 0.5981, 'learning_rate': 9.9652e-05, 'epoch': 0.12, 'throughput': 698.84}\n",
      "{'loss': 0.5981, 'grad_norm': 0.3376992642879486, 'learning_rate': 9.965233851025814e-05, 'epoch': 0.12, 'num_input_tokens_seen': 74240, 'train_runtime': 106.2394, 'train_tokens_per_second': 698.799}\n",
      "  5%|â–ˆâ–‰                                        | 30/639 [02:08<44:40,  4.40s/it][INFO|2025-12-04 19:08:18] llamafactory.train.callbacks:143 >> {'loss': 0.6136, 'learning_rate': 9.9493e-05, 'epoch': 0.14, 'throughput': 698.80}\n",
      "{'loss': 0.6136, 'grad_norm': 0.23448772728443146, 'learning_rate': 9.949266103908895e-05, 'epoch': 0.14, 'num_input_tokens_seen': 89944, 'train_runtime': 128.7181, 'train_tokens_per_second': 698.767}\n",
      "  5%|â–ˆâ–ˆâ–                                       | 35/639 [02:30<43:42,  4.34s/it][INFO|2025-12-04 19:08:40] llamafactory.train.callbacks:143 >> {'loss': 0.6039, 'learning_rate': 9.9303e-05, 'epoch': 0.16, 'throughput': 698.91}\n",
      "{'loss': 0.6039, 'grad_norm': 0.25201863050460815, 'learning_rate': 9.930307766130169e-05, 'epoch': 0.16, 'num_input_tokens_seen': 105288, 'train_runtime': 150.6526, 'train_tokens_per_second': 698.879}\n",
      "  6%|â–ˆâ–ˆâ–‹                                       | 40/639 [02:51<41:29,  4.16s/it][INFO|2025-12-04 19:09:00] llamafactory.train.callbacks:143 >> {'loss': 0.5387, 'learning_rate': 9.9084e-05, 'epoch': 0.19, 'throughput': 697.32}\n",
      "{'loss': 0.5387, 'grad_norm': 0.33039793372154236, 'learning_rate': 9.90837029325229e-05, 'epoch': 0.19, 'num_input_tokens_seen': 119432, 'train_runtime': 171.2783, 'train_tokens_per_second': 697.298}\n",
      "  7%|â–ˆâ–ˆâ–‰                                       | 45/639 [03:10<38:25,  3.88s/it][INFO|2025-12-04 19:09:19] llamafactory.train.callbacks:143 >> {'loss': 0.5764, 'learning_rate': 9.8835e-05, 'epoch': 0.21, 'throughput': 696.40}\n",
      "{'loss': 0.5764, 'grad_norm': 0.2545849680900574, 'learning_rate': 9.883466940978252e-05, 'epoch': 0.21, 'num_input_tokens_seen': 132576, 'train_runtime': 190.3782, 'train_tokens_per_second': 696.382}\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 50/639 [03:29<39:13,  4.00s/it][INFO|2025-12-04 19:09:39] llamafactory.train.callbacks:143 >> {'loss': 0.5287, 'learning_rate': 9.8556e-05, 'epoch': 0.24, 'throughput': 695.89}\n",
      "{'loss': 0.5287, 'grad_norm': 0.28507527709007263, 'learning_rate': 9.855612757141655e-05, 'epoch': 0.24, 'num_input_tokens_seen': 146112, 'train_runtime': 209.9695, 'train_tokens_per_second': 695.872}\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–Œ                                      | 55/639 [03:52<43:40,  4.49s/it][INFO|2025-12-04 19:10:02] llamafactory.train.callbacks:143 >> {'loss': 0.5662, 'learning_rate': 9.8248e-05, 'epoch': 0.26, 'throughput': 695.63}\n",
      "{'loss': 0.5662, 'grad_norm': 0.3804457485675812, 'learning_rate': 9.824824572614051e-05, 'epoch': 0.26, 'num_input_tokens_seen': 162048, 'train_runtime': 232.9568, 'train_tokens_per_second': 695.614}\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 60/639 [04:13<40:41,  4.22s/it][INFO|2025-12-04 19:10:23] llamafactory.train.callbacks:143 >> {'loss': 0.6070, 'learning_rate': 9.7911e-05, 'epoch': 0.28, 'throughput': 693.66}\n",
      "{'loss': 0.607, 'grad_norm': 0.2477499544620514, 'learning_rate': 9.791120991134904e-05, 'epoch': 0.28, 'num_input_tokens_seen': 176064, 'train_runtime': 253.8233, 'train_tokens_per_second': 693.648}\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 65/639 [04:34<39:16,  4.11s/it][INFO|2025-12-04 19:10:43] llamafactory.train.callbacks:143 >> {'loss': 0.5673, 'learning_rate': 9.7545e-05, 'epoch': 0.31, 'throughput': 692.77}\n",
      "{'loss': 0.5673, 'grad_norm': 0.2974146902561188, 'learning_rate': 9.754522378070297e-05, 'epoch': 0.31, 'num_input_tokens_seen': 190008, 'train_runtime': 274.2796, 'train_tokens_per_second': 692.753}\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                     | 70/639 [04:53<36:35,  3.86s/it][INFO|2025-12-04 19:11:02] llamafactory.train.callbacks:143 >> {'loss': 0.5041, 'learning_rate': 9.7151e-05, 'epoch': 0.33, 'throughput': 691.84}\n",
      "{'loss': 0.5041, 'grad_norm': 0.35126590728759766, 'learning_rate': 9.715050848107168e-05, 'epoch': 0.33, 'num_input_tokens_seen': 203000, 'train_runtime': 293.4279, 'train_tokens_per_second': 691.822}\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰                                     | 75/639 [05:15<39:34,  4.21s/it][INFO|2025-12-04 19:11:24] llamafactory.train.callbacks:143 >> {'loss': 0.5521, 'learning_rate': 9.6727e-05, 'epoch': 0.35, 'throughput': 691.33}\n",
      "{'loss': 0.5521, 'grad_norm': 0.9560633897781372, 'learning_rate': 9.67273025189053e-05, 'epoch': 0.35, 'num_input_tokens_seen': 217824, 'train_runtime': 315.0869, 'train_tokens_per_second': 691.314}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 80/639 [05:35<39:50,  4.28s/it][INFO|2025-12-04 19:11:45] llamafactory.train.callbacks:143 >> {'loss': 0.5734, 'learning_rate': 9.6276e-05, 'epoch': 0.38, 'throughput': 690.69}\n",
      "{'loss': 0.5734, 'grad_norm': 0.5834539532661438, 'learning_rate': 9.627586161611732e-05, 'epoch': 0.38, 'num_input_tokens_seen': 231792, 'train_runtime': 335.5991, 'train_tokens_per_second': 690.681}\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 85/639 [05:56<39:50,  4.32s/it][INFO|2025-12-04 19:12:06] llamafactory.train.callbacks:143 >> {'loss': 0.5538, 'learning_rate': 9.5796e-05, 'epoch': 0.40, 'throughput': 689.84}\n",
      "{'loss': 0.5538, 'grad_norm': 0.30696722865104675, 'learning_rate': 9.57964585555648e-05, 'epoch': 0.4, 'num_input_tokens_seen': 245992, 'train_runtime': 356.6004, 'train_tokens_per_second': 689.825}\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 90/639 [06:17<37:45,  4.13s/it][INFO|2025-12-04 19:12:27] llamafactory.train.callbacks:143 >> {'loss': 0.5719, 'learning_rate': 9.5289e-05, 'epoch': 0.42, 'throughput': 689.06}\n",
      "{'loss': 0.5719, 'grad_norm': 0.2754083275794983, 'learning_rate': 9.528938301621956e-05, 'epoch': 0.42, 'num_input_tokens_seen': 260376, 'train_runtime': 377.8787, 'train_tokens_per_second': 689.046}\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 95/639 [06:39<39:08,  4.32s/it][INFO|2025-12-04 19:12:48] llamafactory.train.callbacks:143 >> {'loss': 0.5814, 'learning_rate': 9.4755e-05, 'epoch': 0.45, 'throughput': 688.40}\n",
      "{'loss': 0.5814, 'grad_norm': 0.2978734076023102, 'learning_rate': 9.475494139812979e-05, 'epoch': 0.45, 'num_input_tokens_seen': 274824, 'train_runtime': 399.2287, 'train_tokens_per_second': 688.387}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [07:00<38:17,  4.26s/it][INFO|2025-12-04 19:13:09] llamafactory.train.callbacks:143 >> {'loss': 0.6039, 'learning_rate': 9.4193e-05, 'epoch': 0.47, 'throughput': 688.69}\n",
      "{'loss': 0.6039, 'grad_norm': 0.5732434988021851, 'learning_rate': 9.419345663727805e-05, 'epoch': 0.47, 'num_input_tokens_seen': 289392, 'train_runtime': 420.2147, 'train_tokens_per_second': 688.677}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [07:00<38:17,  4.26s/it][INFO|trainer.py:4327] 2025-12-04 19:13:09,755 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-12-04 19:13:09,755 >>   Num examples = 150\n",
      "[INFO|trainer.py:4332] 2025-12-04 19:13:09,755 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.62it/s]\u001b[A\n",
      "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.32it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 4/150 [00:00<00:37,  3.94it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.72it/s]\u001b[A\n",
      "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.60it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.51it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.45it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.21it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.09it/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.17it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.96it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.07it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:42,  3.14it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.20it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:42,  3.09it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.41it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.36it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.37it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.71it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.22it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.10it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.28it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:14<00:31,  3.29it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.46it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.35it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.16it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.30it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.58it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.02it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.01it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.79it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.76it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.30it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.62it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.59it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.52it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.57it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.50it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.36it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.02it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.94it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.90it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.34it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.33it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.20it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.23it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.17it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.34it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.28it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.33it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.33it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.63it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.06it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.19it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.24it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.05it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.36it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.54it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.30it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.30it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.36it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.42it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.40it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.98it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.19it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.17it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.22it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.61it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.09it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5516486763954163, 'eval_runtime': 45.2801, 'eval_samples_per_second': 3.313, 'eval_steps_per_second': 3.313, 'epoch': 0.47, 'num_input_tokens_seen': 289392}\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 100/639 [07:45<38:17,  4.26s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:45<00:00,  3.98it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-04 19:13:55,032 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:13:55,079 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:13:55,080 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:13:55,198 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:13:55,199 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:13:55,199 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-100/special_tokens_map.json\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 105/639 [08:06<1:05:46,  7.39s/it][INFO|2025-12-04 19:14:16] llamafactory.train.callbacks:143 >> {'loss': 0.5618, 'learning_rate': 9.3605e-05, 'epoch': 0.49, 'throughput': 623.96}\n",
      "{'loss': 0.5618, 'grad_norm': 0.3641050159931183, 'learning_rate': 9.360526801044752e-05, 'epoch': 0.49, 'num_input_tokens_seen': 303552, 'train_runtime': 486.5013, 'train_tokens_per_second': 623.949}\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 110/639 [08:25<39:32,  4.48s/it][INFO|2025-12-04 19:14:35] llamafactory.train.callbacks:143 >> {'loss': 0.5425, 'learning_rate': 9.2991e-05, 'epoch': 0.52, 'throughput': 625.38}\n",
      "{'loss': 0.5425, 'grad_norm': 0.48988020420074463, 'learning_rate': 9.299073093021405e-05, 'epoch': 0.52, 'num_input_tokens_seen': 316176, 'train_runtime': 505.5794, 'train_tokens_per_second': 625.374}\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 115/639 [08:46<37:11,  4.26s/it][INFO|2025-12-04 19:14:55] llamafactory.train.callbacks:143 >> {'loss': 0.5570, 'learning_rate': 9.2350e-05, 'epoch': 0.54, 'throughput': 627.53}\n",
      "{'loss': 0.557, 'grad_norm': 0.3276370167732239, 'learning_rate': 9.235021673018849e-05, 'epoch': 0.54, 'num_input_tokens_seen': 330344, 'train_runtime': 526.4242, 'train_tokens_per_second': 627.524}\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 120/639 [09:08<39:27,  4.56s/it][INFO|2025-12-04 19:15:17] llamafactory.train.callbacks:143 >> {'loss': 0.5373, 'learning_rate': 9.1684e-05, 'epoch': 0.56, 'throughput': 630.29}\n",
      "{'loss': 0.5373, 'grad_norm': 0.2878979444503784, 'learning_rate': 9.168411244063863e-05, 'epoch': 0.56, 'num_input_tokens_seen': 345608, 'train_runtime': 548.3389, 'train_tokens_per_second': 630.282}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 125/639 [09:28<35:34,  4.15s/it][INFO|2025-12-04 19:15:38] llamafactory.train.callbacks:143 >> {'loss': 0.5990, 'learning_rate': 9.0993e-05, 'epoch': 0.59, 'throughput': 631.77}\n",
      "{'loss': 0.599, 'grad_norm': 0.47103503346443176, 'learning_rate': 9.09928205546263e-05, 'epoch': 0.59, 'num_input_tokens_seen': 359168, 'train_runtime': 568.5138, 'train_tokens_per_second': 631.767}\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 130/639 [09:52<41:07,  4.85s/it][INFO|2025-12-04 19:16:02] llamafactory.train.callbacks:143 >> {'loss': 0.6046, 'learning_rate': 9.0277e-05, 'epoch': 0.61, 'throughput': 634.20}\n",
      "{'loss': 0.6046, 'grad_norm': 0.37291550636291504, 'learning_rate': 9.027675878480131e-05, 'epoch': 0.61, 'num_input_tokens_seen': 375824, 'train_runtime': 592.6039, 'train_tokens_per_second': 634.191}\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                | 135/639 [10:13<35:24,  4.22s/it][INFO|2025-12-04 19:16:22] llamafactory.train.callbacks:143 >> {'loss': 0.5704, 'learning_rate': 8.9536e-05, 'epoch': 0.64, 'throughput': 635.79}\n",
      "{'loss': 0.5704, 'grad_norm': 0.4764707386493683, 'learning_rate': 8.953635981099887e-05, 'epoch': 0.64, 'num_input_tokens_seen': 389832, 'train_runtime': 613.1557, 'train_tokens_per_second': 635.78}\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 140/639 [10:34<35:26,  4.26s/it][INFO|2025-12-04 19:16:43] llamafactory.train.callbacks:143 >> {'loss': 0.5996, 'learning_rate': 8.8772e-05, 'epoch': 0.66, 'throughput': 637.31}\n",
      "{'loss': 0.5996, 'grad_norm': 0.41063907742500305, 'learning_rate': 8.877207101879302e-05, 'epoch': 0.66, 'num_input_tokens_seen': 404320, 'train_runtime': 634.4235, 'train_tokens_per_second': 637.303}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 145/639 [10:55<35:56,  4.37s/it][INFO|2025-12-04 19:17:05] llamafactory.train.callbacks:143 >> {'loss': 0.5650, 'learning_rate': 8.7984e-05, 'epoch': 0.68, 'throughput': 639.12}\n",
      "{'loss': 0.565, 'grad_norm': 0.28647473454475403, 'learning_rate': 8.798435422916425e-05, 'epoch': 0.68, 'num_input_tokens_seen': 419128, 'train_runtime': 655.7903, 'train_tokens_per_second': 639.119}\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                               | 150/639 [11:17<35:15,  4.33s/it][INFO|2025-12-04 19:17:27] llamafactory.train.callbacks:143 >> {'loss': 0.5706, 'learning_rate': 8.7174e-05, 'epoch': 0.71, 'throughput': 640.55}\n",
      "{'loss': 0.5706, 'grad_norm': 0.35413092374801636, 'learning_rate': 8.717368541944452e-05, 'epoch': 0.71, 'num_input_tokens_seen': 434248, 'train_runtime': 677.9333, 'train_tokens_per_second': 640.547}\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 155/639 [11:39<34:42,  4.30s/it][INFO|2025-12-04 19:17:48] llamafactory.train.callbacks:143 >> {'loss': 0.5786, 'learning_rate': 8.6341e-05, 'epoch': 0.73, 'throughput': 642.12}\n",
      "{'loss': 0.5786, 'grad_norm': 0.3721427023410797, 'learning_rate': 8.634055443570826e-05, 'epoch': 0.73, 'num_input_tokens_seen': 449104, 'train_runtime': 699.4179, 'train_tokens_per_second': 642.111}\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 160/639 [12:00<33:18,  4.17s/it][INFO|2025-12-04 19:18:10] llamafactory.train.callbacks:143 >> {'loss': 0.5315, 'learning_rate': 8.5485e-05, 'epoch': 0.75, 'throughput': 643.01}\n",
      "{'loss': 0.5315, 'grad_norm': 0.43377748131752014, 'learning_rate': 8.548546469678311e-05, 'epoch': 0.75, 'num_input_tokens_seen': 463328, 'train_runtime': 720.5617, 'train_tokens_per_second': 643.009}\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                              | 165/639 [12:20<31:17,  3.96s/it][INFO|2025-12-04 19:18:29] llamafactory.train.callbacks:143 >> {'loss': 0.5618, 'learning_rate': 8.4609e-05, 'epoch': 0.78, 'throughput': 644.04}\n",
      "{'loss': 0.5618, 'grad_norm': 0.40435293316841125, 'learning_rate': 8.460893289005965e-05, 'epoch': 0.78, 'num_input_tokens_seen': 476752, 'train_runtime': 740.2637, 'train_tokens_per_second': 644.03}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                              | 170/639 [12:41<31:33,  4.04s/it][INFO|2025-12-04 19:18:50] llamafactory.train.callbacks:143 >> {'loss': 0.6172, 'learning_rate': 8.3711e-05, 'epoch': 0.80, 'throughput': 644.94}\n",
      "{'loss': 0.6172, 'grad_norm': 0.755048394203186, 'learning_rate': 8.371148865928319e-05, 'epoch': 0.8, 'num_input_tokens_seen': 490888, 'train_runtime': 761.1456, 'train_tokens_per_second': 644.933}\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 175/639 [13:01<30:56,  4.00s/it][INFO|2025-12-04 19:19:11] llamafactory.train.callbacks:143 >> {'loss': 0.5638, 'learning_rate': 8.2794e-05, 'epoch': 0.82, 'throughput': 645.91}\n",
      "{'loss': 0.5638, 'grad_norm': 0.42955467104911804, 'learning_rate': 8.279367428451702e-05, 'epoch': 0.82, 'num_input_tokens_seen': 504944, 'train_runtime': 781.7652, 'train_tokens_per_second': 645.902}\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 180/639 [13:23<33:39,  4.40s/it][INFO|2025-12-04 19:19:32] llamafactory.train.callbacks:143 >> {'loss': 0.5859, 'learning_rate': 8.1856e-05, 'epoch': 0.85, 'throughput': 646.72}\n",
      "{'loss': 0.5859, 'grad_norm': 0.35513269901275635, 'learning_rate': 8.185604435447002e-05, 'epoch': 0.85, 'num_input_tokens_seen': 519432, 'train_runtime': 803.182, 'train_tokens_per_second': 646.718}\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 185/639 [13:43<31:32,  4.17s/it][INFO|2025-12-04 19:19:53] llamafactory.train.callbacks:143 >> {'loss': 0.5548, 'learning_rate': 8.0899e-05, 'epoch': 0.87, 'throughput': 647.38}\n",
      "{'loss': 0.5548, 'grad_norm': 0.4681668281555176, 'learning_rate': 8.089916543138681e-05, 'epoch': 0.87, 'num_input_tokens_seen': 533328, 'train_runtime': 823.8372, 'train_tokens_per_second': 647.371}\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 190/639 [14:04<31:26,  4.20s/it][INFO|2025-12-04 19:20:14] llamafactory.train.callbacks:143 >> {'loss': 0.5770, 'learning_rate': 7.9924e-05, 'epoch': 0.89, 'throughput': 648.45}\n",
      "{'loss': 0.577, 'grad_norm': 0.4571211636066437, 'learning_rate': 7.992361570870288e-05, 'epoch': 0.89, 'num_input_tokens_seen': 547712, 'train_runtime': 844.6484, 'train_tokens_per_second': 648.45}\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                            | 195/639 [14:25<31:07,  4.21s/it][INFO|2025-12-04 19:20:35] llamafactory.train.callbacks:143 >> {'loss': 0.5777, 'learning_rate': 7.8930e-05, 'epoch': 0.92, 'throughput': 649.10}\n",
      "{'loss': 0.5777, 'grad_norm': 0.34551718831062317, 'learning_rate': 7.892998466167165e-05, 'epoch': 0.92, 'num_input_tokens_seen': 562024, 'train_runtime': 865.8566, 'train_tokens_per_second': 649.096}\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [14:46<30:36,  4.18s/it][INFO|2025-12-04 19:20:56] llamafactory.train.callbacks:143 >> {'loss': 0.6595, 'learning_rate': 7.7919e-05, 'epoch': 0.94, 'throughput': 649.77}\n",
      "{'loss': 0.6595, 'grad_norm': 0.4461055099964142, 'learning_rate': 7.791887269117442e-05, 'epoch': 0.94, 'num_input_tokens_seen': 576232, 'train_runtime': 886.8286, 'train_tokens_per_second': 649.767}\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [14:46<30:36,  4.18s/it][INFO|trainer.py:4327] 2025-12-04 19:20:56,369 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-12-04 19:20:56,369 >>   Num examples = 150\n",
      "[INFO|trainer.py:4332] 2025-12-04 19:20:56,369 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.63it/s]\u001b[A\n",
      "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.33it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 4/150 [00:00<00:37,  3.94it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.72it/s]\u001b[A\n",
      "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.60it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.52it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.10it/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.98it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.09it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:41,  3.15it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.22it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:41,  3.10it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.42it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.72it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.39it/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.39it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.37it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.49it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.14it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.72it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.55it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.22it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.10it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.29it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.36it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.38it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.30it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.46it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.34it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.44it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.15it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.30it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.58it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.91it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.46it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.00it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.79it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.97it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:20,  3.75it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.28it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.61it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.59it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.53it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.58it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.49it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.35it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.01it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.94it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.91it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.25it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.35it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.34it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.15it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.40it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.16it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.34it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.77it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.19it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.22it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.27it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.31it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.15it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.32it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.63it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.05it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.23it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.08it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.04it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.22it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.37it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.46it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:07,  3.43it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.54it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.31it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.37it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.42it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.16it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.05it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.09it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5446581840515137, 'eval_runtime': 45.2784, 'eval_samples_per_second': 3.313, 'eval_steps_per_second': 3.313, 'epoch': 0.94, 'num_input_tokens_seen': 576232}\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                            | 200/639 [15:32<30:36,  4.18s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:45<00:00,  3.98it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-04 19:21:41,644 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:21:41,692 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:21:41,693 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:21:41,777 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:21:41,778 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:21:41,778 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-200/special_tokens_map.json\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 205/639 [15:53<53:19,  7.37s/it][INFO|2025-12-04 19:22:03] llamafactory.train.callbacks:143 >> {'loss': 0.5866, 'learning_rate': 7.6891e-05, 'epoch': 0.96, 'throughput': 619.48}\n",
      "{'loss': 0.5866, 'grad_norm': 0.5498479604721069, 'learning_rate': 7.68908907609285e-05, 'epoch': 0.96, 'num_input_tokens_seen': 590944, 'train_runtime': 953.943, 'train_tokens_per_second': 619.475}\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 210/639 [16:15<34:05,  4.77s/it][INFO|2025-12-04 19:22:24] llamafactory.train.callbacks:143 >> {'loss': 0.5886, 'learning_rate': 7.5847e-05, 'epoch': 0.99, 'throughput': 620.87}\n",
      "{'loss': 0.5886, 'grad_norm': 0.42110908031463623, 'learning_rate': 7.584666002831296e-05, 'epoch': 0.99, 'num_input_tokens_seen': 605408, 'train_runtime': 975.0987, 'train_tokens_per_second': 620.868}\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 215/639 [16:33<27:56,  3.95s/it][INFO|2025-12-04 19:22:42] llamafactory.train.callbacks:143 >> {'loss': 0.4728, 'learning_rate': 7.4787e-05, 'epoch': 1.01, 'throughput': 621.69}\n",
      "{'loss': 0.4728, 'grad_norm': 0.49210885167121887, 'learning_rate': 7.478681146903448e-05, 'epoch': 1.01, 'num_input_tokens_seen': 617520, 'train_runtime': 993.292, 'train_tokens_per_second': 621.69}\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 220/639 [16:55<30:12,  4.33s/it][INFO|2025-12-04 19:23:05] llamafactory.train.callbacks:143 >> {'loss': 0.4468, 'learning_rate': 7.3712e-05, 'epoch': 1.03, 'throughput': 623.23}\n",
      "{'loss': 0.4468, 'grad_norm': 0.36256247758865356, 'learning_rate': 7.371198549586091e-05, 'epoch': 1.03, 'num_input_tokens_seen': 633016, 'train_runtime': 1015.7033, 'train_tokens_per_second': 623.229}\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 225/639 [17:17<30:13,  4.38s/it][INFO|2025-12-04 19:23:27] llamafactory.train.callbacks:143 >> {'loss': 0.3993, 'learning_rate': 7.2623e-05, 'epoch': 1.06, 'throughput': 624.65}\n",
      "{'loss': 0.3993, 'grad_norm': 0.43740350008010864, 'learning_rate': 7.262283157165219e-05, 'epoch': 1.06, 'num_input_tokens_seen': 648120, 'train_runtime': 1037.5816, 'train_tokens_per_second': 624.645}\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 230/639 [17:38<28:09,  4.13s/it][INFO|2025-12-04 19:23:47] llamafactory.train.callbacks:143 >> {'loss': 0.4697, 'learning_rate': 7.1520e-05, 'epoch': 1.08, 'throughput': 625.75}\n",
      "{'loss': 0.4697, 'grad_norm': 0.402011513710022, 'learning_rate': 7.152000781692286e-05, 'epoch': 1.08, 'num_input_tokens_seen': 662304, 'train_runtime': 1058.4262, 'train_tokens_per_second': 625.744}\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 235/639 [18:00<30:19,  4.50s/it][INFO|2025-12-04 19:24:10] llamafactory.train.callbacks:143 >> {'loss': 0.4310, 'learning_rate': 7.0404e-05, 'epoch': 1.10, 'throughput': 627.18}\n",
      "{'loss': 0.431, 'grad_norm': 0.4554682970046997, 'learning_rate': 7.040418061217325e-05, 'epoch': 1.1, 'num_input_tokens_seen': 677896, 'train_runtime': 1080.8739, 'train_tokens_per_second': 627.174}\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 240/639 [18:22<28:59,  4.36s/it][INFO|2025-12-04 19:24:31] llamafactory.train.callbacks:143 >> {'loss': 0.4295, 'learning_rate': 6.9276e-05, 'epoch': 1.13, 'throughput': 628.28}\n",
      "{'loss': 0.4295, 'grad_norm': 0.4741499423980713, 'learning_rate': 6.927602419522947e-05, 'epoch': 1.13, 'num_input_tokens_seen': 692648, 'train_runtime': 1102.4599, 'train_tokens_per_second': 628.275}\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                         | 245/639 [18:42<26:38,  4.06s/it][INFO|2025-12-04 19:24:52] llamafactory.train.callbacks:143 >> {'loss': 0.4233, 'learning_rate': 6.8136e-05, 'epoch': 1.15, 'throughput': 629.20}\n",
      "{'loss': 0.4233, 'grad_norm': 0.4026746451854706, 'learning_rate': 6.813622025383565e-05, 'epoch': 1.15, 'num_input_tokens_seen': 706472, 'train_runtime': 1122.8107, 'train_tokens_per_second': 629.2}\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 250/639 [19:02<25:34,  3.95s/it][INFO|2025-12-04 19:25:12] llamafactory.train.callbacks:143 >> {'loss': 0.3867, 'learning_rate': 6.6985e-05, 'epoch': 1.17, 'throughput': 630.00}\n",
      "{'loss': 0.3867, 'grad_norm': 0.48627060651779175, 'learning_rate': 6.698545751374465e-05, 'epoch': 1.17, 'num_input_tokens_seen': 719920, 'train_runtime': 1142.7429, 'train_tokens_per_second': 629.993}\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 255/639 [19:22<24:42,  3.86s/it][INFO|2025-12-04 19:25:31] llamafactory.train.callbacks:143 >> {'loss': 0.3709, 'learning_rate': 6.5824e-05, 'epoch': 1.20, 'throughput': 630.53}\n",
      "{'loss': 0.3709, 'grad_norm': 0.7616415023803711, 'learning_rate': 6.582443132255592e-05, 'epoch': 1.2, 'num_input_tokens_seen': 732888, 'train_runtime': 1162.3342, 'train_tokens_per_second': 630.531}\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 260/639 [19:42<24:54,  3.94s/it][INFO|2025-12-04 19:25:52] llamafactory.train.callbacks:143 >> {'loss': 0.3777, 'learning_rate': 6.4654e-05, 'epoch': 1.22, 'throughput': 631.43}\n",
      "{'loss': 0.3777, 'grad_norm': 0.6388319730758667, 'learning_rate': 6.465384322955224e-05, 'epoch': 1.22, 'num_input_tokens_seen': 746904, 'train_runtime': 1182.8759, 'train_tokens_per_second': 631.431}\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 265/639 [20:04<26:48,  4.30s/it][INFO|2025-12-04 19:26:14] llamafactory.train.callbacks:143 >> {'loss': 0.3592, 'learning_rate': 6.3474e-05, 'epoch': 1.24, 'throughput': 632.61}\n",
      "{'loss': 0.3592, 'grad_norm': 0.4034753441810608, 'learning_rate': 6.347440056178904e-05, 'epoch': 1.24, 'num_input_tokens_seen': 762040, 'train_runtime': 1204.6071, 'train_tokens_per_second': 632.605}\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 270/639 [20:26<26:59,  4.39s/it][INFO|2025-12-04 19:26:35] llamafactory.train.callbacks:143 >> {'loss': 0.4180, 'learning_rate': 6.2287e-05, 'epoch': 1.27, 'throughput': 633.40}\n",
      "{'loss': 0.418, 'grad_norm': 0.383634477853775, 'learning_rate': 6.228681599669248e-05, 'epoch': 1.27, 'num_input_tokens_seen': 776704, 'train_runtime': 1226.251, 'train_tokens_per_second': 633.397}\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                       | 275/639 [20:45<23:33,  3.88s/it][INFO|2025-12-04 19:26:55] llamafactory.train.callbacks:143 >> {'loss': 0.3786, 'learning_rate': 6.1092e-05, 'epoch': 1.29, 'throughput': 634.00}\n",
      "{'loss': 0.3786, 'grad_norm': 0.5070011019706726, 'learning_rate': 6.109180713142465e-05, 'epoch': 1.29, 'num_input_tokens_seen': 789840, 'train_runtime': 1245.8012, 'train_tokens_per_second': 634.002}\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                       | 280/639 [21:06<24:55,  4.16s/it][INFO|2025-12-04 19:27:16] llamafactory.train.callbacks:143 >> {'loss': 0.3856, 'learning_rate': 5.9890e-05, 'epoch': 1.32, 'throughput': 635.08}\n",
      "{'loss': 0.3856, 'grad_norm': 0.4170735478401184, 'learning_rate': 5.989009604927587e-05, 'epoch': 1.32, 'num_input_tokens_seen': 804600, 'train_runtime': 1266.9238, 'train_tokens_per_second': 635.082}\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 285/639 [21:28<24:45,  4.20s/it][INFO|2025-12-04 19:27:37] llamafactory.train.callbacks:143 >> {'loss': 0.3766, 'learning_rate': 5.8682e-05, 'epoch': 1.34, 'throughput': 635.84}\n",
      "{'loss': 0.3766, 'grad_norm': 0.5109259486198425, 'learning_rate': 5.868240888334653e-05, 'epoch': 1.34, 'num_input_tokens_seen': 818976, 'train_runtime': 1288.0334, 'train_tokens_per_second': 635.834}\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 290/639 [21:50<26:38,  4.58s/it][INFO|2025-12-04 19:28:00] llamafactory.train.callbacks:143 >> {'loss': 0.4212, 'learning_rate': 5.7469e-05, 'epoch': 1.36, 'throughput': 636.65}\n",
      "{'loss': 0.4212, 'grad_norm': 0.4675813913345337, 'learning_rate': 5.74694753777815e-05, 'epoch': 1.36, 'num_input_tokens_seen': 834520, 'train_runtime': 1310.8044, 'train_tokens_per_second': 636.647}\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 295/639 [22:12<25:00,  4.36s/it][INFO|2025-12-04 19:28:22] llamafactory.train.callbacks:143 >> {'loss': 0.4031, 'learning_rate': 5.6252e-05, 'epoch': 1.39, 'throughput': 637.57}\n",
      "{'loss': 0.4031, 'grad_norm': 0.4627811312675476, 'learning_rate': 5.62520284468228e-05, 'epoch': 1.39, 'num_input_tokens_seen': 849544, 'train_runtime': 1332.479, 'train_tokens_per_second': 637.567}\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [22:34<25:17,  4.48s/it][INFO|2025-12-04 19:28:44] llamafactory.train.callbacks:143 >> {'loss': 0.4076, 'learning_rate': 5.5031e-05, 'epoch': 1.41, 'throughput': 638.29}\n",
      "{'loss': 0.4076, 'grad_norm': 0.4208119213581085, 'learning_rate': 5.5030803731946665e-05, 'epoch': 1.41, 'num_input_tokens_seen': 864576, 'train_runtime': 1354.5348, 'train_tokens_per_second': 638.283}\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [22:34<25:17,  4.48s/it][INFO|trainer.py:4327] 2025-12-04 19:28:44,075 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-12-04 19:28:44,075 >>   Num examples = 150\n",
      "[INFO|trainer.py:4332] 2025-12-04 19:28:44,075 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.59it/s]\u001b[A\n",
      "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.30it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 4/150 [00:00<00:36,  3.96it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.75it/s]\u001b[A\n",
      "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.62it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.53it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.56it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.11it/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.97it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:42,  3.14it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.21it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:42,  3.09it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.41it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.39it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.36it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.24it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.72it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.23it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.29it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.30it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.41it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.47it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.05it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.35it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.16it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:23,  3.59it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.53it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.36it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.47it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.00it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.80it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.75it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.30it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.63it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.60it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.53it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.59it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:17,  3.84it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.49it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.28it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.36it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.01it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.04it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.93it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.91it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.25it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.35it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.34it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.15it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.13it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.89it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.16it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.33it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.60it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.29it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.33it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.34it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.64it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.36it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.05it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.24it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.05it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.36it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.62it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.54it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.31it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.37it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.42it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.17it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.14it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.09it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.5768356323242188, 'eval_runtime': 45.2451, 'eval_samples_per_second': 3.315, 'eval_steps_per_second': 3.315, 'epoch': 1.41, 'num_input_tokens_seen': 864576}\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 300/639 [23:19<25:17,  4.48s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.97it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-04 19:29:29,317 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:29:29,365 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:29:29,366 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:29:29,450 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:29:29,451 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:29:29,451 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-300/special_tokens_map.json\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                     | 305/639 [23:41<42:16,  7.59s/it][INFO|2025-12-04 19:29:50] llamafactory.train.callbacks:143 >> {'loss': 0.3732, 'learning_rate': 5.3807e-05, 'epoch': 1.43, 'throughput': 618.50}\n",
      "{'loss': 0.3732, 'grad_norm': 0.5276778340339661, 'learning_rate': 5.380653915735272e-05, 'epoch': 1.43, 'num_input_tokens_seen': 879120, 'train_runtime': 1421.3896, 'train_tokens_per_second': 618.493}\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 310/639 [24:01<26:25,  4.82s/it][INFO|2025-12-04 19:30:11] llamafactory.train.callbacks:143 >> {'loss': 0.3614, 'learning_rate': 5.2580e-05, 'epoch': 1.46, 'throughput': 619.33}\n",
      "{'loss': 0.3614, 'grad_norm': 0.4933299124240875, 'learning_rate': 5.2579974484073655e-05, 'epoch': 1.46, 'num_input_tokens_seen': 892880, 'train_runtime': 1441.6816, 'train_tokens_per_second': 619.332}\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 315/639 [24:22<23:23,  4.33s/it][INFO|2025-12-04 19:30:32] llamafactory.train.callbacks:143 >> {'loss': 0.3661, 'learning_rate': 5.1352e-05, 'epoch': 1.48, 'throughput': 620.15}\n",
      "{'loss': 0.3661, 'grad_norm': 0.5306238532066345, 'learning_rate': 5.1351850862975315e-05, 'epoch': 1.48, 'num_input_tokens_seen': 906960, 'train_runtime': 1462.4961, 'train_tokens_per_second': 620.145}\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 320/639 [24:41<20:42,  3.89s/it][INFO|2025-12-04 19:30:51] llamafactory.train.callbacks:143 >> {'loss': 0.3770, 'learning_rate': 5.0123e-05, 'epoch': 1.50, 'throughput': 620.90}\n",
      "{'loss': 0.377, 'grad_norm': 0.5807981491088867, 'learning_rate': 5.0122910386916656e-05, 'epoch': 1.5, 'num_input_tokens_seen': 920144, 'train_runtime': 1481.9612, 'train_tokens_per_second': 620.896}\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 325/639 [25:02<21:51,  4.18s/it][INFO|2025-12-04 19:31:12] llamafactory.train.callbacks:143 >> {'loss': 0.3885, 'learning_rate': 4.8894e-05, 'epoch': 1.53, 'throughput': 621.83}\n",
      "{'loss': 0.3885, 'grad_norm': 0.6108599901199341, 'learning_rate': 4.889389564234066e-05, 'epoch': 1.53, 'num_input_tokens_seen': 934536, 'train_runtime': 1502.8902, 'train_tokens_per_second': 621.826}\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 330/639 [25:23<22:08,  4.30s/it][INFO|2025-12-04 19:31:33] llamafactory.train.callbacks:143 >> {'loss': 0.4047, 'learning_rate': 4.7666e-05, 'epoch': 1.55, 'throughput': 622.72}\n",
      "{'loss': 0.4047, 'grad_norm': 0.4433024525642395, 'learning_rate': 4.766554926056707e-05, 'epoch': 1.55, 'num_input_tokens_seen': 948784, 'train_runtime': 1523.6097, 'train_tokens_per_second': 622.721}\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 335/639 [25:44<20:33,  4.06s/it][INFO|2025-12-04 19:31:53] llamafactory.train.callbacks:143 >> {'loss': 0.3847, 'learning_rate': 4.6439e-05, 'epoch': 1.57, 'throughput': 623.36}\n",
      "{'loss': 0.3847, 'grad_norm': 0.4564903974533081, 'learning_rate': 4.643861346905781e-05, 'epoch': 1.57, 'num_input_tokens_seen': 962552, 'train_runtime': 1544.1407, 'train_tokens_per_second': 623.358}\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 340/639 [26:05<20:51,  4.18s/it][INFO|2025-12-04 19:32:14] llamafactory.train.callbacks:143 >> {'loss': 0.4056, 'learning_rate': 4.5214e-05, 'epoch': 1.60, 'throughput': 624.08}\n",
      "{'loss': 0.4056, 'grad_norm': 0.5551087856292725, 'learning_rate': 4.521382964292663e-05, 'epoch': 1.6, 'num_input_tokens_seen': 976848, 'train_runtime': 1565.2702, 'train_tokens_per_second': 624.076}\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 345/639 [26:28<22:02,  4.50s/it][INFO|2025-12-04 19:32:38] llamafactory.train.callbacks:143 >> {'loss': 0.4301, 'learning_rate': 4.3992e-05, 'epoch': 1.62, 'throughput': 624.96}\n",
      "{'loss': 0.4301, 'grad_norm': 0.40703701972961426, 'learning_rate': 4.399193785696366e-05, 'epoch': 1.62, 'num_input_tokens_seen': 992824, 'train_runtime': 1588.6224, 'train_tokens_per_second': 624.959}\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 350/639 [26:48<19:13,  3.99s/it][INFO|2025-12-04 19:32:57] llamafactory.train.callbacks:143 >> {'loss': 0.3942, 'learning_rate': 4.2774e-05, 'epoch': 1.64, 'throughput': 625.40}\n",
      "{'loss': 0.3942, 'grad_norm': 0.6319884657859802, 'learning_rate': 4.277367643844574e-05, 'epoch': 1.64, 'num_input_tokens_seen': 1005672, 'train_runtime': 1608.055, 'train_tokens_per_second': 625.397}\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 355/639 [27:09<20:05,  4.24s/it][INFO|2025-12-04 19:33:18] llamafactory.train.callbacks:143 >> {'loss': 0.4638, 'learning_rate': 4.1560e-05, 'epoch': 1.67, 'throughput': 626.10}\n",
      "{'loss': 0.4638, 'grad_norm': 0.5046573281288147, 'learning_rate': 4.1559781521002664e-05, 'epoch': 1.67, 'num_input_tokens_seen': 1020032, 'train_runtime': 1629.198, 'train_tokens_per_second': 626.095}\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 360/639 [27:29<18:43,  4.03s/it][INFO|2025-12-04 19:33:38] llamafactory.train.callbacks:143 >> {'loss': 0.3601, 'learning_rate': 4.0351e-05, 'epoch': 1.69, 'throughput': 626.74}\n",
      "{'loss': 0.3601, 'grad_norm': 0.44419023394584656, 'learning_rate': 4.035098659980891e-05, 'epoch': 1.69, 'num_input_tokens_seen': 1033576, 'train_runtime': 1649.1295, 'train_tokens_per_second': 626.74}\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 365/639 [27:51<19:06,  4.18s/it][INFO|2025-12-04 19:34:00] llamafactory.train.callbacks:143 >> {'loss': 0.4354, 'learning_rate': 3.9148e-05, 'epoch': 1.72, 'throughput': 627.66}\n",
      "{'loss': 0.4354, 'grad_norm': 0.5245075225830078, 'learning_rate': 3.914802208836973e-05, 'epoch': 1.72, 'num_input_tokens_seen': 1048952, 'train_runtime': 1671.2104, 'train_tokens_per_second': 627.66}\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 370/639 [28:12<19:29,  4.35s/it][INFO|2025-12-04 19:34:22] llamafactory.train.callbacks:143 >> {'loss': 0.3863, 'learning_rate': 3.7952e-05, 'epoch': 1.74, 'throughput': 628.56}\n",
      "{'loss': 0.3863, 'grad_norm': 0.5564009547233582, 'learning_rate': 3.7951614877169284e-05, 'epoch': 1.74, 'num_input_tokens_seen': 1063992, 'train_runtime': 1692.7572, 'train_tokens_per_second': 628.556}\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 375/639 [28:35<20:16,  4.61s/it][INFO|2025-12-04 19:34:45] llamafactory.train.callbacks:143 >> {'loss': 0.3883, 'learning_rate': 3.6762e-05, 'epoch': 1.76, 'throughput': 629.44}\n",
      "{'loss': 0.3883, 'grad_norm': 0.3564496338367462, 'learning_rate': 3.67624878944475e-05, 'epoch': 1.76, 'num_input_tokens_seen': 1080056, 'train_runtime': 1715.9085, 'train_tokens_per_second': 629.437}\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 380/639 [28:57<19:05,  4.42s/it][INFO|2025-12-04 19:35:07] llamafactory.train.callbacks:143 >> {'loss': 0.4027, 'learning_rate': 3.5581e-05, 'epoch': 1.79, 'throughput': 630.13}\n",
      "{'loss': 0.4027, 'grad_norm': 0.5695515871047974, 'learning_rate': 3.558135966937123e-05, 'epoch': 1.79, 'num_input_tokens_seen': 1095056, 'train_runtime': 1737.8423, 'train_tokens_per_second': 630.124}\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 385/639 [29:19<18:08,  4.29s/it][INFO|2025-12-04 19:35:28] llamafactory.train.callbacks:143 >> {'loss': 0.3612, 'learning_rate': 3.4409e-05, 'epoch': 1.81, 'throughput': 630.77}\n",
      "{'loss': 0.3612, 'grad_norm': 0.45643237233161926, 'learning_rate': 3.440894389786352e-05, 'epoch': 1.81, 'num_input_tokens_seen': 1109552, 'train_runtime': 1759.0607, 'train_tokens_per_second': 630.764}\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 390/639 [29:39<16:53,  4.07s/it][INFO|2025-12-04 19:35:49] llamafactory.train.callbacks:143 >> {'loss': 0.3676, 'learning_rate': 3.3246e-05, 'epoch': 1.83, 'throughput': 631.35}\n",
      "{'loss': 0.3676, 'grad_norm': 0.6372836828231812, 'learning_rate': 3.3245949011353264e-05, 'epoch': 1.83, 'num_input_tokens_seen': 1123480, 'train_runtime': 1779.4816, 'train_tokens_per_second': 631.352}\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 395/639 [29:59<16:25,  4.04s/it][INFO|2025-12-04 19:36:09] llamafactory.train.callbacks:143 >> {'loss': 0.4137, 'learning_rate': 3.2093e-05, 'epoch': 1.86, 'throughput': 632.08}\n",
      "{'loss': 0.4137, 'grad_norm': 0.5252777934074402, 'learning_rate': 3.209307774870603e-05, 'epoch': 1.86, 'num_input_tokens_seen': 1137664, 'train_runtime': 1799.8748, 'train_tokens_per_second': 632.08}\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [30:20<16:37,  4.17s/it][INFO|2025-12-04 19:36:30] llamafactory.train.callbacks:143 >> {'loss': 0.3426, 'learning_rate': 3.0951e-05, 'epoch': 1.88, 'throughput': 632.60}\n",
      "{'loss': 0.3426, 'grad_norm': 0.5461771488189697, 'learning_rate': 3.0951026731594635e-05, 'epoch': 1.88, 'num_input_tokens_seen': 1151792, 'train_runtime': 1820.7239, 'train_tokens_per_second': 632.601}\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [30:20<16:37,  4.17s/it][INFO|trainer.py:4327] 2025-12-04 19:36:30,264 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-12-04 19:36:30,264 >>   Num examples = 150\n",
      "[INFO|trainer.py:4332] 2025-12-04 19:36:30,264 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.62it/s]\u001b[A\n",
      "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.30it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 4/150 [00:00<00:37,  3.93it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.74it/s]\u001b[A\n",
      "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.61it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.51it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.43it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.55it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.49it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.11it/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.98it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:42,  3.14it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.20it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:42,  3.09it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.41it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.36it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.47it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.37it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.71it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.23it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.28it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:14<00:31,  3.29it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.41it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.47it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.05it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.36it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.13it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.16it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:23,  3.59it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.53it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.36it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.47it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.73it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.00it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.80it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.97it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.75it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.29it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.62it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.59it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.53it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.59it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.50it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.28it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.35it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.01it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.01it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.95it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.91it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.34it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.34it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.20it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.17it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.34it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.60it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.59it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.29it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.33it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.34it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.64it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.36it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.07it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:09,  3.20it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.25it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.10it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.21it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.06it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.37it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.48it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.44it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.34it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.55it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.32it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.17it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.38it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.41it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.06it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.18it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.61it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.08it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.21it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.582048237323761, 'eval_runtime': 45.2294, 'eval_samples_per_second': 3.316, 'eval_steps_per_second': 3.316, 'epoch': 1.88, 'num_input_tokens_seen': 1151792}\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 400/639 [31:05<16:37,  4.17s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.98it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-04 19:37:15,490 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:37:15,538 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:37:15,538 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:37:15,623 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:37:15,623 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:37:15,623 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-400/special_tokens_map.json\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 405/639 [31:26<28:30,  7.31s/it][INFO|2025-12-04 19:37:35] llamafactory.train.callbacks:143 >> {'loss': 0.3577, 'learning_rate': 2.9820e-05, 'epoch': 1.90, 'throughput': 617.66}\n",
      "{'loss': 0.3577, 'grad_norm': 0.7871028184890747, 'learning_rate': 2.9820486043565854e-05, 'epoch': 1.9, 'num_input_tokens_seen': 1165048, 'train_runtime': 1886.2362, 'train_tokens_per_second': 617.658}\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 410/639 [31:47<17:55,  4.70s/it][INFO|2025-12-04 19:37:56] llamafactory.train.callbacks:143 >> {'loss': 0.3717, 'learning_rate': 2.8702e-05, 'epoch': 1.93, 'throughput': 618.36}\n",
      "{'loss': 0.3717, 'grad_norm': 0.5074209570884705, 'learning_rate': 2.870213881305802e-05, 'epoch': 1.93, 'num_input_tokens_seen': 1179328, 'train_runtime': 1907.2045, 'train_tokens_per_second': 618.354}\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 415/639 [32:08<16:37,  4.45s/it][INFO|2025-12-04 19:38:17] llamafactory.train.callbacks:143 >> {'loss': 0.4490, 'learning_rate': 2.7597e-05, 'epoch': 1.95, 'throughput': 618.92}\n",
      "{'loss': 0.449, 'grad_norm': 0.47436830401420593, 'learning_rate': 2.7596660800621078e-05, 'epoch': 1.95, 'num_input_tokens_seen': 1193472, 'train_runtime': 1928.3147, 'train_tokens_per_second': 618.92}\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 420/639 [32:28<15:05,  4.13s/it][INFO|2025-12-04 19:38:37] llamafactory.train.callbacks:143 >> {'loss': 0.3816, 'learning_rate': 2.6505e-05, 'epoch': 1.97, 'throughput': 619.43}\n",
      "{'loss': 0.3816, 'grad_norm': 0.5366332530975342, 'learning_rate': 2.650471999058875e-05, 'epoch': 1.97, 'num_input_tokens_seen': 1206920, 'train_runtime': 1948.436, 'train_tokens_per_second': 619.43}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 425/639 [32:50<14:34,  4.09s/it][INFO|2025-12-04 19:38:59] llamafactory.train.callbacks:143 >> {'loss': 0.3983, 'learning_rate': 2.5427e-05, 'epoch': 2.00, 'throughput': 620.33}\n",
      "{'loss': 0.3983, 'grad_norm': 0.5117286443710327, 'learning_rate': 2.542697618744945e-05, 'epoch': 2.0, 'num_input_tokens_seen': 1222080, 'train_runtime': 1970.0385, 'train_tokens_per_second': 620.333}\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 430/639 [33:07<13:01,  3.74s/it][INFO|2025-12-04 19:39:17] llamafactory.train.callbacks:143 >> {'loss': 0.2464, 'learning_rate': 2.4364e-05, 'epoch': 2.02, 'throughput': 620.99}\n",
      "{'loss': 0.2464, 'grad_norm': 0.8380260467529297, 'learning_rate': 2.4364080617159886e-05, 'epoch': 2.02, 'num_input_tokens_seen': 1234488, 'train_runtime': 1987.9502, 'train_tokens_per_second': 620.985}\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 435/639 [33:28<13:56,  4.10s/it][INFO|2025-12-04 19:39:38] llamafactory.train.callbacks:143 >> {'loss': 0.2237, 'learning_rate': 2.3317e-05, 'epoch': 2.04, 'throughput': 621.53}\n",
      "{'loss': 0.2237, 'grad_norm': 0.4559262990951538, 'learning_rate': 2.3316675533642214e-05, 'epoch': 2.04, 'num_input_tokens_seen': 1248328, 'train_runtime': 2008.4729, 'train_tokens_per_second': 621.531}\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 440/639 [33:50<14:28,  4.36s/it][INFO|2025-12-04 19:40:00] llamafactory.train.callbacks:143 >> {'loss': 0.2394, 'learning_rate': 2.2285e-05, 'epoch': 2.07, 'throughput': 622.10}\n",
      "{'loss': 0.2394, 'grad_norm': 0.5217824578285217, 'learning_rate': 2.22853938307025e-05, 'epoch': 2.07, 'num_input_tokens_seen': 1263224, 'train_runtime': 2030.5946, 'train_tokens_per_second': 622.096}\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ            | 445/639 [34:09<12:38,  3.91s/it][INFO|2025-12-04 19:40:18] llamafactory.train.callbacks:143 >> {'loss': 0.2701, 'learning_rate': 2.1271e-05, 'epoch': 2.09, 'throughput': 622.51}\n",
      "{'loss': 0.2701, 'grad_norm': 0.9445173144340515, 'learning_rate': 2.1270858659605158e-05, 'epoch': 2.09, 'num_input_tokens_seen': 1275792, 'train_runtime': 2049.4348, 'train_tokens_per_second': 622.509}\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 450/639 [34:28<12:30,  3.97s/it][INFO|2025-12-04 19:40:38] llamafactory.train.callbacks:143 >> {'loss': 0.2105, 'learning_rate': 2.0274e-05, 'epoch': 2.11, 'throughput': 622.88}\n",
      "{'loss': 0.2105, 'grad_norm': 0.5436469912528992, 'learning_rate': 2.0273683052534175e-05, 'epoch': 2.11, 'num_input_tokens_seen': 1288592, 'train_runtime': 2068.766, 'train_tokens_per_second': 622.88}\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 455/639 [34:50<12:50,  4.19s/it][INFO|2025-12-04 19:40:59] llamafactory.train.callbacks:143 >> {'loss': 0.2344, 'learning_rate': 1.9294e-05, 'epoch': 2.14, 'throughput': 623.59}\n",
      "{'loss': 0.2344, 'grad_norm': 0.6321210265159607, 'learning_rate': 1.9294469552168813e-05, 'epoch': 2.14, 'num_input_tokens_seen': 1303376, 'train_runtime': 2090.1165, 'train_tokens_per_second': 623.59}\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 460/639 [35:11<12:15,  4.11s/it][INFO|2025-12-04 19:41:20] llamafactory.train.callbacks:143 >> {'loss': 0.2572, 'learning_rate': 1.8334e-05, 'epoch': 2.16, 'throughput': 624.18}\n",
      "{'loss': 0.2572, 'grad_norm': 0.6212900876998901, 'learning_rate': 1.8333809847597642e-05, 'epoch': 2.16, 'num_input_tokens_seen': 1317856, 'train_runtime': 2111.3592, 'train_tokens_per_second': 624.174}\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 465/639 [35:32<12:09,  4.19s/it][INFO|2025-12-04 19:41:41] llamafactory.train.callbacks:143 >> {'loss': 0.2222, 'learning_rate': 1.7392e-05, 'epoch': 2.18, 'throughput': 624.64}\n",
      "{'loss': 0.2222, 'grad_norm': 0.8060994148254395, 'learning_rate': 1.739228441679081e-05, 'epoch': 2.18, 'num_input_tokens_seen': 1331792, 'train_runtime': 2132.1068, 'train_tokens_per_second': 624.637}\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 470/639 [35:53<12:13,  4.34s/it][INFO|2025-12-04 19:42:03] llamafactory.train.callbacks:143 >> {'loss': 0.2205, 'learning_rate': 1.6470e-05, 'epoch': 2.21, 'throughput': 625.32}\n",
      "{'loss': 0.2205, 'grad_norm': 0.4554629921913147, 'learning_rate': 1.647046217584661e-05, 'epoch': 2.21, 'num_input_tokens_seen': 1346640, 'train_runtime': 2153.5175, 'train_tokens_per_second': 625.321}\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 475/639 [36:13<11:07,  4.07s/it][INFO|2025-12-04 19:42:23] llamafactory.train.callbacks:143 >> {'loss': 0.2254, 'learning_rate': 1.5569e-05, 'epoch': 2.23, 'throughput': 625.93}\n",
      "{'loss': 0.2254, 'grad_norm': 0.6985599398612976, 'learning_rate': 1.556890013522428e-05, 'epoch': 2.23, 'num_input_tokens_seen': 1360720, 'train_runtime': 2173.9366, 'train_tokens_per_second': 625.924}\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 480/639 [36:36<11:25,  4.31s/it][INFO|2025-12-04 19:42:45] llamafactory.train.callbacks:143 >> {'loss': 0.2295, 'learning_rate': 1.4688e-05, 'epoch': 2.25, 'throughput': 626.58}\n",
      "{'loss': 0.2295, 'grad_norm': 0.6668505072593689, 'learning_rate': 1.4688143063170923e-05, 'epoch': 2.25, 'num_input_tokens_seen': 1375984, 'train_runtime': 2196.0303, 'train_tokens_per_second': 626.578}\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ          | 485/639 [36:57<10:36,  4.13s/it][INFO|2025-12-04 19:43:06] llamafactory.train.callbacks:143 >> {'loss': 0.2024, 'learning_rate': 1.3829e-05, 'epoch': 2.28, 'throughput': 627.21}\n",
      "{'loss': 0.2024, 'grad_norm': 0.5283108949661255, 'learning_rate': 1.3828723156545553e-05, 'epoch': 2.28, 'num_input_tokens_seen': 1390624, 'train_runtime': 2217.1625, 'train_tokens_per_second': 627.209}\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 490/639 [37:20<11:41,  4.71s/it][INFO|2025-12-04 19:43:30] llamafactory.train.callbacks:143 >> {'loss': 0.2335, 'learning_rate': 1.2991e-05, 'epoch': 2.30, 'throughput': 627.73}\n",
      "{'loss': 0.2335, 'grad_norm': 0.6252283453941345, 'learning_rate': 1.2991159719239582e-05, 'epoch': 2.3, 'num_input_tokens_seen': 1406672, 'train_runtime': 2240.8972, 'train_tokens_per_second': 627.727}\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š         | 495/639 [37:42<10:32,  4.39s/it][INFO|2025-12-04 19:43:52] llamafactory.train.callbacks:143 >> {'loss': 0.2429, 'learning_rate': 1.2176e-05, 'epoch': 2.32, 'throughput': 628.42}\n",
      "{'loss': 0.2429, 'grad_norm': 0.7144733667373657, 'learning_rate': 1.2175958848387765e-05, 'epoch': 2.32, 'num_input_tokens_seen': 1421880, 'train_runtime': 2262.6184, 'train_tokens_per_second': 628.422}\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [38:04<10:01,  4.33s/it][INFO|2025-12-04 19:44:13] llamafactory.train.callbacks:143 >> {'loss': 0.2311, 'learning_rate': 1.1384e-05, 'epoch': 2.35, 'throughput': 629.07}\n",
      "{'loss': 0.2311, 'grad_norm': 0.7061449289321899, 'learning_rate': 1.1383613128559306e-05, 'epoch': 2.35, 'num_input_tokens_seen': 1437056, 'train_runtime': 2284.4098, 'train_tokens_per_second': 629.071}\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [38:04<10:01,  4.33s/it][INFO|trainer.py:4327] 2025-12-04 19:44:13,950 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-12-04 19:44:13,950 >>   Num examples = 150\n",
      "[INFO|trainer.py:4332] 2025-12-04 19:44:13,950 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.63it/s]\u001b[A\n",
      "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.33it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 4/150 [00:00<00:36,  3.95it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.74it/s]\u001b[A\n",
      "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.61it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.52it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.56it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.50it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.47it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.22it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.11it/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.18it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.97it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:41,  3.15it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.22it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:41,  3.10it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.42it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.40it/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.37it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.49it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.71it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.25it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.55it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.22it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.11it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.17it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.28it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.35it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.29it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.29it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.47it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.25it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.35it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.46it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.12it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.05it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.15it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.29it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.31it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.58it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.03it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.02it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.80it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.76it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.30it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.62it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.58it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.52it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.57it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.82it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.48it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.49it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.35it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.02it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.04it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.00it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.93it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.90it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.34it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.33it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.16it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.33it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.77it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.21it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.25it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.28it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.32it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.17it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.33it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.64it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.06it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.25it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.05it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.36it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.20it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.35it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.55it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.30it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.37it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.43it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.41it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.99it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.20it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:41<00:03,  3.05it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.17it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.96it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.06it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.15it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.28it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.08it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6827391386032104, 'eval_runtime': 45.2589, 'eval_samples_per_second': 3.314, 'eval_steps_per_second': 3.314, 'epoch': 2.35, 'num_input_tokens_seen': 1437056}\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ         | 500/639 [38:49<10:01,  4.33s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.98it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-04 19:44:59,205 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:44:59,254 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:44:59,254 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:44:59,339 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:44:59,340 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:44:59,340 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-500/special_tokens_map.json\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 505/639 [39:11<16:56,  7.59s/it][INFO|2025-12-04 19:45:20] llamafactory.train.callbacks:143 >> {'loss': 0.2051, 'learning_rate': 1.0615e-05, 'epoch': 2.37, 'throughput': 617.34}\n",
      "{'loss': 0.2051, 'grad_norm': 0.7082416415214539, 'learning_rate': 1.0614601334114099e-05, 'epoch': 2.37, 'num_input_tokens_seen': 1451560, 'train_runtime': 2351.3321, 'train_tokens_per_second': 617.335}\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹        | 510/639 [39:32<10:29,  4.88s/it][INFO|2025-12-04 19:45:42] llamafactory.train.callbacks:143 >> {'loss': 0.2161, 'learning_rate': 9.8694e-06, 'epoch': 2.40, 'throughput': 617.87}\n",
      "{'loss': 0.2161, 'grad_norm': 0.5055545568466187, 'learning_rate': 9.869388139903496e-06, 'epoch': 2.4, 'num_input_tokens_seen': 1466016, 'train_runtime': 2372.7021, 'train_tokens_per_second': 617.868}\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 515/639 [39:51<08:21,  4.05s/it][INFO|2025-12-04 19:46:01] llamafactory.train.callbacks:143 >> {'loss': 0.1847, 'learning_rate': 9.1484e-06, 'epoch': 2.42, 'throughput': 618.32}\n",
      "{'loss': 0.1847, 'grad_norm': 0.6534410715103149, 'learning_rate': 9.148423840490954e-06, 'epoch': 2.42, 'num_input_tokens_seen': 1478944, 'train_runtime': 2391.883, 'train_tokens_per_second': 618.318}\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 520/639 [40:14<08:35,  4.33s/it][INFO|2025-12-04 19:46:23] llamafactory.train.callbacks:143 >> {'loss': 0.2416, 'learning_rate': 8.4521e-06, 'epoch': 2.44, 'throughput': 618.99}\n",
      "{'loss': 0.2416, 'grad_norm': 0.543830156326294, 'learning_rate': 8.452144078061818e-06, 'epoch': 2.44, 'num_input_tokens_seen': 1494488, 'train_runtime': 2414.415, 'train_tokens_per_second': 618.986}\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹       | 525/639 [40:35<08:14,  4.34s/it][INFO|2025-12-04 19:46:45] llamafactory.train.callbacks:143 >> {'loss': 0.2410, 'learning_rate': 7.7810e-06, 'epoch': 2.47, 'throughput': 619.55}\n",
      "{'loss': 0.241, 'grad_norm': 0.5543383359909058, 'learning_rate': 7.780969579186814e-06, 'epoch': 2.47, 'num_input_tokens_seen': 1509128, 'train_runtime': 2435.8485, 'train_tokens_per_second': 619.549}\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       | 530/639 [40:58<08:27,  4.66s/it][INFO|2025-12-04 19:47:07] llamafactory.train.callbacks:143 >> {'loss': 0.2784, 'learning_rate': 7.1353e-06, 'epoch': 2.49, 'throughput': 620.17}\n",
      "{'loss': 0.2784, 'grad_norm': 0.37712037563323975, 'learning_rate': 7.135305900598321e-06, 'epoch': 2.49, 'num_input_tokens_seen': 1524464, 'train_runtime': 2458.1382, 'train_tokens_per_second': 620.17}\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 535/639 [41:21<07:54,  4.57s/it][INFO|2025-12-04 19:47:30] llamafactory.train.callbacks:143 >> {'loss': 0.2794, 'learning_rate': 6.5155e-06, 'epoch': 2.51, 'throughput': 620.91}\n",
      "{'loss': 0.2794, 'grad_norm': 0.7365447878837585, 'learning_rate': 6.515543184132999e-06, 'epoch': 2.51, 'num_input_tokens_seen': 1540688, 'train_runtime': 2481.3489, 'train_tokens_per_second': 620.907}\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 540/639 [41:41<06:39,  4.04s/it][INFO|2025-12-04 19:47:50] llamafactory.train.callbacks:143 >> {'loss': 0.2341, 'learning_rate': 5.9221e-06, 'epoch': 2.54, 'throughput': 621.39}\n",
      "{'loss': 0.2341, 'grad_norm': 0.5706935524940491, 'learning_rate': 5.922055920988817e-06, 'epoch': 2.54, 'num_input_tokens_seen': 1554264, 'train_runtime': 2501.2651, 'train_tokens_per_second': 621.391}\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 545/639 [42:02<06:35,  4.21s/it][INFO|2025-12-04 19:48:11] llamafactory.train.callbacks:143 >> {'loss': 0.2178, 'learning_rate': 5.3552e-06, 'epoch': 2.56, 'throughput': 621.88}\n",
      "{'loss': 0.2178, 'grad_norm': 0.7282590270042419, 'learning_rate': 5.355202725439046e-06, 'epoch': 2.56, 'num_input_tokens_seen': 1568584, 'train_runtime': 2522.3372, 'train_tokens_per_second': 621.877}\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 550/639 [42:22<05:57,  4.02s/it][INFO|2025-12-04 19:48:32] llamafactory.train.callbacks:143 >> {'loss': 0.2050, 'learning_rate': 4.8153e-06, 'epoch': 2.58, 'throughput': 622.38}\n",
      "{'loss': 0.205, 'grad_norm': 0.8727412819862366, 'learning_rate': 4.8153261181398125e-06, 'epoch': 2.58, 'num_input_tokens_seen': 1582712, 'train_runtime': 2542.9866, 'train_tokens_per_second': 622.383}\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 555/639 [42:42<05:27,  3.90s/it][INFO|2025-12-04 19:48:52] llamafactory.train.callbacks:143 >> {'loss': 0.2161, 'learning_rate': 4.3028e-06, 'epoch': 2.61, 'throughput': 622.80}\n",
      "{'loss': 0.2161, 'grad_norm': 0.5880314111709595, 'learning_rate': 4.302752319162212e-06, 'epoch': 2.61, 'num_input_tokens_seen': 1596192, 'train_runtime': 2562.9466, 'train_tokens_per_second': 622.796}\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 560/639 [43:04<05:49,  4.42s/it][INFO|2025-12-04 19:49:14] llamafactory.train.callbacks:143 >> {'loss': 0.2147, 'learning_rate': 3.8178e-06, 'epoch': 2.63, 'throughput': 623.33}\n",
      "{'loss': 0.2147, 'grad_norm': 0.4431954324245453, 'learning_rate': 3.81779105087407e-06, 'epoch': 2.63, 'num_input_tokens_seen': 1611088, 'train_runtime': 2584.6615, 'train_tokens_per_second': 623.326}\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 565/639 [43:26<05:10,  4.20s/it][INFO|2025-12-04 19:49:35] llamafactory.train.callbacks:143 >> {'loss': 0.2420, 'learning_rate': 3.3607e-06, 'epoch': 2.65, 'throughput': 623.83}\n",
      "{'loss': 0.242, 'grad_norm': 0.5944216847419739, 'learning_rate': 3.3607353507904283e-06, 'epoch': 2.65, 'num_input_tokens_seen': 1625696, 'train_runtime': 2606.0146, 'train_tokens_per_second': 623.825}\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 570/639 [43:48<05:19,  4.64s/it][INFO|2025-12-04 19:49:58] llamafactory.train.callbacks:143 >> {'loss': 0.2359, 'learning_rate': 2.9319e-06, 'epoch': 2.68, 'throughput': 624.29}\n",
      "{'loss': 0.2359, 'grad_norm': 0.5675414204597473, 'learning_rate': 2.931861394505764e-06, 'epoch': 2.68, 'num_input_tokens_seen': 1641048, 'train_runtime': 2628.6622, 'train_tokens_per_second': 624.29}\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 575/639 [44:10<04:48,  4.51s/it][INFO|2025-12-04 19:50:20] llamafactory.train.callbacks:143 >> {'loss': 0.2082, 'learning_rate': 2.5314e-06, 'epoch': 2.70, 'throughput': 624.80}\n",
      "{'loss': 0.2082, 'grad_norm': 0.6842307448387146, 'learning_rate': 2.531428328815155e-06, 'epoch': 2.7, 'num_input_tokens_seen': 1656160, 'train_runtime': 2650.7123, 'train_tokens_per_second': 624.798}\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 580/639 [44:30<03:56,  4.01s/it][INFO|2025-12-04 19:50:39] llamafactory.train.callbacks:143 >> {'loss': 0.2095, 'learning_rate': 2.1597e-06, 'epoch': 2.72, 'throughput': 625.15}\n",
      "{'loss': 0.2095, 'grad_norm': 0.8588398098945618, 'learning_rate': 2.1596781151249524e-06, 'epoch': 2.72, 'num_input_tokens_seen': 1669320, 'train_runtime': 2670.2655, 'train_tokens_per_second': 625.151}\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 585/639 [44:51<03:37,  4.04s/it][INFO|2025-12-04 19:51:00] llamafactory.train.callbacks:143 >> {'loss': 0.2409, 'learning_rate': 1.8168e-06, 'epoch': 2.75, 'throughput': 625.59}\n",
      "{'loss': 0.2409, 'grad_norm': 0.777917206287384, 'learning_rate': 1.8168353832477947e-06, 'epoch': 2.75, 'num_input_tokens_seen': 1683616, 'train_runtime': 2691.2351, 'train_tokens_per_second': 625.592}\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 590/639 [45:13<03:31,  4.32s/it][INFO|2025-12-04 19:51:22] llamafactory.train.callbacks:143 >> {'loss': 0.2346, 'learning_rate': 1.5031e-06, 'epoch': 2.77, 'throughput': 626.06}\n",
      "{'loss': 0.2346, 'grad_norm': 0.6048610210418701, 'learning_rate': 1.5031072956701697e-06, 'epoch': 2.77, 'num_input_tokens_seen': 1698728, 'train_runtime': 2713.3782, 'train_tokens_per_second': 626.056}\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 595/639 [45:32<02:51,  3.90s/it][INFO|2025-12-04 19:51:41] llamafactory.train.callbacks:143 >> {'loss': 0.2282, 'learning_rate': 1.2187e-06, 'epoch': 2.80, 'throughput': 626.36}\n",
      "{'loss': 0.2282, 'grad_norm': 0.5462923049926758, 'learning_rate': 1.2186834223746612e-06, 'epoch': 2.8, 'num_input_tokens_seen': 1711320, 'train_runtime': 2732.1752, 'train_tokens_per_second': 626.358}\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [45:54<02:50,  4.37s/it][INFO|2025-12-04 19:52:03] llamafactory.train.callbacks:143 >> {'loss': 0.2684, 'learning_rate': 9.6374e-07, 'epoch': 2.82, 'throughput': 626.84}\n",
      "{'loss': 0.2684, 'grad_norm': 0.5676615238189697, 'learning_rate': 9.637356262923725e-07, 'epoch': 2.82, 'num_input_tokens_seen': 1726344, 'train_runtime': 2754.0608, 'train_tokens_per_second': 626.836}\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [45:54<02:50,  4.37s/it][INFO|trainer.py:4327] 2025-12-04 19:52:03,601 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-12-04 19:52:03,601 >>   Num examples = 150\n",
      "[INFO|trainer.py:4332] 2025-12-04 19:52:03,601 >>   Batch size = 1\n",
      "\n",
      "  0%|                                                   | 0/150 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|â–Œ                                          | 2/150 [00:00<00:19,  7.60it/s]\u001b[A\n",
      "  2%|â–Š                                          | 3/150 [00:00<00:27,  5.31it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 4/150 [00:00<00:36,  3.96it/s]\u001b[A\n",
      "  3%|â–ˆâ–                                         | 5/150 [00:01<00:38,  3.73it/s]\u001b[A\n",
      "  4%|â–ˆâ–‹                                         | 6/150 [00:01<00:39,  3.62it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                         | 7/150 [00:01<00:40,  3.52it/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆâ–                                        | 8/150 [00:02<00:41,  3.44it/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–Œ                                        | 9/150 [00:02<00:39,  3.56it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Š                                       | 10/150 [00:02<00:40,  3.50it/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–ˆ                                       | 11/150 [00:02<00:40,  3.46it/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆâ–                                      | 12/150 [00:03<00:40,  3.43it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‹                                      | 13/150 [00:03<00:42,  3.21it/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‰                                      | 14/150 [00:03<00:43,  3.10it/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 15/150 [00:04<00:42,  3.17it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                     | 16/150 [00:04<00:45,  2.97it/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                     | 17/150 [00:04<00:43,  3.08it/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                     | 18/150 [00:05<00:41,  3.15it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                    | 19/150 [00:05<00:40,  3.21it/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                    | 20/150 [00:05<00:41,  3.10it/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                    | 21/150 [00:06<00:37,  3.42it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 22/150 [00:06<00:34,  3.71it/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                   | 23/150 [00:06<00:37,  3.39it/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                   | 24/150 [00:06<00:37,  3.38it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                   | 25/150 [00:07<00:37,  3.37it/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 26/150 [00:07<00:35,  3.48it/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 27/150 [00:07<00:35,  3.50it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 28/150 [00:08<00:37,  3.28it/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 29/150 [00:08<00:38,  3.13it/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 30/150 [00:08<00:37,  3.23it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                                 | 31/150 [00:09<00:35,  3.38it/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                 | 32/150 [00:09<00:31,  3.72it/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 33/150 [00:09<00:35,  3.32it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 34/150 [00:09<00:33,  3.45it/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 35/150 [00:10<00:35,  3.24it/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 36/150 [00:10<00:32,  3.54it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 37/150 [00:10<00:35,  3.17it/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 38/150 [00:11<00:34,  3.21it/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 39/150 [00:11<00:35,  3.10it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 40/150 [00:11<00:34,  3.18it/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 41/150 [00:12<00:33,  3.29it/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 42/150 [00:12<00:32,  3.36it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 43/150 [00:12<00:31,  3.37it/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 44/150 [00:13<00:35,  2.96it/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 45/150 [00:13<00:33,  3.11it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 46/150 [00:13<00:35,  2.91it/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 47/150 [00:13<00:31,  3.30it/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 48/150 [00:14<00:31,  3.28it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 49/150 [00:14<00:29,  3.39it/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                            | 50/150 [00:14<00:29,  3.42it/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 51/150 [00:15<00:28,  3.46it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                           | 52/150 [00:15<00:32,  3.04it/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                           | 53/150 [00:15<00:29,  3.24it/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 54/150 [00:16<00:28,  3.33it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 55/150 [00:16<00:27,  3.43it/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                          | 56/150 [00:16<00:30,  3.12it/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                          | 57/150 [00:17<00:30,  3.03it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 58/150 [00:17<00:29,  3.15it/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 59/150 [00:17<00:27,  3.28it/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 60/150 [00:17<00:27,  3.30it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 61/150 [00:18<00:27,  3.30it/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 62/150 [00:18<00:26,  3.30it/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 63/150 [00:18<00:26,  3.30it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 64/150 [00:19<00:24,  3.57it/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 65/150 [00:19<00:24,  3.52it/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 66/150 [00:19<00:25,  3.26it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 67/150 [00:20<00:28,  2.90it/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 68/150 [00:20<00:27,  3.02it/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 69/150 [00:20<00:24,  3.35it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 70/150 [00:20<00:23,  3.45it/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                      | 71/150 [00:21<00:21,  3.72it/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 72/150 [00:21<00:19,  4.01it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 73/150 [00:21<00:20,  3.79it/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 74/150 [00:21<00:19,  3.98it/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                     | 75/150 [00:22<00:19,  3.76it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 76/150 [00:22<00:22,  3.29it/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                    | 77/150 [00:22<00:20,  3.60it/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                    | 78/150 [00:23<00:20,  3.58it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 79/150 [00:23<00:20,  3.52it/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 80/150 [00:23<00:19,  3.57it/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                   | 81/150 [00:23<00:18,  3.83it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                   | 82/150 [00:24<00:19,  3.49it/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 83/150 [00:24<00:19,  3.50it/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                  | 84/150 [00:24<00:20,  3.27it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                  | 85/150 [00:25<00:19,  3.36it/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                  | 86/150 [00:25<00:21,  3.02it/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 87/150 [00:25<00:21,  2.93it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 88/150 [00:26<00:20,  3.05it/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 89/150 [00:26<00:21,  2.78it/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 90/150 [00:26<00:19,  3.00it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 91/150 [00:27<00:20,  2.93it/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 92/150 [00:27<00:19,  2.90it/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 93/150 [00:27<00:17,  3.24it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 94/150 [00:28<00:16,  3.33it/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 95/150 [00:28<00:16,  3.33it/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 96/150 [00:28<00:17,  3.14it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 97/150 [00:29<00:16,  3.21it/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 98/150 [00:29<00:16,  3.24it/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 99/150 [00:29<00:16,  3.12it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 100/150 [00:30<00:17,  2.88it/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 101/150 [00:30<00:16,  3.06it/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 102/150 [00:30<00:14,  3.39it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 103/150 [00:31<00:15,  3.09it/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 104/150 [00:31<00:14,  3.17it/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 105/150 [00:31<00:13,  3.33it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 106/150 [00:31<00:11,  3.78it/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 107/150 [00:32<00:11,  3.59it/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ           | 108/150 [00:32<00:11,  3.58it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 109/150 [00:32<00:12,  3.20it/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 110/150 [00:33<00:12,  3.23it/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 111/150 [00:33<00:11,  3.27it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 112/150 [00:33<00:11,  3.31it/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 113/150 [00:34<00:11,  3.16it/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 114/150 [00:34<00:10,  3.32it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 115/150 [00:34<00:09,  3.63it/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 116/150 [00:34<00:10,  3.35it/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰         | 117/150 [00:35<00:10,  3.05it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 118/150 [00:35<00:10,  3.18it/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 119/150 [00:35<00:09,  3.25it/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 120/150 [00:36<00:09,  3.09it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 121/150 [00:36<00:09,  3.20it/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 122/150 [00:36<00:09,  3.06it/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ       | 123/150 [00:37<00:08,  3.23it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 124/150 [00:37<00:07,  3.37it/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 125/150 [00:37<00:07,  3.47it/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 126/150 [00:37<00:06,  3.43it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹      | 127/150 [00:38<00:07,  3.19it/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰      | 128/150 [00:38<00:06,  3.33it/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 129/150 [00:38<00:05,  3.63it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 130/150 [00:39<00:05,  3.55it/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 131/150 [00:39<00:05,  3.30it/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 132/150 [00:39<00:05,  3.16it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 133/150 [00:40<00:05,  3.31it/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 134/150 [00:40<00:04,  3.36it/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 135/150 [00:40<00:04,  3.41it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 136/150 [00:40<00:04,  3.41it/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 137/150 [00:41<00:04,  2.97it/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 138/150 [00:41<00:03,  3.19it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 139/150 [00:42<00:03,  3.04it/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 140/150 [00:42<00:03,  3.16it/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 141/150 [00:42<00:03,  2.95it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 142/150 [00:42<00:02,  3.05it/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 143/150 [00:43<00:02,  3.14it/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 144/150 [00:43<00:01,  3.21it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 145/150 [00:43<00:01,  3.27it/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 146/150 [00:44<00:01,  3.60it/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 147/150 [00:44<00:00,  3.67it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 148/150 [00:44<00:00,  4.08it/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 149/150 [00:44<00:00,  4.20it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 0.6894978284835815, 'eval_runtime': 45.301, 'eval_samples_per_second': 3.311, 'eval_steps_per_second': 3.311, 'epoch': 2.82, 'num_input_tokens_seen': 1726344}\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 600/639 [46:39<02:50,  4.37s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:45<00:00,  3.98it/s]\u001b[A\n",
      "                                                                                \u001b[A[INFO|trainer.py:3993] 2025-12-04 19:52:48,899 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:52:48,948 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:52:48,949 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:52:49,046 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:52:49,047 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:52:49,047 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-600/special_tokens_map.json\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 605/639 [47:00<04:06,  7.24s/it][INFO|2025-12-04 19:53:09] llamafactory.train.callbacks:143 >> {'loss': 0.2441, 'learning_rate': 7.3842e-07, 'epoch': 2.84, 'throughput': 617.14}\n",
      "{'loss': 0.2441, 'grad_norm': 0.8173639178276062, 'learning_rate': 7.384179594548957e-07, 'epoch': 2.84, 'num_input_tokens_seen': 1740472, 'train_runtime': 2820.2348, 'train_tokens_per_second': 617.137}\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 610/639 [47:20<02:12,  4.56s/it][INFO|2025-12-04 19:53:30] llamafactory.train.callbacks:143 >> {'loss': 0.2168, 'learning_rate': 5.4287e-07, 'epoch': 2.87, 'throughput': 617.50}\n",
      "{'loss': 0.2168, 'grad_norm': 0.5683459639549255, 'learning_rate': 5.428665699084789e-07, 'epoch': 2.87, 'num_input_tokens_seen': 1754136, 'train_runtime': 2840.7246, 'train_tokens_per_second': 617.496}\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 615/639 [47:42<01:46,  4.44s/it][INFO|2025-12-04 19:53:51] llamafactory.train.callbacks:143 >> {'loss': 0.2421, 'learning_rate': 3.7720e-07, 'epoch': 2.89, 'throughput': 618.01}\n",
      "{'loss': 0.2421, 'grad_norm': 0.5586677193641663, 'learning_rate': 3.7719961944664985e-07, 'epoch': 2.89, 'num_input_tokens_seen': 1768960, 'train_runtime': 2862.3719, 'train_tokens_per_second': 618.005}\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 620/639 [48:02<01:16,  4.05s/it][INFO|2025-12-04 19:54:12] llamafactory.train.callbacks:143 >> {'loss': 0.2027, 'learning_rate': 2.4152e-07, 'epoch': 2.91, 'throughput': 618.39}\n",
      "{'loss': 0.2027, 'grad_norm': 0.7121002674102783, 'learning_rate': 2.415172122110343e-07, 'epoch': 2.91, 'num_input_tokens_seen': 1782728, 'train_runtime': 2882.8828, 'train_tokens_per_second': 618.384}\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 625/639 [48:24<00:59,  4.24s/it][INFO|2025-12-04 19:54:34] llamafactory.train.callbacks:143 >> {'loss': 0.2514, 'learning_rate': 1.3590e-07, 'epoch': 2.94, 'throughput': 618.89}\n",
      "{'loss': 0.2514, 'grad_norm': 0.6133226752281189, 'learning_rate': 1.3590133420350315e-07, 'epoch': 2.94, 'num_input_tokens_seen': 1797592, 'train_runtime': 2904.5693, 'train_tokens_per_second': 618.884}\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 630/639 [48:45<00:38,  4.25s/it][INFO|2025-12-04 19:54:55] llamafactory.train.callbacks:143 >> {'loss': 0.2103, 'learning_rate': 6.0416e-08, 'epoch': 2.96, 'throughput': 619.38}\n",
      "{'loss': 0.2103, 'grad_norm': 0.6335474848747253, 'learning_rate': 6.041580374618328e-08, 'epoch': 2.96, 'num_input_tokens_seen': 1812104, 'train_runtime': 2925.6652, 'train_tokens_per_second': 619.382}\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 635/639 [49:05<00:15,  3.93s/it][INFO|2025-12-04 19:55:14] llamafactory.train.callbacks:143 >> {'loss': 0.2202, 'learning_rate': 1.5106e-08, 'epoch': 2.98, 'throughput': 619.76}\n",
      "{'loss': 0.2202, 'grad_norm': 0.7536317110061646, 'learning_rate': 1.5106232919276375e-08, 'epoch': 2.98, 'num_input_tokens_seen': 1825320, 'train_runtime': 2945.211, 'train_tokens_per_second': 619.759}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 639/639 [49:19<00:00,  3.48s/it][INFO|trainer.py:3993] 2025-12-04 19:55:29,244 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:55:29,292 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:55:29,293 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:55:29,378 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:55:29,378 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:55:29,378 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/checkpoint-639/special_tokens_map.json\n",
      "[INFO|trainer.py:2676] 2025-12-04 19:55:29,734 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 2960.1995, 'train_samples_per_second': 0.861, 'train_steps_per_second': 0.216, 'train_loss': 0.40086729491074136, 'epoch': 3.0, 'num_input_tokens_seen': 1835184}\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 639/639 [49:20<00:00,  4.63s/it]\n",
      "[INFO|trainer.py:3993] 2025-12-04 19:55:29,736 >> Saving model checkpoint to saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58\n",
      "[INFO|configuration_utils.py:696] 2025-12-04 19:55:29,785 >> loading configuration file /mnt/workspace/Qwen3-8B/config.json\n",
      "[INFO|configuration_utils.py:770] 2025-12-04 19:55:29,786 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.52.4\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2356] 2025-12-04 19:55:29,870 >> chat template saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2525] 2025-12-04 19:55:29,871 >> tokenizer config file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2534] 2025-12-04 19:55:29,871 >> Special tokens file saved in saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  num_input_tokens_seen    =    1835184\n",
      "  total_flos               = 77836961GF\n",
      "  train_loss               =     0.4009\n",
      "  train_runtime            = 0:49:20.19\n",
      "  train_samples_per_second =      0.861\n",
      "  train_steps_per_second   =      0.216\n",
      "Figure saved at: saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/training_loss.png\n",
      "Figure saved at: saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/training_eval_loss.png\n",
      "[WARNING|2025-12-04 19:55:30] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|trainer.py:4327] 2025-12-04 19:55:30,222 >> \n",
      "***** Running Evaluation *****\n",
      "[INFO|trainer.py:4329] 2025-12-04 19:55:30,222 >>   Num examples = 150\n",
      "[INFO|trainer.py:4332] 2025-12-04 19:55:30,222 >>   Batch size = 1\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:44<00:00,  3.34it/s]\n",
      "***** eval metrics *****\n",
      "  epoch                   =        3.0\n",
      "  eval_loss               =      0.688\n",
      "  eval_runtime            = 0:00:45.23\n",
      "  eval_samples_per_second =      3.316\n",
      "  eval_steps_per_second   =      3.316\n",
      "  num_input_tokens_seen   =    1835184\n",
      "[INFO|modelcard.py:450] 2025-12-04 19:56:15,453 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    }
   ],
   "source": [
    "!export USE_MODELSCOPE_HUB=1 && \\\n",
    "llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7ff4f1-3221-4478-984b-f37f4fa57ff5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2 é…ç½®å‚æ•°\n",
    "ç»“åˆ **A10 (24G)** æ˜¾å¡ä¸ **Qwen3-8B** çš„ç¯å¢ƒï¼Œä»¥ä¸‹æ˜¯é’ˆå¯¹é…ç½®ç•Œé¢çš„è¯¦ç»†å‚æ•°è§£è¯»ã€‚è¿™äº›è®¾ç½®å…±åŒæ„æˆäº† **QLoRA** å¾®è°ƒæ–¹æ¡ˆï¼Œæ˜¯åœ¨å•å¡ç¯å¢ƒä¸‹å¤„ç†åŒ»ç–—æ¨ç†ï¼ˆReasoningï¼‰é•¿æ–‡æœ¬æ•°æ®çš„æœ€ä½³å®è·µã€‚\n",
    "\n",
    "![image-20251204185719305](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041857405.png)\n",
    "\n",
    "#### 3.2.1 æ¨¡å‹è·¯å¾„ä¸åç§°é…ç½® (Model Configuration)\n",
    "* **å‚æ•°é¡¹**ï¼š`æ¨¡å‹åç§°` & `æ¨¡å‹è·¯å¾„`\n",
    "* **å½“å‰å€¼**ï¼š\n",
    "    * åç§°ï¼š`Qwen3-8B-Instruct`\n",
    "    * è·¯å¾„ï¼š`/mnt/workspace/Qwen3-8B`\n",
    "* **é…ç½®ä½œç”¨**ï¼š\n",
    "    * **æ¨¡å‹è·¯å¾„**ï¼šè¿™æ˜¯â€œåœ°åŸºâ€ã€‚ç¨‹åºä¼šä» `/mnt/workspace/Qwen3-8B` è¯»å–æ¨¡å‹æƒé‡çš„ç‰©ç†æ–‡ä»¶ã€‚\n",
    "    * **æ¨¡å‹åç§°**ï¼šè¿™æ˜¯â€œæ ‡è¯†â€ã€‚LLaMA-Factory ä¼šæ ¹æ®è¿™ä¸ªåç§°è‡ªåŠ¨åŒ¹é…é»˜è®¤çš„è¶…å‚æ•°å’Œæ¨¡æ¿ã€‚\n",
    "* **ç¡¬ä»¶å½±å“**ï¼šA10 çš„ 24G æ˜¾å­˜åŠ è½½ 8B æ¨¡å‹æ¯«æ— å‹åŠ›ï¼Œè¿™ä¸€æ­¥æ˜¯åŸºç¡€ã€‚\n",
    "\n",
    "æ¨¡å‹è·¯å¾„ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼Œå¦‚ï¼š`/mnt/workspace/Qwen3-8B`ã€‚ï¼ˆè€ƒè™‘å…¬ç½‘æœ‰æ—¶å€™ä¸‹è½½æ¨¡å‹æ…¢ï¼Œæä¾›ç»™å¤§å®¶å‡†å¤‡å¥½äº†ç¦»çº¿æ¨¡å‹å‹ç¼©åŒ…ï¼Œå¯ä»¥è§£å‹åˆ°`/mnt/workspace/Qwen3-8B`ï¼‰\n",
    "\n",
    "å…¶ä¸­`/mnt/workspace/Qwen3-8B`å†…å®¹å¦‚ä¸‹ï¼Œé‡åˆ°å‡ºé”™ï¼Œè¯·æ ¸å¯¹æ–‡ä»¶å¤¹æ˜¯ä¸æ˜¯å°‘å†…å®¹\n",
    "\n",
    "```bash\n",
    "root@dsw-523480-6d6bb75cf6-kz46d:/mnt/workspace/Qwen3-8B# ll\n",
    "total 16013364\n",
    "drwxr-xr-x  2 root root       4096 Dec  4 17:47 ./\n",
    "drwxr-xr-x 10 root root       4096 Dec  4 18:38 ../\n",
    "-rw-r--r--  1 root root        757 Dec  4 17:02 config.json\n",
    "-rw-r--r--  1 root root         73 Dec  4 17:02 configuration.json\n",
    "-rw-r--r--  1 root root        251 Dec  4 17:02 generation_config.json\n",
    "-rw-r--r--  1 root root       2175 Dec  4 17:02 .gitattributes\n",
    "-rw-r--r--  1 root root      11544 Dec  4 17:02 LICENSE\n",
    "-rw-r--r--  1 root root    1823241 Dec  4 17:02 merges.txt\n",
    "-rw-r--r--  1 root root 3996250744 Dec  4 17:41 model-00001-of-00005.safetensors\n",
    "-rw-r--r--  1 root root 3993160032 Dec  4 17:45 model-00002-of-00005.safetensors\n",
    "-rw-r--r--  1 root root 3959604768 Dec  4 17:47 model-00003-of-00005.safetensors\n",
    "-rw-r--r--  1 root root 3187841392 Dec  4 17:43 model-00004-of-00005.safetensors\n",
    "-rw-r--r--  1 root root 1244659840 Dec  4 17:14 model-00005-of-00005.safetensors\n",
    "-rw-r--r--  1 root root      33284 Dec  4 17:02 model.safetensors.index.json\n",
    "-rw-r--r--  1 root root      17012 Dec  4 17:02 README.md\n",
    "-rw-r--r--  1 root root       9971 Dec  4 17:02 tokenizer_config.json\n",
    "-rw-r--r--  1 root root   11422654 Dec  4 17:02 tokenizer.json\n",
    "-rw-r--r--  1 root root    2776833 Dec  4 17:02 vocab.json\n",
    "```\n",
    "\n",
    "è¿™æ˜¯å…¸å‹çš„ Hugging Face `transformers` æ ¼å¼çš„æ¨¡å‹ç›®å½•ç»“æ„ï¼ˆQwen ç³»åˆ—ï¼‰ã€‚\n",
    "\n",
    "è¿™ä¸ªç›®å½•åŒ…å«äº† **æ¨¡å‹æ¶æ„é…ç½®**ã€**åˆ†è¯å™¨ï¼ˆTokenizerï¼‰** å’Œ **æ¨¡å‹æƒé‡ï¼ˆWeightsï¼‰** ä¸‰å¤§æ ¸å¿ƒéƒ¨åˆ†ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯é€è¡Œæ³¨é‡Šè§£é‡Šï¼š\n",
    "\n",
    "```bash\n",
    "# ----------------- æ ¸å¿ƒé…ç½®åŒº -----------------\n",
    "-rw-r--r--  1 root root        757 Dec  4 17:02 config.json             # ã€æœ€å…³é”®ã€‘æ¨¡å‹æ¶æ„é…ç½®æ–‡ä»¶ã€‚å®šä¹‰äº†æ¨¡å‹æœ‰å¤šå°‘å±‚ã€éšè—å±‚å¤§å°ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ã€‚ä»£ç é€šè¿‡å®ƒæ¥æ„å»ºæ¨¡å‹éª¨æ¶ã€‚\n",
    "-rw-r--r--  1 root root         73 Dec  4 17:02 configuration.json      # è¾…åŠ©é…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸è¾ƒå°‘è§ï¼Œå¯èƒ½æ˜¯ç‰¹å®šæ¡†æ¶ç”Ÿæˆçš„å…ƒæ•°æ®ï¼Œé€šå¸¸ config.json æ˜¯ä¸»æ–‡ä»¶ï¼‰ã€‚\n",
    "-rw-r--r--  1 root root        251 Dec  4 17:02 generation_config.json  # æ¨ç†å‚æ•°é…ç½®ã€‚å®šä¹‰äº†æ¨¡å‹ç”Ÿæˆæ–‡æœ¬æ—¶çš„é»˜è®¤è¡Œä¸ºï¼ˆå¦‚ max_length, temperature, top_p, eos_token_id ç­‰ï¼‰ã€‚\n",
    "\n",
    "# ----------------- ç‰ˆæœ¬æ§åˆ¶ä¸åè®® -----------------\n",
    "-rw-r--r--  1 root root       2175 Dec  4 17:02 .gitattributes          # Git LFS é…ç½®æ–‡ä»¶ã€‚å‘Šè¯‰ git å“ªäº›æ–‡ä»¶æ˜¯å¤§æ–‡ä»¶ï¼ˆå¦‚ .safetensorsï¼‰ï¼Œéœ€è¦ç”¨ LFS æ–¹å¼ä¸‹è½½ã€‚\n",
    "-rw-r--r--  1 root root      11544 Dec  4 17:02 LICENSE                 # æ¨¡å‹çš„å¼€æºè®¸å¯è¯æ–‡ä»¶ï¼ˆå¦‚ Apache 2.0 æˆ– Qwen Research Licenseï¼‰ï¼Œè§„å®šäº†ä½ èƒ½å¦å•†ç”¨ã€‚\n",
    "\n",
    "# ----------------- åˆ†è¯å™¨ (Tokenizer) è¯è¡¨åŒº -----------------\n",
    "# ä½œç”¨ï¼šå°†äººç±»æ–‡å­—è½¬æ¢æˆæ•°å­— IDï¼Œåä¹‹äº¦ç„¶\n",
    "-rw-r--r--  1 root root    1823241 Dec  4 17:02 merges.txt              # BPE åˆå¹¶è§„åˆ™æ–‡ä»¶ã€‚è®°å½•äº†å“ªäº›å­—ç¬¦ç»„åˆå¯ä»¥åˆå¹¶æˆä¸€ä¸ª Tokenï¼ˆå¸¸è§äº GPT ç±»æ¨¡å‹ï¼‰ã€‚\n",
    "-rw-r--r--  1 root root    2776833 Dec  4 17:02 vocab.json              # è¯è¡¨æ–‡ä»¶ã€‚ä¸€ä¸ªå·¨å¤§çš„å­—å…¸ï¼Œè®°å½•äº†æ‰€æœ‰ Token å­—ç¬¦ä¸²åˆ°æ•°å­— ID çš„æ˜ å°„å…³ç³»ã€‚\n",
    "-rw-r--r--  1 root root   11422654 Dec  4 17:02 tokenizer.json          # ã€é‡è¦ã€‘Fast Tokenizer çš„å®Œæ•´å®šä¹‰æ–‡ä»¶ã€‚åŒ…å«äº†åˆ†è¯çš„æ‰€æœ‰é€»è¾‘ï¼ŒåŠ è½½é€Ÿåº¦æ¯” Python ä»£ç å¿«ã€‚\n",
    "-rw-r--r--  1 root root       9971 Dec  4 17:02 tokenizer_config.json   # åˆ†è¯å™¨è®¾ç½®æ–‡ä»¶ã€‚å®šä¹‰äº†ç‰¹æ®Š Tokenï¼ˆå¦‚ <|endoftext|>ï¼‰ï¼Œä»¥åŠä½¿ç”¨å“ªä¸ª Tokenizer ç±»ã€‚\n",
    "\n",
    "# ----------------- æ¨¡å‹æƒé‡åŒº (Weights) -----------------\n",
    "# ä½œç”¨ï¼šæ¨¡å‹çš„â€œå¤§è„‘â€ï¼Œå­˜å‚¨äº†è®­ç»ƒå¥½çš„å‚æ•°ï¼ˆçŸ©é˜µæ•°å€¼ï¼‰\n",
    "# ç”±äº 8B æ¨¡å‹å¾ˆå¤§ï¼ˆçº¦ 15GB+ï¼‰ï¼Œæ‰€ä»¥è¢«åˆ‡åˆ†æˆäº† 5 ä¸ªæ–‡ä»¶ï¼ˆShardingï¼‰\n",
    "-rw-r--r--  1 root root 3996250744 Dec  4 17:41 model-00001-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 1/5\n",
    "-rw-r--r--  1 root root 3993160032 Dec  4 17:45 model-00002-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 2/5\n",
    "-rw-r--r--  1 root root 3959604768 Dec  4 17:47 model-00003-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 3/5\n",
    "-rw-r--r--  1 root root 3187841392 Dec  4 17:43 model-00004-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 4/5\n",
    "-rw-r--r--  1 root root 1244659840 Dec  4 17:14 model-00005-of-00005.safetensors # æƒé‡åˆ†ç‰‡ 5/5\n",
    "\n",
    "# ----------------- æƒé‡ç´¢å¼•ä¸è¯´æ˜ -----------------\n",
    "-rw-r--r--  1 root root      33284 Dec  4 17:02 model.safetensors.index.json     # æƒé‡ç´¢å¼•æ˜ å°„è¡¨ã€‚å› ä¸ºæƒé‡è¢«åˆ‡åˆ†äº†ï¼Œè¿™ä¸ªæ–‡ä»¶å‘Šè¯‰åŠ è½½å™¨ï¼šâ€œç¬¬ X å±‚çš„å‚æ•°åœ¨ç¬¬ Y ä¸ª safetensors æ–‡ä»¶é‡Œâ€ã€‚\n",
    "-rw-r--r--  1 root root      17012 Dec  4 17:02 README.md               # æ¨¡å‹è¯´æ˜ä¹¦ï¼ˆMarkdown æ ¼å¼ï¼‰ã€‚é€šå¸¸åŒ…å«æ¨¡å‹ä»‹ç»ã€å¼•ç”¨æ–¹æ³•ã€ä½¿ç”¨ç¤ºä¾‹ä»£ç ç­‰ã€‚\n",
    "```\n",
    "\n",
    "å½“ä½ è¿è¡Œ `AutoModelForCausalLM.from_pretrained(\"è¿™ä¸ªç›®å½•\")` æ—¶ï¼š\n",
    "\n",
    "1. **è¯»å– `config.json`**ï¼šæ­å»ºç©ºçš„ç¥ç»ç½‘ç»œéª¨æ¶ã€‚\n",
    "2. **è¯»å– `model.safetensors.index.json`**ï¼šæŸ¥æ‰¾å‚æ•°éƒ½åœ¨å“ªã€‚\n",
    "3. **åŠ è½½ `model-0000\\*.safetensors`**ï¼šæŠŠå…·ä½“çš„æ•°å­—å¡«å…¥éª¨æ¶ä¸­ã€‚\n",
    "\n",
    "å½“ä½ è¿è¡Œ `AutoTokenizer.from_pretrained(\"è¿™ä¸ªç›®å½•\")` æ—¶ï¼š\n",
    "\n",
    "1. **è¯»å– `tokenizer_config.json`**ï¼šç¡®è®¤é…ç½®ã€‚\n",
    "2. **ä¼˜å…ˆåŠ è½½ `tokenizer.json`**ï¼šå¦‚æœå¤±è´¥ï¼Œæ‰å»å°è¯•ç»„åˆ `vocab.json` å’Œ `merges.txt`ã€‚\n",
    "\n",
    "#### 3.2.2 å¾®è°ƒæ ¸å¿ƒæ–¹æ³• (Finetuning Method)\n",
    "* **å‚æ•°é¡¹**ï¼š`å¾®è°ƒæ–¹æ³•`\n",
    "* **å½“å‰å€¼**ï¼š`lora`\n",
    "* **é…ç½®ä½œç”¨**ï¼š\n",
    "    * é€‰æ‹© **LoRA** (Low-Rank Adaptation) æŠ€æœ¯ã€‚å®ƒä¸ä¼šä¿®æ”¹æ¨¡å‹åŸæœ¬çš„å‡ åäº¿ä¸ªå‚æ•°ï¼Œè€Œæ˜¯åœ¨æ¨¡å‹æ—æŒ‚è½½ä¸€äº›å°çš„â€œé€‚é…å™¨â€çŸ©é˜µæ¥è®­ç»ƒã€‚\n",
    "* **ä¸ºä»€ä¹ˆä¸é€‰ fullï¼ˆå…¨é‡ï¼‰**ï¼š\n",
    "    * å…¨é‡å¾®è°ƒ 8B æ¨¡å‹éœ€è¦ä¿å­˜åºå¤§çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œè‡³å°‘éœ€è¦ 100GB+ çš„æ˜¾å­˜ã€‚\n",
    "    * **A10 (24G) çš„å”¯ä¸€å‡ºè·¯**ï¼šå¯¹äºå•å¡ A10ï¼Œå¿…é¡»ä½¿ç”¨ LoRA æˆ– QLoRA æ‰èƒ½è·‘å¾—èµ·æ¥ã€‚\n",
    "\n",
    "#### 3.2.3 é‡åŒ–è®¾ç½® (Quantization) â€”â€” å…³é”®ä¼˜åŒ–\n",
    "è¿™ä¸€æ­¥æ˜¯å°† **LoRA** å‡çº§ä¸º **QLoRA** çš„å…³é”®ï¼Œç›´æ¥å†³å®šäº†æ‚¨èƒ½è®­ç»ƒå¤šé•¿çš„åŒ»ç–—ç—…ä¾‹ã€‚\n",
    "\n",
    "* **å‚æ•°é¡¹**ï¼š`é‡åŒ–ç­‰çº§`\n",
    "* **å½“å‰å€¼**ï¼š`4`\n",
    "* **é…ç½®ä½œç”¨**ï¼š\n",
    "    * å°†æ¨¡å‹æƒé‡çš„ç²¾åº¦ä» 16-bit (åŠç²¾åº¦) å‹ç¼©åˆ° 4-bitã€‚\n",
    "* **å¯¹ A10 (24G) çš„å·¨å¤§å½±å“**ï¼š\n",
    "    * **ä¸é‡åŒ– (16-bit)**ï¼š8B æ¨¡å‹ä»…æƒé‡å°±å ç”¨çº¦ **16GB** æ˜¾å­˜ã€‚æ­¤æ—¶ A10 åªå‰© 8GB ç»™è®­ç»ƒæ•°æ®ï¼Œåªè¦åŒ»ç–—æ–‡æœ¬ç¨å¾®é•¿ä¸€ç‚¹ï¼ˆä¾‹å¦‚åŒ…å«å¤æ‚çš„ç—…æƒ…æè¿°ï¼‰ï¼Œæ˜¾å­˜å°±ä¼šç¬é—´æº¢å‡º (OOM)ã€‚\n",
    "    * **4-bit é‡åŒ–**ï¼šæ¨¡å‹æƒé‡ä»…å ç”¨çº¦ **5.5GB** æ˜¾å­˜ã€‚\n",
    "    * **æ”¶ç›Š**ï¼šæ‚¨ç©ºå‡ºäº†è¿‘ **18GB** çš„æ˜¾å­˜ï¼è¿™æ„å‘³ç€æ‚¨å¯ä»¥å°† `Context Length`ï¼ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼‰æ‹‰å¾—å¾ˆå¤§ï¼ˆä¾‹å¦‚ 4096 æˆ– 8192ï¼‰ï¼Œè¿™å¯¹äº **medical-o1-reasoning** è¿™ç§éœ€è¦é•¿é€»è¾‘é“¾æ¨ç†çš„æ•°æ®é›†è‡³å…³é‡è¦ã€‚\n",
    "\n",
    "* **å‚æ•°é¡¹**ï¼š`é‡åŒ–æ–¹æ³•`\n",
    "* **å½“å‰å€¼**ï¼š`bnb` (bitsandbytes)\n",
    "* **é…ç½®ä½œç”¨**ï¼š\n",
    "    * è¿™æ˜¯å®ç° 4-bit é‡åŒ–çš„åº•å±‚åŠ é€Ÿåº“ã€‚æ‚¨çš„ç¯å¢ƒæ˜¯ Ubuntu 22.04 + CUDA 12.4ï¼Œ`bnb` èƒ½å¤Ÿå®Œç¾è°ƒç”¨ A10 çš„ç¡¬ä»¶åŠ é€Ÿèƒ½åŠ›ã€‚\n",
    "\n",
    "#### 3.2.4 æ•°æ®æ¨¡æ¿ä¸æ ¼å¼ (Template)\n",
    "* **å‚æ•°é¡¹**ï¼š`å¯¹è¯æ¨¡æ¿`\n",
    "* **å½“å‰å€¼**ï¼š`qwen3` (æ³¨æ„ï¼šæˆªå›¾å¯èƒ½éœ€ç¡®è®¤ä¸º qwen æˆ– qwen2.5/3 å¯¹åº”æ¨¡æ¿ï¼ŒQwen3 é€šå¸¸å…¼å®¹ Qwen2.5 æ¨¡æ¿)\n",
    "* **é…ç½®ä½œç”¨**ï¼š\n",
    "    * å¦‚åŒâ€œç¿»è¯‘å™¨â€ï¼Œå®ƒå°†æ‚¨çš„åŒ»ç–—æ•°æ®é›†æ ¼å¼åŒ–ä¸º Qwen æ¨¡å‹èƒ½å¬æ‡‚çš„ç‰¹æ®Šå­—ç¬¦ç»“æ„ï¼ˆä¾‹å¦‚ `<|im_start|>`ï¼‰ã€‚\n",
    "* **ä¸¥é‡åæœ**ï¼šå¦‚æœé€‰é”™ï¼ˆä¾‹å¦‚é€‰æˆ llama3ï¼‰ï¼Œæ¨¡å‹ä¼šå› ä¸ºçœ‹ä¸æ‡‚æ•°æ®ç»“æ„è€Œè¾“å‡ºä¹±ç ï¼Œæˆ–è€…åœ¨è®­ç»ƒæ—¶ loss æ— æ³•ä¸‹é™ã€‚åŠ¡å¿…ç¡®ä¿ä¸æ¨¡å‹ç³»åˆ—ä¸€è‡´ã€‚\n",
    "\n",
    "#### 3.2.5 ç¡¬ä»¶åŠ é€Ÿç­–ç•¥ (Booster)\n",
    "* **å‚æ•°é¡¹**ï¼š`åŠ é€Ÿæ–¹å¼`\n",
    "* **å½“å‰å€¼**ï¼š`auto`\n",
    "* **é…ç½®ä½œç”¨**ï¼š\n",
    "    * è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„æ³¨æ„åŠ›æœºåˆ¶åŠ é€Ÿç®—æ³•ï¼Œé€šå¸¸ä¼šå¯ç”¨ **FlashAttention-2**ã€‚\n",
    "* **ç¯å¢ƒåŒ¹é…**ï¼š\n",
    "    * A10 æ˜¾å¡å±äº Ampere æ¶æ„ï¼Œå®Œç¾æ”¯æŒ FlashAttention-2ã€‚\n",
    "    * **æ•ˆæœ**ï¼šç›¸æ¯”ä¸å¼€å¯åŠ é€Ÿï¼Œè®­ç»ƒé€Ÿåº¦é€šå¸¸èƒ½æå‡ 2-3 å€ï¼Œä¸”èƒ½è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ã€‚ä¿æŒ `auto` æ˜¯æœ€çœå¿ƒçš„é€‰æ‹©ã€‚\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d9f0fc-4705-4808-8da8-1282e85c5b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-04T10:59:53.666507Z",
     "iopub.status.busy": "2025-12-04T10:59:53.666356Z",
     "iopub.status.idle": "2025-12-04T10:59:53.670561Z",
     "shell.execute_reply": "2025-12-04T10:59:53.669904Z",
     "shell.execute_reply.started": "2025-12-04T10:59:53.666492Z"
    },
    "tags": []
   },
   "source": [
    "#### 3.2.6 è®­ç»ƒé˜¶æ®µ (Training Stage)\n",
    "* **é…ç½®é¡¹**ï¼š`è®­ç»ƒé˜¶æ®µ`\n",
    "* **å½“å‰è®¾ç½®**ï¼š`Supervised Fine-Tuning` (SFT)\n",
    "* **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "    * **å®šä¹‰**ï¼šå…¨ç§°æ˜¯â€œæœ‰ç›‘ç£å¾®è°ƒâ€ã€‚è¿™æ˜¯ç›®å‰è®©å¤§æ¨¡å‹é€‚é…ç‰¹å®šé¢†åŸŸï¼ˆå¦‚åŒ»ç–—ï¼‰æœ€ç›´æ¥ã€æœ€é«˜æ•ˆçš„æ–¹æ³•ã€‚\n",
    "    * **åŸç†**ï¼šå°±å¥½æ¯”ç»™å­¦ç”Ÿï¼ˆæ¨¡å‹ï¼‰çœ‹å¤§é‡çš„â€œé—®é¢˜+æ ‡å‡†ç­”æ¡ˆâ€çš„è¯•å·ï¼Œè®©å®ƒå­¦ä¼šç…§è‘«èŠ¦ç”»ç“¢ã€‚ä¸ä¹‹ç›¸å¯¹çš„è¿˜æœ‰â€œé¢„è®­ç»ƒï¼ˆPre-Trainingï¼‰â€ï¼ˆè¯»è¯¾æœ¬ï¼Œå­¦é€šè¯†ï¼‰å’Œâ€œDPO/RLHFâ€ï¼ˆå­¦ä»·å€¼è§‚ï¼Œåˆ†è¾¨å¥½åï¼‰ã€‚\n",
    "    * **é€‚ç”¨åœºæ™¯**ï¼šæ‚¨ç°åœ¨çš„ç›®æ ‡æ˜¯è®© Qwen3 æ‹¥æœ‰åŒ»ç–—æ¨ç†èƒ½åŠ›ï¼ŒSFT æ˜¯æ ‡å‡†é€‰æ‹©ã€‚\n",
    "    * **ç¯å¢ƒå½±å“**ï¼šSFT ç›¸æ¯”é¢„è®­ç»ƒæå…¶èŠ‚çœèµ„æºã€‚åœ¨æ‚¨çš„ A10 (24G) ä¸Šï¼Œé…åˆ LoRAï¼ŒSFT è¿è¡Œèµ·æ¥éå¸¸è½»é‡ã€‚\n",
    "\n",
    "#### 3.2.7 æ•°æ®è·¯å¾„ä¸æ•°æ®é›†é€‰æ‹© (Data Configuration)\n",
    "* **é…ç½®é¡¹**ï¼š`æ•°æ®è·¯å¾„ (Data Path)` & `æ•°æ®é›† (Dataset)`\n",
    "* **å½“å‰è®¾ç½®**ï¼š\n",
    "    * è·¯å¾„ï¼š`data`\n",
    "    * æ•°æ®é›†ï¼š`medical_sft` (æˆªå›¾æ˜¾ç¤ºæ‚¨å·²é€‰ä¸­æ­¤é¡¹)\n",
    "* **è¯¦ç»†è§£é‡Š (æ–°æ‰‹å¿…è¯»)**ï¼š\n",
    "    * **æ•°æ®è·¯å¾„**ï¼šæŒ‡å‘ LLaMA-Factory æ ¹ç›®å½•ä¸‹çš„ `data` æ–‡ä»¶å¤¹ã€‚è¿™é‡Œé¢å­˜æ”¾ç€è‡³å…³é‡è¦çš„ `dataset_info.json` é…ç½®æ–‡ä»¶ã€‚\n",
    "    * **æ•°æ®é›†é€‰æ‹©**ï¼š\n",
    "        * è¿™é‡Œçš„ `medical_sft` å¹¶ä¸æ˜¯æ–‡ä»¶åï¼Œè€Œæ˜¯åœ¨ `dataset_info.json` ä¸­æ³¨å†Œçš„ä¸€ä¸ª**æ ‡ç­¾ï¼ˆKeyï¼‰**ã€‚\n",
    "        * **å…³é”®æ“ä½œ**ï¼šæ‚¨ä¹‹å‰æåˆ°äº† `medical-o1-reasoning-SFT` æ•°æ®é›†ã€‚æ‚¨éœ€è¦ç¡®ä¿å·²ç»åœ¨ `data/dataset_info.json` æ–‡ä»¶ä¸­æ·»åŠ äº†å¯¹åº”çš„é…ç½®ï¼Œå°† `medical_sft` è¿™ä¸ªæ ‡ç­¾æŒ‡å‘æ‚¨å®é™…ä¸‹è½½çš„ JSON æˆ– Parquet æ–‡ä»¶ã€‚\n",
    "        * *é”™è¯¯æ’æŸ¥*ï¼šå¦‚æœæ‚¨åœ¨ä¸‹æ‹‰èœå•é‡Œæ‰¾ä¸åˆ°æ‚¨çš„æ•°æ®é›†ï¼Œé€šå¸¸æ˜¯å› ä¸ºæ²¡æœ‰åœ¨ `dataset_info.json` é‡Œæ³¨å†Œï¼Œæˆ–è€…æ–‡ä»¶æ²¡æ”¾å¯¹ä½ç½®ã€‚\n",
    "* **ä½œç”¨**ï¼šå‘Šè¯‰ç¨‹åºä½¿ç”¨å“ªä¸€ä»½å…·ä½“çš„åŒ»ç–—ç—…å†æ•°æ®æ¥å–‚ç»™æ¨¡å‹ã€‚\n",
    "\n",
    "#### 3.2.8 æ•°æ®é¢„è§ˆåŠŸèƒ½ (Dataset Preview)\n",
    "* **é…ç½®é¡¹**ï¼š`é¢„è§ˆæ•°æ®é›†` (æŒ‰é’®)\n",
    "* **å»ºè®®æ“ä½œ**ï¼š**å¼ºçƒˆå»ºè®®ç‚¹å‡»**ã€‚\n",
    "* **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "    * **ä½œç”¨**ï¼šå®ƒä¼šè¯»å–å‰å‡ è¡Œæ•°æ®å¹¶å±•ç¤ºå‡ºæ¥ã€‚\n",
    "    * **æ–°æ‰‹æ£€æŸ¥ç‚¹**ï¼š\n",
    "        1.  **åˆ—ååŒ¹é…**ï¼šæ£€æŸ¥å±•ç¤ºçš„æ•°æ®ä¸­ï¼Œæ˜¯å¦åŒ…å«â€œinstructionâ€ï¼ˆæŒ‡ä»¤/é—®é¢˜ï¼‰ã€â€œinputâ€ï¼ˆè¾“å…¥/ç—…å†è¯¦æƒ…ï¼Œå¯é€‰ï¼‰ã€â€œoutputâ€ï¼ˆè¾“å‡º/è¯Šæ–­ç»“æœï¼‰è¿™å‡ åˆ—ã€‚\n",
    "        2.  **æ ¼å¼æ­£å¸¸**ï¼šç¡®ä¿æ²¡æœ‰ä¹±ç ï¼Œä¸”å†…å®¹ç¡®å®æ˜¯æ‚¨é¢„æœŸçš„åŒ»ç–—æ¨ç†æ•°æ®ã€‚\n",
    "    * **ä¸ºä»€ä¹ˆé‡è¦**ï¼šå¦‚æœæ•°æ®æ ¼å¼ä¸å¯¹ï¼ˆæ¯”å¦‚åˆ—åå†™æˆäº† `question` è€Œä¸æ˜¯ `instruction`ï¼‰ï¼Œæ¨¡å‹è®­ç»ƒæ—¶ä¼šæ‰¾ä¸åˆ°è¾“å…¥ï¼Œå¯¼è‡´è®­ç»ƒè™½ç„¶åœ¨è·‘ï¼Œä½†ä»€ä¹ˆéƒ½æ²¡å­¦åˆ°ï¼ˆLoss ä¸ä¸‹é™ï¼‰ã€‚åœ¨å¼€å§‹æ¼«é•¿çš„è®­ç»ƒå‰ï¼ŒèŠ± 10 ç§’é’Ÿé¢„è§ˆæ˜¯æ€§ä»·æ¯”æœ€é«˜çš„æ“ä½œã€‚\n",
    "\n",
    "æ•°æ®é›†ä½¿ç”¨æˆ‘ä»¬çš„åŒ»ç–—æ•°æ®é›†`medical_sft`ã€‚\n",
    "![image-20251203165018105](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031650258.png)\n",
    "å¯ä»¥ç‚¹å‡»ã€Œé¢„è§ˆæ•°æ®é›†ã€ã€‚ç‚¹å‡»å…³é—­è¿”å›è®­ç»ƒç•Œé¢ã€‚\n",
    "![image-20251203165125842](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031651285.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bb212f-24d8-46a1-b4d1-8345f4f5531f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "è®¾ç½®`å­¦ä¹ ç‡`ä¸º`1e-4`ï¼Œ`æ¢¯åº¦ç´¯ç§¯`ä¸º`2`ï¼Œæœ‰åˆ©äºæ¨¡å‹æ‹Ÿåˆã€‚å¦‚æœæ˜¾å¡æ˜¯`V100`ï¼Œè®¡ç®—ç±»å‹ä¿æŒä¸º`fp16`ï¼›å¦‚æœä½¿ç”¨äº†`A10`ï¼Œå¯ä»¥æ›´æ”¹è®¡ç®—ç±»å‹ä¸º`bf16`ã€‚\n",
    "![image-20251203165304488](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031653595.png)\n",
    "\n",
    "ç‚¹å‡»`LoRAå‚æ•°è®¾ç½®`å±•å¼€å‚æ•°åˆ—è¡¨ï¼Œè®¾ç½®`LoRA+å­¦ä¹ ç‡æ¯”ä¾‹`ä¸º`16`ï¼Œ`LoRA+`è¢«è¯æ˜æ˜¯æ¯”LoRAå­¦ä¹ æ•ˆæœæ›´å¥½çš„ç®—æ³•ã€‚åœ¨`LoRAä½œç”¨æ¨¡å—`ä¸­å¡«å†™`all`ï¼Œå³å°†LoRAå±‚æŒ‚è½½åˆ°æ¨¡å‹çš„æ‰€æœ‰çº¿æ€§å±‚ä¸Šï¼Œæé«˜æ‹Ÿåˆæ•ˆæœã€‚\n",
    "![image-20251203165600495](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031656725.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1f273-1915-49a6-9b58-f0c4889e8c6f",
   "metadata": {},
   "source": [
    "#### 3.2.9 å­¦ä¹ ç‡ (Learning Rate)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`å­¦ä¹ ç‡`\n",
    "- **å½“å‰å€¼**ï¼š`1e-4` (å³ 0.0001)\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šå†³å®šäº†æ¨¡å‹â€œè¿ˆæ­¥å­â€çš„å¤§å°ã€‚\n",
    "  - **æ–°æ‰‹æŒ‡å—**ï¼š\n",
    "    - å¯¹äº **LoRA** å¾®è°ƒï¼Œ`1e-4` æ˜¯ä¸€ä¸ªéå¸¸æ ‡å‡†ä¸”ç¨³å¥çš„â€œé»„é‡‘åˆå§‹å€¼â€ã€‚\n",
    "    - å¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼Œè¿™ä¸ªå€¼é€šå¸¸è¦å°å¾—å¤šï¼ˆå¦‚ `1e-5`ï¼‰ã€‚ä½†å¯¹äº LoRAï¼Œä¿æŒ `1e-4` é€šå¸¸èƒ½è·å¾—ä¸é”™çš„æ”¶æ•›é€Ÿåº¦ã€‚\n",
    "  - **è°ƒæ•´å»ºè®®**ï¼šåˆæ¬¡è¿è¡Œå»ºè®®ä¿æŒä¸å˜ã€‚å¦‚æœå‘ç° Lossï¼ˆæŸå¤±å€¼ï¼‰å®Œå…¨ä¸ä¸‹é™ï¼Œå¯ä»¥å°è¯•ç¨å¾®è°ƒå¤§ï¼›å¦‚æœ Loss å‰§çƒˆéœ‡è¡ï¼Œåˆ™è°ƒå°ã€‚\n",
    "\n",
    "#### 3.2.10 è®­ç»ƒè½®æ•° (Epochs)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`è®­ç»ƒè½®æ•°`\n",
    "- **å½“å‰å€¼**ï¼š`3.0`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šæŒ‡çš„æ˜¯æ¨¡å‹è¦æŠŠä½ çš„æ•´ä¸ªåŒ»ç–—æ•°æ®é›†å®Œæ•´åœ°â€œçœ‹â€å‡ éã€‚\n",
    "  - **å½±å“**ï¼š\n",
    "    - **è¿‡å°‘ (æ¯”å¦‚ 1)**ï¼šæ¨¡å‹å¯èƒ½è¿˜æ²¡å­¦ä¼šï¼Œå¯¼è‡´â€œæ¬ æ‹Ÿåˆâ€ã€‚\n",
    "    - **è¿‡å¤š (æ¯”å¦‚ 10)**ï¼šæ¨¡å‹å¯èƒ½ä¼šæ­»è®°ç¡¬èƒŒè®­ç»ƒé¢˜ï¼Œå¯¼è‡´â€œè¿‡æ‹Ÿåˆâ€ï¼Œé‡åˆ°æ–°ç—…å†å°±ä¸ä¼šæ²»äº†ã€‚\n",
    "  - **æ¨è**ï¼š`3.0` æ˜¯ç»å¤§å¤šæ•° SFT ä»»åŠ¡çš„æ ‡å‡†èµ·ç‚¹ï¼Œéå¸¸åˆé€‚ã€‚\n",
    "\n",
    "#### 3.2.11 æœ€å¤§æ ·æœ¬æ•° (Max Samples) â€”â€” **é‡è¦è­¦ç¤º**\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`æœ€å¤§æ ·æœ¬æ•°`\n",
    "- **å½“å‰å€¼**ï¼š`1000`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šå¼ºè¡Œé™åˆ¶åªä½¿ç”¨æ•°æ®é›†é‡Œçš„å‰ 1000 æ¡æ•°æ®æ¥è®­ç»ƒã€‚\n",
    "  - **æ–°æ‰‹è¯¯åŒºè­¦æŠ¥**ï¼š\n",
    "    - **è°ƒè¯•æ¨¡å¼**ï¼šå¦‚æœä½ åªæ˜¯æƒ³èŠ±å‡ åˆ†é’Ÿæµ‹è¯•ä¸€ä¸‹ A10 æ˜¾å¡èƒ½ä¸èƒ½è·‘é€šä»£ç ï¼Œè®¾ç½® `1000` æ˜¯å¯¹çš„ã€‚\n",
    "    - **æ­£å¼è®­ç»ƒ**ï¼š**è¯·åŠ¡å¿…æ¸…ç©ºæ­¤é¡¹**ï¼ˆç•™ç©ºï¼‰ã€‚å¦‚æœä½ çš„åŒ»ç–—æ•°æ®é›†æœ‰ 10 ä¸‡æ¡æ•°æ®ï¼Œå¡«äº† `1000` å°±æ„å‘³ç€æ¨¡å‹ä¸¢å¼ƒäº† 99% çš„çŸ¥è¯†ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹æå…¶å¼±æ™ºã€‚\n",
    "  - **æ“ä½œå»ºè®®**ï¼šè·‘é€šæµ‹è¯•åï¼Œæ­£å¼è·‘æ•°æ®æ—¶è®°å¾—åˆ æ‰è¿™ä¸ªæ•°å­—ã€‚\n",
    "\n",
    "#### 3.2.12 æˆªæ–­é•¿åº¦ (Cutoff Length)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`æˆªæ–­é•¿åº¦`\n",
    "- **å½“å‰å€¼**ï¼š`2048`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šé™åˆ¶ä¸€æ¡æ•°æ®ï¼ˆè¾“å…¥é—®é¢˜ + æ¨¡å‹å›ç­”ï¼‰çš„æœ€å¤§æ€»å­—æ•°ï¼ˆToken æ•°ï¼‰ã€‚è¶…è¿‡çš„éƒ¨åˆ†ä¼šè¢«â€œä¸€åˆ€åˆ‡â€åˆ‡æ‰ã€‚\n",
    "  - **ç”Ÿäº§å»ºè®®**ï¼š **Medical O1 Reasoning (åŒ»ç–—æ¨ç†)**ã€‚è¿™ç±»æ•°æ®é€šå¸¸åŒ…å«å¾ˆé•¿çš„æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰ï¼Œè¯¦ç»†æè¿°ç—…æƒ…å’Œæ¨ç†è¿‡ç¨‹ã€‚`2048` å¯èƒ½åçŸ­ï¼Œå®¹æ˜“æŠŠå…³é”®çš„æ¨ç†ç»“è®ºåˆ‡æ‰ã€‚\n",
    "\n",
    "#### 3.2.13 æ˜¾å­˜æ§åˆ¶æ ¸å¿ƒï¼šæ‰¹å¤„ç†ä¸æ¢¯åº¦ç´¯ç§¯\n",
    "\n",
    "è¿™é‡Œæ˜¯å†³å®šä½ è®­ç»ƒé€Ÿåº¦å’Œæ˜¾å­˜å ç”¨çš„â€œæ²¹é—¨â€å’Œâ€œåˆ¹è½¦â€ã€‚\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`æ‰¹å¤„ç†å¤§å° (Batch Size)`\n",
    "- **å½“å‰å€¼**ï¼š`1`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šæ¯ä¸ª GPU ä¸€æ¬¡è¯»å…¥å¹¶è®¡ç®— 1 æ¡æ•°æ®ã€‚\n",
    "- **é…ç½®é¡¹**ï¼š`æ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation)`\n",
    "- **å½“å‰å€¼**ï¼š`4`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šæ”’å¤Ÿ 4 æ¬¡ï¼ˆ1x4=4ï¼‰å†ä¿®æ”¹ä¸€æ¬¡æ¨¡å‹å‚æ•°ã€‚\n",
    "  - **çœŸå®æ‰¹å¤§å°**ï¼šå®é™…ç”Ÿæ•ˆçš„ Batch Size = `æ‰¹å¤„ç†å¤§å°` * `æ¢¯åº¦ç´¯ç§¯` = 1 * 4 = 4ã€‚\n",
    "\n",
    "#### 3.2.14 éªŒè¯é›†æ¯”ä¾‹ (Val Ratio)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`éªŒè¯é›†æ¯”ä¾‹`\n",
    "- **å½“å‰å€¼**ï¼š`0.15` (15%)\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šä»è®­ç»ƒæ•°æ®é‡Œåˆ‡å‡º 15% çš„é¢˜ä¸ç»™æ¨¡å‹å­¦ï¼Œä¸“é—¨ç•™åˆ°è€ƒè¯•ç”¨ï¼Œçœ‹çœ‹æ¨¡å‹æ˜¯ä¸æ˜¯çœŸçš„æ‡‚äº†ï¼ˆè¿˜æ˜¯æ­»è®°ç¡¬èƒŒï¼‰ã€‚\n",
    "  - **å»ºè®®**ï¼š\n",
    "    - å¦‚æœæ•°æ®æ€»é‡å¾ˆå¤§ï¼ˆæ¯”å¦‚ >1ä¸‡æ¡ï¼‰ï¼Œ15% å¯èƒ½å¤ªå¤šäº†ï¼ˆæµªè´¹æ•°æ®ï¼‰ï¼Œå¯ä»¥æ”¹ä¸º `0.05`ã€‚\n",
    "    - å¦‚æœæ•°æ®é‡å°‘ï¼ˆ<1000æ¡ï¼‰ï¼Œ`0.15` æ˜¯åˆç†çš„ã€‚\n",
    "\n",
    "#### 3.2.15 è®¡ç®—ç±»å‹ (Compute Type)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`è®¡ç®—ç±»å‹`\n",
    "- **å½“å‰å€¼**ï¼š`bf16`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šä½¿ç”¨ Brain Float 16 ç²¾åº¦è¿›è¡Œè®¡ç®—ã€‚\n",
    "  - **ç¯å¢ƒåŒ¹é…**ï¼š**å®Œç¾**ã€‚ä½ çš„ A10 æ˜¾å¡ï¼ˆAmpere æ¶æ„ï¼‰åŸç”Ÿæ”¯æŒ bf16ã€‚ç›¸æ¯” fp16ï¼Œbf16 æ›´åŠ ç¨³å®šï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¸å®¹æ˜“å‡ºç° Loss å˜æˆ NaNï¼ˆæ•°å€¼æº¢å‡ºï¼‰çš„æŠ¥é”™ã€‚è¯·åŠ¡å¿…ä¿æŒ `bf16`ã€‚\n",
    "\n",
    "![image-20251204190718700](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041907846.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11679d68-de89-4d29-ba89-8704ba4027d9",
   "metadata": {},
   "source": [
    "#### 3.2.16 æ—¥å¿—ä¸ä¿å­˜é…ç½® (Logging & Checkpointing)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`æ—¥å¿—é—´éš”`\n",
    "- **å½“å‰å€¼**ï¼š`5`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šæ¯è®­ç»ƒ 5 æ­¥ï¼ˆStepsï¼‰ï¼Œåœ¨å±å¹•ä¸Šæ‰“å°ä¸€æ¬¡å½“å‰çš„ Lossï¼ˆæŸå¤±å€¼ï¼‰å’Œå­¦ä¹ ç‡ã€‚\n",
    "  - **å»ºè®®**ï¼šä¿æŒé»˜è®¤ã€‚è¿™ä¸ªé¢‘ç‡èƒ½è®©ä½ å®æ—¶çœ‹åˆ°æ¨¡å‹æ˜¯ä¸æ˜¯â€œå­¦åºŸäº†â€ï¼ˆLoss æš´æ¶¨ï¼‰æˆ–è€…â€œå­¦å¾—å¥½â€ï¼ˆLoss ç¨³æ­¥ä¸‹é™ï¼‰ï¼Œæ–¹ä¾¿åŠæ—¶ç»ˆæ­¢é”™è¯¯è®­ç»ƒã€‚\n",
    "- **é…ç½®é¡¹**ï¼š`ä¿å­˜é—´éš”`\n",
    "- **å½“å‰å€¼**ï¼š`100`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šæ¯è®­ç»ƒ 100 æ­¥ï¼Œè‡ªåŠ¨æŠŠå½“å‰è®­ç»ƒå¥½çš„æ¨¡å‹æƒé‡ä¿å­˜ä¸ºä¸€ä¸ªæ–‡ä»¶å¤¹ï¼ˆCheckpointï¼‰ã€‚\n",
    "  - **ç¡¬ç›˜ç©ºé—´é¢„è­¦**ï¼šè™½ç„¶ LoRA æ–‡ä»¶å¾ˆå°ï¼ˆå‡ å MBï¼‰ï¼Œä½†å¦‚æœä½ çš„è®­ç»ƒæ€»æ­¥æ•°å¾ˆå¤šï¼ˆæ¯”å¦‚ 10000 æ­¥ï¼‰ï¼Œæ¯ 100 æ­¥å­˜ä¸€æ¬¡ä¼šäº§ç”Ÿ 100 ä¸ªæ–‡ä»¶å¤¹ï¼Œç®¡ç†èµ·æ¥å¾ˆéº»çƒ¦ã€‚\n",
    "  - **ä¼˜åŒ–å»ºè®®**ï¼šå¦‚æœæ˜¯æ­£å¼è®­ç»ƒï¼Œå»ºè®®æ”¹ä¸º `500` æˆ– `1000`ï¼Œé¿å…ç”Ÿæˆå¤ªå¤šä¸­é—´æ–‡ä»¶ã€‚\n",
    "\n",
    "#### 3.2.17 é¢„çƒ­æ­¥æ•° (Warmup Steps)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`é¢„çƒ­æ­¥æ•°`\n",
    "- **å½“å‰å€¼**ï¼š`0`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šåœ¨è®­ç»ƒåˆšå¼€å§‹æ—¶ï¼Œå…ˆç”¨å¾ˆå°çš„å­¦ä¹ ç‡æ…¢æ…¢çƒ­èº«ï¼Œç„¶åå†å¢åŠ åˆ° `1e-4`ã€‚\n",
    "  - **ä½œç”¨**ï¼šé˜²æ­¢æ¨¡å‹ä¸€å¼€å§‹å› ä¸ºâ€œæ­¥å­è¿ˆå¤ªå¤§â€è€Œè·‘åã€‚\n",
    "  - **å»ºè®®**ï¼šå¯¹äºåŒ»ç–—æ¨ç†è¿™ç§å¤æ‚ä»»åŠ¡ï¼Œå»ºè®®è®¾ç½®ä¸ºæ€»æ­¥æ•°çš„ 10% å·¦å³ï¼ˆä¾‹å¦‚å¡« `10` æˆ– `20`ï¼‰ã€‚è¿™èƒ½è®©æ¨¡å‹æ›´å¹³æ»‘åœ°è¿›å…¥å­¦ä¹ çŠ¶æ€ã€‚å¡« `0` ä¹Ÿèƒ½è·‘ï¼Œä½†ä¸å¤Ÿç¨³å¥ã€‚\n",
    "\n",
    "#### 3.2.18 é¢å¤–å‚æ•°æ·±åº¦è§£æ (Extra Arguments) â€”â€” **æ ¸å¿ƒä¼˜åŒ–**\n",
    "\n",
    "è¾“å…¥æ¡†ä¸­å¡«å†™çš„è¿™æ®µä»£ç æ˜¯æ•´ä¸ªé…ç½®çš„â€œç‚¹ç›ä¹‹ç¬”â€ï¼š\n",
    "\n",
    "`{\"optim\": \"adamw_torch\", \"gradient_checkpointing\": true}`\n",
    "\n",
    "è¿™ä¸¤ä¸ªå‚æ•°å¯¹ A10 æ˜¾å¡çš„å½±å“æå¤§ï¼Œä¸‹é¢é€ä¸€è§£é‡Šï¼š\n",
    "\n",
    "**1. ä¼˜åŒ–å™¨è®¾ç½® (`\"optim\": \"adamw_torch\"`)**\n",
    "\n",
    "- **å«ä¹‰**ï¼šæŒ‡å®šä½¿ç”¨ PyTorch å®˜æ–¹å®ç°çš„ AdamW ä¼˜åŒ–å™¨ã€‚\n",
    "- **ä½œç”¨**ï¼š\n",
    "  - **ç¨³å®šæ€§**ï¼šè¿™æ˜¯æœ€æ ‡å‡†ã€å…¼å®¹æ€§æœ€å¥½çš„ä¼˜åŒ–å™¨å®ç°ã€‚ç›¸æ¯”äºä¸€äº›é­”æ”¹ç‰ˆï¼ˆå¦‚ 8bit-adamï¼‰ï¼Œå®ƒæ›´ä¸å®¹æ˜“æŠ¥é”™ã€‚\n",
    "  - **æ˜¾å­˜ä»£ä»·**ï¼šå®ƒéœ€è¦çš„æ˜¾å­˜ç¨å¤šä¸€ç‚¹ç‚¹ï¼Œä½†åœ¨ä½ çš„ 24G æ˜¾å¡ + 4-bit é‡åŒ–ç¯å¢ƒä¸‹ï¼Œè¿™ç‚¹å¼€é”€å®Œå…¨å¯ä»¥æ¥å—ã€‚\n",
    "\n",
    "**2. æ¢¯åº¦æ£€æŸ¥ç‚¹ (`\"gradient_checkpointing\": true`) â€”â€” çœæ˜¾å­˜ç¥å™¨**\n",
    "\n",
    "- **å«ä¹‰**ï¼šè¿™æ˜¯ä¸€ç§â€œä»¥æ—¶é—´æ¢ç©ºé—´â€çš„æŠ€æœ¯ã€‚\n",
    "  - **ä¸å¼€å¯æ—¶**ï¼šæ¨¡å‹åœ¨å‘å‰è®¡ç®—ï¼ˆForwardï¼‰æ—¶ï¼Œä¼šæŠŠæ¯ä¸€å±‚çš„ä¸­é—´ç»“æœï¼ˆActivationsï¼‰éƒ½å­˜æ»¡æ˜¾å­˜ï¼Œç•™ç€ç»™åå‘ä¼ æ’­ï¼ˆBackwardï¼‰ç”¨ã€‚è¿™éå¸¸åƒæ˜¾å­˜ï¼\n",
    "  - **å¼€å¯å**ï¼šæ¨¡å‹ä¸å­˜ä¸­é—´ç»“æœäº†ã€‚ç­‰éœ€è¦åå‘ä¼ æ’­æ—¶ï¼Œå†ä¸´æ—¶é‡æ–°ç®—ä¸€éã€‚\n",
    "- **å¯¹ A10 (24G) çš„å·¨å¤§ä»·å€¼**ï¼š\n",
    "  - **æ˜¾å­˜æš´é™**ï¼šå¼€å¯æ­¤é€‰é¡¹é€šå¸¸èƒ½èŠ‚çœ **20%** çš„æ˜¾å­˜å ç”¨ã€‚\n",
    "  - **ä»£ä»·**ï¼šè®­ç»ƒé€Ÿåº¦ä¼šå˜æ…¢çº¦ 20-30%ã€‚\n",
    "  - **ç»“è®º**ï¼š**å¿…é¡»å¼€å¯**ã€‚å¯¹äºå•å¡è®­ç»ƒï¼Œä¸ºäº†èƒ½è·‘é•¿æ–‡æœ¬ï¼ˆåŒ»ç–—ç—…å†ï¼‰ï¼Œç‰ºç‰²ä¸€ç‚¹é€Ÿåº¦æ¢å–æ›´å¤§çš„ä¸Šä¸‹æ–‡ç©ºé—´æ˜¯éå¸¸åˆ’ç®—çš„ã€‚\n",
    "\n",
    "#### 3.2.19 NEFTune å™ªå£°å‚æ•°\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`NEFTune å™ªå£°å‚æ•°`\n",
    "- **å½“å‰å€¼**ï¼š`0`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **ä½œç”¨**ï¼šä¸€ç§ç»™ Embedding å±‚åŠ å™ªå£°çš„æŠ€æœ¯ï¼Œèƒ½é˜²æ­¢è¿‡æ‹Ÿåˆã€‚\n",
    "  - **åˆå­¦è€…å»ºè®®**ï¼šä¿æŒ `0`ï¼ˆå…³é—­ï¼‰ã€‚ç›®å‰è¿™ä¸ªæŠ€æœ¯è¿˜åœ¨å®éªŒé˜¶æ®µï¼Œæœ‰æ—¶å€™ä¼šå¯¼è‡´æ¨¡å‹å˜å¾—â€œèƒ¡è¨€ä¹±è¯­â€ã€‚å¯¹äºåˆæ¬¡ä¸Šæ‰‹ï¼Œæˆ‘ä»¬å…ˆè¿½æ±‚â€œè·‘é€šâ€å’Œâ€œç¨³å®šâ€ã€‚\n",
    "\n",
    "![image-20251204191239259](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041912330.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33260fe5-3f6b-4873-9e25-711a0c8523da",
   "metadata": {},
   "source": [
    "#### 3.2.20 LoRA ç§© (LoRA Rank)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`LoRA ç§©`\n",
    "- **å½“å‰å€¼**ï¼š`8`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šå¯ä»¥ç†è§£ä¸º LoRA è¿™ä¸ªâ€œå¤–æŒ‚å¤§è„‘â€çš„å®¹é‡æˆ–å¸¦å®½ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½å­¦åˆ°çš„å¤æ‚çŸ¥è¯†è¶Šå¤šï¼Œä½†ç”Ÿæˆçš„æƒé‡æ–‡ä»¶ï¼ˆAdapterï¼‰ä¹Ÿè¶Šå¤§ï¼Œæ˜¾å­˜å ç”¨ç¨é«˜ã€‚\n",
    "  - **A10 24G å»ºè®®**ï¼š\n",
    "    - `8` æ˜¯ LLaMA-Factory çš„é»˜è®¤å€¼ï¼Œéå¸¸èŠ‚çœèµ„æºï¼Œé€‚åˆå…¥é—¨è·‘é€šæµç¨‹ã€‚\n",
    "    - **è¿›é˜¶å»ºè®®**ï¼šé’ˆå¯¹ **Medical O1 Reasoning** è¿™ç§éœ€è¦å¼ºé€»è¾‘æ¨ç†çš„ä»»åŠ¡ï¼Œ`8` å¯èƒ½ç•¥æ˜¾å•è–„ã€‚å¦‚æœæ‚¨çš„æ˜¾å­˜è¿˜å¾ˆå¯Œè£•ï¼ˆç»“åˆ 4-bit é‡åŒ–é€šå¸¸æ˜¯å¯Œè£•çš„ï¼‰ï¼Œå»ºè®®å°è¯•å°†æ­¤å€¼è°ƒé«˜è‡³ **`16`** æˆ– **`32`**ï¼Œç”šè‡³ **`64`**ã€‚è¿™èƒ½æ˜¾è‘—æå‡æ¨¡å‹å¯¹å¤æ‚åŒ»ç–—é€»è¾‘çš„æ‹Ÿåˆèƒ½åŠ›ã€‚\n",
    "\n",
    "#### 3.2.21 LoRA ç¼©æ”¾ç³»æ•° (LoRA Alpha)\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`LoRA ç¼©æ”¾ç³»æ•°`\n",
    "- **å½“å‰å€¼**ï¼š`16`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šå†³å®šäº† LoRA å­¦åˆ°çš„æ–°æƒé‡å¯¹åŸæ¨¡å‹å½±å“çš„â€œå€ç‡â€ã€‚\n",
    "  - **é»„é‡‘æ³•åˆ™**ï¼šé€šå¸¸è®¾ç½®ä¸º `LoRA ç§©` çš„ 2 å€ã€‚\n",
    "  - **æ“ä½œå»ºè®®**ï¼šå¦‚æœæ‚¨æŠŠä¸Šé¢çš„ Rank æ”¹æˆäº† 16ï¼Œè®°å¾—æŠŠè¿™é‡Œçš„ Alpha æ”¹æˆ 32ï¼›å¦‚æœ Rank æ˜¯ 32ï¼Œè¿™é‡Œå°±å¡« 64ã€‚ä¿æŒ `Alpha = 2 * Rank` è¿™ä¸ªæ¯”ä¾‹æ˜¯æœ€ç¨³å¥çš„åšæ³•ã€‚\n",
    "\n",
    "#### 3.2.22 LoRA+ å­¦ä¹ ç‡æ¯”ä¾‹ (LoRA+ LR Ratio) â€”â€” **é«˜çº§æŠ€å·§**\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`LoRA+ å­¦ä¹ ç‡æ¯”ä¾‹`\n",
    "- **å½“å‰å€¼**ï¼š`16`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šè¿™æ˜¯ä¸€ä¸ªåä¸º **LoRA+** çš„ä¼˜åŒ–ç®—æ³•ã€‚å®ƒè®© LoRA çš„ä¸¤ä¸ªçŸ©é˜µï¼ˆçŸ©é˜µ A å’ŒçŸ©é˜µ Bï¼‰ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡è¿›è¡Œè®­ç»ƒã€‚\n",
    "  - **ä½œç”¨**ï¼šç ”ç©¶è¡¨æ˜ï¼Œè®©çŸ©é˜µ B å­¦å¾—æ¯”çŸ©é˜µ A å¿«ï¼ˆé€šå¸¸å¿« 16 å€å·¦å³ï¼‰ï¼Œèƒ½å¤§å¹…æå‡è®­ç»ƒæ•ˆç‡å’Œæ•ˆæœã€‚\n",
    "  - **ç¯å¢ƒå½±å“**ï¼šå¼€å¯è¿™ä¸ªé€‰é¡¹ï¼ˆåªè¦æ•°å€¼ä¸ä¸º 0 å³å¼€å¯ï¼‰ä¸ä¼šå¢åŠ æ˜¾å­˜æ¶ˆè€—ï¼Œå±äºâ€œå…è´¹çš„åˆé¤â€ã€‚å¯¹äº Qwen ç³»åˆ—æ¨¡å‹ï¼Œä¿ç•™é»˜è®¤å€¼ `16` æ•ˆæœé€šå¸¸å¾ˆå¥½ã€‚\n",
    "\n",
    "#### 3.2.23 LoRA ä½œç”¨æ¨¡å— (Target Modules) â€”â€” **å…³é”®é…ç½®**\n",
    "\n",
    "- **é…ç½®é¡¹**ï¼š`LoRA ä½œç”¨æ¨¡å—`\n",
    "- **å½“å‰å€¼**ï¼š`all`\n",
    "- **è¯¦ç»†è§£é‡Š**ï¼š\n",
    "  - **å«ä¹‰**ï¼šæŒ‡å®š LoRA å…·ä½“è¦æŒ‚è½½åœ¨æ¨¡å‹çš„å“ªäº›å±‚ä¸Šï¼ˆä¾‹å¦‚ Qã€Kã€V æ³¨æ„åŠ›å±‚ï¼Œæˆ–è€… MLP å±‚ï¼‰ã€‚\n",
    "  - **å½“å‰è®¾ç½® \"all\"**ï¼šè¿™æ˜¯ä¸€ä¸ªéå¸¸æ¿€è¿›ä¸”ä¼˜ç§€çš„è®¾ç½®ã€‚å®ƒæ„å‘³ç€ LoRA ä¼šæŒ‚è½½åˆ°æ¨¡å‹æ‰€æœ‰çš„çº¿æ€§å±‚ä¸Šï¼ˆ`q_proj`, `k_proj`, `v_proj`, `o_proj`, `gate_proj`, `up_proj`, `down_proj`ï¼‰ã€‚\n",
    "  - **å¯¹åŒ»ç–—æ¨ç†çš„å½±å“**ï¼š\n",
    "    - ä»…æŒ‚è½½ Q/Vï¼ˆä¼ ç»Ÿåšæ³•ï¼‰ï¼šæ¨¡å‹åªèƒ½å­¦åˆ°ä¸€ç‚¹çš®æ¯›ã€‚\n",
    "    - **æŒ‚è½½ \"all\"**ï¼šæ¨¡å‹èƒ½å…¨æ–¹ä½åœ°è°ƒæ•´è‡ªå·±çš„é€»è¾‘å¤„ç†èƒ½åŠ›ã€‚å¯¹äºæ¨ç†ç±»ï¼ˆReasoningï¼‰ä»»åŠ¡ï¼Œå¿…é¡»é€‰ `all` æ‰èƒ½è·å¾—æœ€ä½³æ•ˆæœã€‚\n",
    "  - **ç¡¬ä»¶å‹åŠ›**ï¼šç›¸æ¯”åªæŒ‚è½½ Q/Vï¼Œé€‰æ‹© `all` ä¼šå¢åŠ ä¸€äº›æ˜¾å­˜æ¶ˆè€—å’Œè®¡ç®—é‡ã€‚ä½†åœ¨ **A10 (24G)** ä¸Šï¼Œè¿™å®Œå…¨åœ¨æ‰¿å—èŒƒå›´å†…ã€‚**è¯·åŠ¡å¿…ä¿æŒ \"all\"**ã€‚\n",
    "\n",
    "![image-20251204191714495](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512041917591.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f6dac-54af-4f83-90a6-922c7054301d",
   "metadata": {},
   "source": [
    "### 3.3 å¯åŠ¨å¾®è°ƒ\n",
    "å°†è¾“å‡ºç›®å½•ä¿®æ”¹ä¸º`train_qwen3_8b`ï¼Œè®­ç»ƒåçš„LoRAæƒé‡å°†ä¼šä¿å­˜åœ¨æ­¤ç›®å½•ä¸­ã€‚ç‚¹å‡»ã€Œé¢„è§ˆå‘½ä»¤ã€å¯å±•ç¤ºæ‰€æœ‰å·²é…ç½®çš„å‚æ•°ï¼Œæ‚¨å¦‚æœæƒ³é€šè¿‡ä»£ç è¿è¡Œå¾®è°ƒï¼Œå¯ä»¥å¤åˆ¶è¿™æ®µå‘½ä»¤ï¼Œåœ¨å‘½ä»¤è¡Œè¿è¡Œã€‚\n",
    "\n",
    "ç‚¹å‡»ã€Œå¼€å§‹ã€å¯åŠ¨æ¨¡å‹å¾®è°ƒã€‚\n",
    "![image-20251203170030807](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512031700144.png)\n",
    "\n",
    "å¯åŠ¨å¾®è°ƒåéœ€è¦ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œå¾…æ¨¡å‹ä¸‹è½½å®Œæ¯•åå¯åœ¨ç•Œé¢è§‚å¯Ÿåˆ°è®­ç»ƒè¿›åº¦å’ŒæŸå¤±æ›²çº¿ã€‚æ¨¡å‹å¾®è°ƒå¤§çº¦éœ€è¦50åˆ†é’Ÿï¼Œæ˜¾ç¤ºâ€œè®­ç»ƒå®Œæ¯•â€ä»£è¡¨å¾®è°ƒæˆåŠŸã€‚\n",
    "\n",
    "![image-20251204200651649](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042006782.png)\n",
    "\n",
    "#### 3.3.1 è®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§æ·±åº¦è§£æ (Efficiency & Stability)\n",
    "\n",
    "è¿™éƒ¨åˆ†ä¸»è¦è¯„ä¼°æ‚¨çš„ç¡¬ä»¶èµ„æºæ˜¯å¦è¢«å……åˆ†åˆ©ç”¨ï¼Œä»¥åŠè®­ç»ƒè¿‡ç¨‹æ˜¯å¦å¥åº·ã€‚\n",
    "\n",
    "- **ç¡¬ä»¶é€‚é…åº¦ï¼šæ»¡åˆ†**\n",
    "  - **æ˜¾å­˜çŠ¶æ€**ï¼šæ—¥å¿—å…¨ç§°æ—  OOM (Out Of Memory) æŠ¥é”™ï¼Œè¯´æ˜ `4-bit é‡åŒ–` + `æ¢¯åº¦æ£€æŸ¥ç‚¹` çš„ç­–ç•¥éå¸¸æˆåŠŸã€‚åœ¨ A10 (24G) æ˜¾å¡ä¸Šï¼Œæ˜¾å­˜å ç”¨è¢«ç‰¢ç‰¢æ§åˆ¶åœ¨å®‰å…¨çº¿ä»¥å†…ã€‚\n",
    "  - **è®¡ç®—ç²¾åº¦**ï¼šæ—¥å¿—æ˜¾ç¤º `compute dtype: torch.bfloat16`ã€‚A10 æ˜¾å¡åŸç”Ÿæ”¯æŒ `bf16`ï¼Œè¿™æ¯” `fp16` æ›´ç¨³å®šï¼Œèƒ½æœ‰æ•ˆé˜²æ­¢è®­ç»ƒä¸­å‡ºç° Loss=NaNï¼ˆæ•°å€¼æº¢å‡ºï¼‰çš„ç¾éš¾æ€§é”™è¯¯ã€‚\n",
    "- **è®­ç»ƒé€Ÿåº¦ï¼šæå¿«**\n",
    "  - **ååé‡**ï¼šç¨³å®šåœ¨ **620 - 630 tokens/s**ã€‚å¯¹äº 8B å‚æ•°é‡çš„æ¨¡å‹ï¼Œåœ¨å•å¡ä¸Šè¾¾åˆ°è¿™ä¸ªé€Ÿåº¦å±äº**ç¬¬ä¸€æ¢¯é˜Ÿ**çš„æ•ˆç‡ã€‚\n",
    "  - **æ ·æœ¬å¤„ç†**ï¼šçº¦ `0.86 samples/s`ã€‚è¿™æ„å‘³ç€å¤„ç†ä¸€æ¡å‡ åƒå­—çš„å¤æ‚åŒ»ç–—æ¨ç†æ•°æ®ï¼Œåªéœ€è¦ 1.2 ç§’å·¦å³ã€‚\n",
    "- **ç»“è®º**ï¼šæ‚¨çš„ç¯å¢ƒé…ç½®ï¼ˆPyTorch 2.6 + CUDA 12.4 + A10ï¼‰éå¸¸å¥å£®ï¼Œå®Œå…¨å¯ä»¥æ”¯æ’‘åç»­æ›´å¤§è§„æ¨¡çš„è®­ç»ƒã€‚\n",
    "\n",
    "#### 3.3.2 æŸå¤±å‡½æ•°ä¸è¿‡æ‹Ÿåˆæ·±åº¦è¯Šæ–­ (Loss Analysis) â€”â€” **æœ€å…³é”®çš„å‘ç°**\n",
    "> è¾“å‡ºç›®å½•ï¼ˆåœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºçš„ train_2025-12-04-19-04-58ï¼‰ï¼Œè·¯å¾„é€šå¸¸æ˜¯ï¼š saves/Qwen3-8B-Instruct/lora/train_2025-12-04-19-04-58/\n",
    "![image-20251204201640965](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042016031.png)\n",
    "![image-20251204201557168](https://cdn.jsdelivr.net/gh/Fly0905/note-picture@main/imag/202512042015285.png)\n",
    "\n",
    "è¿™æ˜¯æ•´ä¸ªæ—¥å¿—ä¸­æœ€æœ‰ä»·å€¼çš„æ•°æ®ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬æ¨¡å‹åˆ°åº•å­¦å¾—æ€ä¹ˆæ ·ã€‚è¯·é‡ç‚¹å…³æ³¨ **Eval Loss (æ¨¡æ‹Ÿè€ƒè¯•æˆç»©)** çš„å˜åŒ–è½¨è¿¹ã€‚\n",
    "\n",
    "- **ç¬¬ä¸€é˜¶æ®µï¼šå¿«é€Ÿå­¦ä¹ æœŸ (Step 0 - 200)**\n",
    "  - **è¡¨ç°**ï¼šTrain Loss è¿…é€Ÿä¸‹é™ï¼ŒEval Loss ä¹Ÿä» `0.5516` é™è‡³ **`0.5446`** (Step 200)ã€‚\n",
    "  - **è§£è¯»**ï¼šæ­¤æ—¶æ¨¡å‹æ­£åœ¨åƒæµ·ç»µä¸€æ ·å¸æ”¶çŸ¥è¯†ï¼Œå®ƒå­¦ä¼šäº†å¦‚ä½•æ¨¡ä»¿åŒ»ç”Ÿçš„è¯­æ°”ï¼Œä¹ŸæŒæ¡äº†åŸºæœ¬çš„æ¨ç†é€»è¾‘ã€‚\n",
    "  - **çŠ¶æ€**ï¼š**æœ€ä½³ (Optimal)**ã€‚\n",
    "- **ç¬¬äºŒé˜¶æ®µï¼šæ­»è®°ç¡¬èƒŒæœŸ (Step 200 - 400)**\n",
    "  - **è¡¨ç°**ï¼šTrain Loss ç»§ç»­ä¸‹é™ï¼ˆå¹³æ—¶ä½œä¸šå…¨å¯¹ï¼‰ï¼Œä½† Eval Loss å¼€å§‹åå¼¹è‡³ `0.57` - `0.60`ã€‚\n",
    "  - **è§£è¯»**ï¼šæ¨¡å‹å¼€å§‹ä¸ºäº†åˆ·ä½è®­ç»ƒé›†çš„ Lossï¼Œå»æ­»è®°ç¡¬èƒŒæŸäº›ç‰¹å®šçš„ç—…å†æè¿°ï¼Œè€Œä¸æ˜¯ç†è§£å…¶ä¸­çš„åŒ»ç†ã€‚å®ƒå¼€å§‹â€œåç§‘â€äº†ã€‚\n",
    "- **ç¬¬ä¸‰é˜¶æ®µï¼šä¸¥é‡è¿‡æ‹ŸåˆæœŸ (Step 400 - 639)**\n",
    "  - **è¡¨ç°**ï¼šTrain Loss é™åˆ°äº†æä½çš„ `0.21`ï¼Œä½† Eval Loss é£™å‡è‡³ **`0.688`**ï¼Œç”šè‡³æ¯”åˆšå¼€å§‹è®­ç»ƒæ—¶è¿˜è¦å·®ã€‚\n",
    "  - **è§£è¯»**ï¼šæ¨¡å‹å½»åº•â€œå­¦å‚»äº†â€ã€‚å¦‚æœä½ é—®å®ƒè®­ç»ƒé›†é‡Œä¸€æ¨¡ä¸€æ ·çš„é—®é¢˜ï¼Œå®ƒèƒ½ç­”å¾—å¾ˆå®Œç¾ï¼›ä½†å¦‚æœä½ é—®ä¸€ä¸ªæ–°çš„ç—…äººæƒ…å†µï¼Œå®ƒå¯èƒ½ä¼šèƒ¡è¨€ä¹±è¯­ï¼Œå› ä¸ºå®ƒçš„é€šç”¨èƒ½åŠ›è¢«ç ´åäº†ã€‚\n",
    "\n",
    "#### 3.3.3 å­¦ä¹ å†…å®¹ä¸æ•°æ®è´¨é‡éªŒè¯ (Data Quality Verification)\n",
    "\n",
    "æ—¥å¿—ä¸­æœ‰ä¸€æ®µæ•°æ®çš„ Input/Output é¢„è§ˆï¼Œè¿™è¯å®äº†æ‚¨çš„æ•°æ®æ¸…æ´—å·¥ä½œæ˜¯æˆåŠŸçš„ã€‚\n",
    "\n",
    "- **æ ‡ç­¾è¯†åˆ«**ï¼šæ—¥å¿—æ˜¾ç¤º `labels: <think>ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹...</think>`ã€‚\n",
    "- **æ·±åº¦è§£è¯»**ï¼š\n",
    "  - æ¨¡å‹ä¸ä»…å­¦åˆ°äº†â€œç­”æ¡ˆâ€ï¼Œè¿˜å­¦åˆ°äº†è¢« `<think>` æ ‡ç­¾åŒ…è£¹çš„**â€œéšå¼æ€ç»´é“¾â€**ã€‚\n",
    "  - è¿™æ„å‘³ç€åœ¨æ¨ç†æ—¶ï¼Œæ¨¡å‹ä¼šå…ˆåœ¨å†…éƒ¨è¿›è¡Œä¸€æ­¥æ­¥çš„é€»è¾‘æ¨å¯¼ï¼ˆæ¯”å¦‚å…ˆåˆ†æç—…å› ï¼Œå†å®šæ€§ï¼Œæœ€åç»™æ–¹æ¡ˆï¼‰ï¼Œè¿™æ­£æ˜¯ OpenAI o1 ç­‰æ¨ç†æ¨¡å‹çš„æ ¸å¿ƒç‰¹æ€§ã€‚\n",
    "\n",
    "#### 3.3.4 æœ€ä½³æ¨¡å‹é”å®š (Model Selection)\n",
    "\n",
    "è¿™æ˜¯æ–°æ‰‹æœ€å®¹æ˜“çŠ¯é”™çš„åœ°æ–¹ï¼š**åƒä¸‡ä¸è¦ä»¥ä¸ºè·‘å®Œæœ€åä¸€æ­¥çš„æ¨¡å‹å°±æ˜¯æœ€å¥½çš„ã€‚**\n",
    "\n",
    "- **å½“å‰é™·é˜±**ï¼šé»˜è®¤è¾“å‡ºçš„ `checkpoint-639` æ˜¯ä¸€ä¸ªè¿‡æ‹Ÿåˆçš„æ¨¡å‹ï¼Œæ³›åŒ–èƒ½åŠ›å¾ˆå·®ã€‚\n",
    "- **æ­£ç¡®é€‰æ‹©**ï¼šè¯·å›æº¯åˆ° **`checkpoint-200`**ã€‚\n",
    "- **æ“ä½œå»ºè®®**ï¼šåœ¨ WebUI ä¸­åŠ è½½æ¨¡å‹æ—¶ï¼Œè¯·æ‰‹åŠ¨é€‰æ‹© `checkpoint-200` æ–‡ä»¶å¤¹ã€‚å¦‚æœå¯èƒ½ï¼Œå»ºè®®æ‚¨æŠŠè¿™ä¸ªæ–‡ä»¶å¤¹å•ç‹¬å¤åˆ¶å‡ºæ¥å¤‡ä»½ï¼Œé‡å‘½åä¸º `Qwen3-Medical-Best`ã€‚\n",
    "\n",
    "------\n",
    "\n",
    "#### 3.3.5 æ ¸å¿ƒæŒ‡æ ‡æ€»ç»“è¡¨ (Summary Table)\n",
    "\n",
    "ä¸ºäº†æ–¹ä¾¿æ‚¨å­˜æ¡£å’Œå¯¹æ¯”ï¼Œæˆ‘ä¸ºæ‚¨æ€»ç»“äº†è¿™ä»½è¯¦ç»†çš„è®­ç»ƒæŠ¥å‘Šè¡¨ï¼š\n",
    "\n",
    "| **æ ¸å¿ƒç»´åº¦**   | **ç»†åˆ†æŒ‡æ ‡** | **ç»“æœæ•°å€¼/çŠ¶æ€**      | **åˆå­¦è€…è§£è¯»ä¸å»ºè®®**                                         |\n",
    "| -------------- | ------------ | ---------------------- | ------------------------------------------------------------ |\n",
    "| **ç¡¬ä»¶ä¸ç¯å¢ƒ** | æ˜¾å¡èµ„æº     | A10 (24G) å•å¡         | **å®Œç¾**ã€‚æ˜¾å­˜å ç”¨åˆç†ï¼Œæ— æº¢å‡ºï¼Œåˆ©ç”¨ç‡é«˜ã€‚                   |\n",
    "|                | è®¡ç®—ç²¾åº¦     | bfloat16 (bf16)        | **ä¼˜ç§€**ã€‚æ¯” fp16 æ›´ç¨³å®šï¼Œé€‚åˆ A10 æ¶æ„ã€‚                    |\n",
    "|                | é‡åŒ–ç­–ç•¥     | 4-bit (QLoRA)          | **å…³é”®**ã€‚è¿™æ˜¯å®ç° 620+ tokens/s é«˜é€Ÿè®­ç»ƒçš„æ ¸å¿ƒåŸå› ã€‚        |\n",
    "| **æ•°æ®æƒ…å†µ**   | æ•°æ®æ€»é‡     | 850 æ¡                 | **åå°‘**ã€‚æ•°æ®è¶Šå°‘ï¼Œè¶Šå®¹æ˜“è¿‡æ‹Ÿåˆï¼ˆæœ¬æ¬¡ Step 200 å°±è¿‡æ‹Ÿåˆäº†ï¼‰ã€‚ |\n",
    "|                | æ•°æ®æ ¼å¼     | CoT (æ€ç»´é“¾)           | **æ­£ç¡®**ã€‚æˆåŠŸè¯†åˆ« `<think>` æ ‡ç­¾ï¼Œæ¨¡å‹å­¦ä¼šäº†æ¨ç†æ­¥éª¤ã€‚      |\n",
    "| **è®­ç»ƒè¡¨ç°**   | æ€»è€—æ—¶       | 49 åˆ†é’Ÿ                | æ•ˆç‡å¾ˆé«˜ï¼Œé€‚åˆå¿«é€Ÿè¿­ä»£å®éªŒã€‚                                 |\n",
    "|                | æœ€ä½³è½®æ•°     | Epoch ~0.94 (Step 200) | **é‡è¦çŸ¥è¯†ç‚¹**ï¼šå¹¶ä¸æ˜¯è½®æ•°è¶Šå¤šè¶Šå¥½ï¼Œå¯¹äºå°æ•°æ®ï¼Œ1 è½®å¾€å¾€å°±å¤Ÿäº†ã€‚ |\n",
    "| **æ¨¡å‹æ•ˆæœ**   | æœ€ä½³ Loss    | **0.5446** (éªŒè¯é›†)    | å‡ºç°åœ¨ **Step 200**ã€‚è¿™æ˜¯æ¨¡å‹â€œæ™ºåŠ›â€çš„å·…å³°æ—¶åˆ»ã€‚              |\n",
    "|                | æœ€ç»ˆ Loss    | 0.6880 (éªŒè¯é›†)        | å‡ºç°åœ¨ç»“æŸæ—¶ã€‚è¯´æ˜æ¨¡å‹åæœŸåœ¨â€œè´Ÿä¼˜åŒ–â€ã€‚                       |\n",
    "| **æœ€ç»ˆå†³ç­–**   | **æœ€ä½³å­˜æ¡£** | **checkpoint-200**     | **è¯·ä½¿ç”¨æ­¤ç‰ˆæœ¬ï¼** å®ƒæ˜¯æœ¬æ¬¡è®­ç»ƒçš„ç²¾åã€‚                      |\n",
    "|                | **é¿å‘å­˜æ¡£** | checkpoint-639         | å·²è¿‡æ‹Ÿåˆï¼Œå»ºè®®ä»…ä½œå¯¹æ¯”ç ”ç©¶ï¼Œä¸å»ºè®®ç”¨äºç”Ÿäº§ã€‚                 |\n",
    "| **åç»­ä¼˜åŒ–**   | ç­–ç•¥è°ƒæ•´     | å‡å°‘ Epoch / å¢åŠ æ•°æ®  | å»ºè®®ä¸‹æ¬¡å°† `Epoch` è®¾ä¸º 1ï¼Œæˆ–è€…å°†æ•°æ®æ‰©å……åˆ° 3000 æ¡ä»¥ä¸Šã€‚    |\n",
    "\n",
    "**ä¸€å¥è¯æ€»ç»“**ï¼šè¿™æ˜¯ä¸€æ¬¡æŠ€æœ¯ä¸Šéå¸¸æˆåŠŸçš„è¿è¡Œï¼Œç¯å¢ƒä¸å‚æ•°é…ç½®å®Œç¾ï¼Œä½†ç”±äºæ•°æ®é‡è¾ƒå°‘ï¼ˆ850æ¡ï¼‰ï¼Œæ¨¡å‹åœ¨ç¬¬ 1 è½®ï¼ˆStep 200ï¼‰å°±è¾¾åˆ°äº†æœ€ä½³çŠ¶æ€ï¼Œåç»­çš„è®­ç»ƒå±äºè¿‡åº¦å­¦ä¹ ã€‚è¯·ä½¿ç”¨ **Checkpoint-200** ä½œä¸ºæœ€ç»ˆæˆæœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484967d-f3a1-4f44-851e-39b378dda452",
   "metadata": {},
   "source": [
    "## 4. æ¨¡å‹è¯„ä¼°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1647de-2b53-4874-917e-9469aaa645c0",
   "metadata": {},
   "source": [
    "å¾®è°ƒå®Œæˆåï¼Œç‚¹å‡»é¡µé¢é¡¶éƒ¨çš„ã€Œåˆ·æ–°é€‚é…å™¨ã€ï¼Œç„¶åç‚¹å‡»é€‚é…å™¨è·¯å¾„ï¼Œå³å¯å¼¹å‡ºåˆšåˆšè®­ç»ƒå®Œæˆçš„LoRAæƒé‡ï¼Œç‚¹å‡»é€‰æ‹©ä¸‹æ‹‰åˆ—è¡¨ä¸­çš„train_llama3é€‰é¡¹ï¼Œåœ¨æ¨¡å‹å¯åŠ¨æ—¶å³å¯åŠ è½½å¾®è°ƒç»“æœã€‚\n",
    "![image.png](./_html/evaluate_adaptor.jpg)\n",
    "\n",
    "é€‰æ‹©ã€ŒEvaluate&Predictã€æ ï¼Œåœ¨æ•°æ®é›†ä¸‹æ‹‰åˆ—è¡¨ä¸­é€‰æ‹©ã€Œevalã€ï¼ˆéªŒè¯é›†ï¼‰è¯„ä¼°æ¨¡å‹ã€‚æ›´æ”¹è¾“å‡ºç›®å½•ä¸º`eval_llama3`ï¼Œæ¨¡å‹è¯„ä¼°ç»“æœå°†ä¼šä¿å­˜åœ¨è¯¥ç›®å½•ä¸­ã€‚æœ€åç‚¹å‡»å¼€å§‹æŒ‰é’®å¯åŠ¨æ¨¡å‹è¯„ä¼°ã€‚\n",
    "![image.png](./_html/evaluate_start.jpg)\n",
    "\n",
    "æ¨¡å‹è¯„ä¼°å¤§çº¦éœ€è¦5åˆ†é’Ÿå·¦å³ï¼Œè¯„ä¼°å®Œæˆåä¼šåœ¨ç•Œé¢ä¸Šæ˜¾ç¤ºéªŒè¯é›†çš„åˆ†æ•°ã€‚å…¶ä¸­ROUGEåˆ†æ•°è¡¡é‡äº†æ¨¡å‹è¾“å‡ºç­”æ¡ˆï¼ˆpredictï¼‰å’ŒéªŒè¯é›†ä¸­æ ‡å‡†ç­”æ¡ˆï¼ˆlabelï¼‰çš„ç›¸ä¼¼åº¦ï¼ŒROUGEåˆ†æ•°è¶Šé«˜ä»£è¡¨æ¨¡å‹å­¦ä¹ å¾—æ›´å¥½ã€‚\n",
    "![image.png](./_html/evaluate_result.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb18065-a0f2-4106-8c98-e35e4455e4d4",
   "metadata": {},
   "source": [
    "## 5. æ¨¡å‹å¯¹è¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df83adb-3fdc-4029-9ee9-24eec0a38b0e",
   "metadata": {},
   "source": [
    "é€‰æ‹©ã€ŒChatã€æ ï¼Œç¡®ä¿é€‚é…å™¨è·¯å¾„æ˜¯`train_llama3`ï¼Œç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€å³å¯åœ¨Web UIä¸­å’Œå¾®è°ƒæ¨¡å‹è¿›è¡Œå¯¹è¯ã€‚\n",
    "![image.png](./_html/chat_params.jpg)\n",
    "\n",
    "åœ¨é¡µé¢åº•éƒ¨çš„å¯¹è¯æ¡†è¾“å…¥æƒ³è¦å’Œæ¨¡å‹å¯¹è¯çš„å†…å®¹ï¼Œç‚¹å‡»ã€Œæäº¤ã€å³å¯å‘é€æ¶ˆæ¯ã€‚å‘é€åæ¨¡å‹ä¼šé€å­—ç”Ÿæˆå›ç­”ï¼Œä»å›ç­”ä¸­å¯ä»¥å‘ç°æ¨¡å‹å­¦ä¹ åˆ°äº†æ•°æ®é›†ä¸­çš„å†…å®¹ï¼Œèƒ½å¤Ÿæ°å½“åœ°æ¨¡ä»¿è¯¸è‘›äº®çš„è¯­æ°”å¯¹è¯ã€‚\n",
    "![image.png](./_html/chat_result1.jpg)\n",
    "\n",
    "ç‚¹å‡»ã€Œå¸è½½æ¨¡å‹ã€ï¼Œç‚¹å‡»â€œÃ—â€å·å–æ¶ˆé€‚é…å™¨è·¯å¾„ï¼Œå†æ¬¡ç‚¹å‡»ã€ŒåŠ è½½æ¨¡å‹ã€ï¼Œå³å¯ä¸å¾®è°ƒå‰çš„åŸå§‹æ¨¡å‹èŠå¤©ã€‚\n",
    "![image.png](./_html/chat_uninstall.jpg)\n",
    "\n",
    "é‡æ–°å‘æ¨¡å‹å‘é€ç›¸åŒçš„å†…å®¹ï¼Œå‘ç°åŸå§‹æ¨¡å‹æ— æ³•æ¨¡ä»¿è¯¸è‘›äº®çš„è¯­æ°”ç”Ÿæˆä¸­æ–‡å›ç­”ã€‚\n",
    "![image.png](./_html/chat_result2.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bf8e55-95ce-460c-ace9-a7996824b9e3",
   "metadata": {},
   "source": [
    "## 6. æ€»ç»“\n",
    "\n",
    "æœ¬æ¬¡æ•™ç¨‹ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨PAIå’ŒLLaMA Factoryæ¡†æ¶ï¼ŒåŸºäºè½»é‡åŒ–LoRAæ–¹æ³•å¾®è°ƒQwen3æ¨¡å‹ï¼Œä½¿å…¶èƒ½å¤Ÿè¿›è¡Œä¸­æ–‡é—®ç­”å’Œè§’è‰²æ‰®æ¼”ï¼ŒåŒæ—¶é€šè¿‡éªŒè¯é›†ROUGEåˆ†æ•°å’Œäººå·¥æµ‹è¯•éªŒè¯äº†å¾®è°ƒçš„æ•ˆæœã€‚åœ¨åç»­å®è·µä¸­ï¼Œå¯ä»¥ä½¿ç”¨å®é™…ä¸šåŠ¡æ•°æ®é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¾—åˆ°èƒ½å¤Ÿè§£å†³å®é™…ä¸šåŠ¡åœºæ™¯é—®é¢˜çš„æœ¬åœ°é¢†åŸŸå¤§æ¨¡å‹ã€‚"
   ]
  }
 ],
 "metadata": {
  "dsw_sample": {
   "buildId": "1175",
   "pipeline": "pai-dsw-examples-master"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "share": {
   "datetime": "2024-05-08T07:37:10.242Z",
   "image": {
    "name": "modelscope:1.14.0-pytorch2.1.2-gpu-py310-cu121-ubuntu22.04",
    "url": "dsw-registry-vpc.cn-hangzhou.cr.aliyuncs.com/pai/modelscope:1.14.0-pytorch2.1.2-gpu-py310-cu121-ubuntu22.04"
   },
   "instance": "dsw-d03c4949959b7041",
   "spec": {
    "id": "ecs.gn7i-c8g1.2xlarge",
    "type": "GPU"
   },
   "uid": "1157703270994901"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
