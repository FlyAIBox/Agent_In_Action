{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FlyAIBox/Agent_In_Action/blob/main/01-agent-llm-mcp/ASimpleAgentFramework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8WI3so-YCs-"
      },
      "source": [
        "# GAME框架：AI智能体设计架构\n",
        "\n",
        "本笔记实现了一个“可复用的智能体（Agent）最小框架”，用来演示如何基于 GAME 设计法将智能体拆分为四个核心部件，并以可插拔方式组织：\n",
        "\n",
        "- **G（Goals / Instructions）**：目标与指令。描述智能体要实现的结果，以及实现策略/规则。\n",
        "- **A（Actions）**：动作/工具。定义智能体可以调用的能力（如读取文件、列目录、结束会话等）。\n",
        "- **M（Memory）**：记忆。跨回合保留上下文（用户输入、助手决策、工具执行结果），支持后续回合继续推理。\n",
        "- **E（Environment）**：环境。动作在真实世界中的执行载体，负责真正“落地执行”动作并返回结果（含时间戳与错误信息）。\n",
        "\n",
        "本框架通过一个统一的 **Agent** 循环（Loop）把 G/A/M/E 串起来：\n",
        "1. 构造 Prompt（包含 Goals、可用 Actions 的函数调用Schema、Memory 历史）。\n",
        "2. 发送给 LLM，得到“选择的动作以及参数”（函数调用）。」\n",
        "3. 在 **Environment** 中执行该动作，得到结果（或错误）。\n",
        "4. 将决策与结果写入 **Memory**，进入下一轮。\n",
        "5. 如果动作为终止类动作（如 `terminate`），则结束循环。\n",
        "\n",
        "你可以把 **Actions** 看成“能力接口”，把 **Environment** 看成“执行实现”。这种解耦使得：\n",
        "- 你可以替换不同环境（本地、云端、GitHub Actions、容器等），而无需修改智能体决策逻辑；\n",
        "- 你可以更换一组 Actions（比如从文件工具换成 Web API 工具），而无需修改主循环；\n",
        "- 你可以替换/扩展 **AgentLanguage**（Prompt 格式与解析逻辑），以适配“函数调用/纯文本解析”等不同LLM交互方式。\n",
        "\n",
        "本笔记下半部分提供了一个最小示例：\n",
        "- 定义了 3 个动作：`list_project_files`、`read_project_file`、`terminate`\n",
        "- 目标：读取项目文件并在结束时输出 README 内容（示例运行环境为空目录时会直接终止）\n",
        "- 使用 OpenAI 调用 `gpt-4o`，但可轻松替换为任意 LLM 提供商\n",
        "\n",
        "### GAME 智能体业务流程\n",
        "```mermaid\n",
        "sequenceDiagram\n",
        "  participant U as 用户输入\n",
        "  participant G as Goals / 指令\n",
        "  participant M as Memory / 记忆\n",
        "  participant L as AgentLanguage / 提示与解析\n",
        "  participant A as Actions / 工具\n",
        "  participant E as Environment / 环境执行\n",
        "  participant LLM as 大语言模型\n",
        "\n",
        "  U->>G: 任务/指令\n",
        "  G->>L: 合并目标到 system 提示\n",
        "  M->>L: 历史对话（user/assistant/environment）\n",
        "  A->>L: 动作工具的 JSON Schema\n",
        "  L->>LLM: 构造 Prompt + tools 并请求\n",
        "  LLM-->>L: 工具调用/文本回复\n",
        "  L->>A: 解析调用 {tool, args}\n",
        "  A->>E: 在环境中执行对应函数\n",
        "  E-->>M: 记录执行结果（标准化 + 时间戳）\n",
        "  L->>M: 记录助手决策（response）\n",
        "  M->>L: 新一轮上下文（若未终止）\n",
        "  note over A: 若动作为终止型（terminal）则结束循环\n",
        "```\n",
        "\n",
        "通过阅读与运行本笔记，你将能掌握：\n",
        "- 如何将智能体设计（GAME）直接映射为代码结构；\n",
        "- 如何注册工具、格式化 Prompt、解析 LLM 工具调用并在环境中执行；\n",
        "- 如何使用记忆把“决策 + 结果”闭环起来，形成稳健的 Agent Loop。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLUK5fflYeVS",
        "outputId": "b901bcad-dffe-4362-a545-7543cf855ecd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Collecting openai==1.107.0',\n",
              " '  Downloading openai-1.107.0-py3-none-any.whl.metadata (29 kB)',\n",
              " 'Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (4.11.0)',\n",
              " 'Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (1.9.0)',\n",
              " 'Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (0.28.1)',\n",
              " 'Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (0.11.1)',\n",
              " 'Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (2.11.10)',\n",
              " 'Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (1.3.1)',\n",
              " 'Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (4.67.1)',\n",
              " 'Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai==1.107.0) (4.15.0)',\n",
              " 'Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai==1.107.0) (3.11)',\n",
              " 'Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.107.0) (2025.10.5)',\n",
              " 'Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai==1.107.0) (1.0.9)',\n",
              " 'Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.107.0) (0.16.0)',\n",
              " 'Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.107.0) (0.7.0)',\n",
              " 'Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.107.0) (2.33.2)',\n",
              " 'Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai==1.107.0) (0.4.2)',\n",
              " 'Downloading openai-1.107.0-py3-none-any.whl (950 kB)',\n",
              " '\\x1b[?25l   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/951.0 kB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━\\x1b[0m \\x1b[32m757.8/951.0 kB\\x1b[0m \\x1b[31m22.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K   \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m951.0/951.0 kB\\x1b[0m \\x1b[31m16.8 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hInstalling collected packages: openai',\n",
              " '  Attempting uninstall: openai',\n",
              " '    Found existing installation: openai 1.109.1',\n",
              " '    Uninstalling openai-1.109.1:',\n",
              " '      Successfully uninstalled openai-1.109.1',\n",
              " 'Successfully installed openai-1.107.0']"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 安装必要的依赖包\n",
        "!!pip install openai==1.107.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEYrzG2vB8Ip",
        "outputId": "cbf03795-01f5-4e0a-bc69-a722efcda383"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n",
            "OPENAI_BASE_URL: ··········\n"
          ]
        }
      ],
      "source": [
        "# 导入必要的模块\n",
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    \"\"\"\n",
        "    设置环境变量的辅助函数\n",
        "\n",
        "    参数:\n",
        "        var (str): 要设置的环境变量名称\n",
        "\n",
        "    功能:\n",
        "        - 检查环境变量是否已存在\n",
        "        - 如果不存在，则提示用户输入并设置\n",
        "    \"\"\"\n",
        "    if not os.environ.get(var):  # 检查环境变量是否已设置\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")  # 安全地获取用户输入\n",
        "\n",
        "# 设置 OpenAI API 密钥\n",
        "# 这是使用 OpenAI 模型所必需的\n",
        "_set_env(\"OPENAI_API_KEY\")\n",
        "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
        "_set_env(\"OPENAI_BASE_URL\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mwe2eeOQB0cC"
      },
      "outputs": [],
      "source": [
        "# =============================== 核心框架：导入与类型定义 ===============================\n",
        "# 说明：以下代码实现了一个最小可复用的智能体框架（面向函数调用工具）。\n",
        "# - 不修改任何原有逻辑，仅通过中文注释解释设计意图与用法。\n",
        "# - 关键模块：Prompt 数据结构、LLM 响应函数、Goal/Action/ActionRegistry、Memory、Environment、AgentLanguage、Agent。\n",
        "\n",
        "import json\n",
        "import time\n",
        "import traceback\n",
        "from openai import OpenAI # 用于调用OpenAI API\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Callable, Dict, Any\n",
        "\n",
        "\n",
        "# 大语言模型\n",
        "client=OpenAI(\n",
        "    base_url=os.environ['OPENAI_BASE_URL'],\n",
        "    api_key=os.environ['OPENAI_API_KEY']\n",
        ")\n",
        "\n",
        "# Prompt：封装要发给 LLM 的消息与工具定义\n",
        "# - messages：对话上下文（系统/用户/助手三类）\n",
        "# - tools：工具（函数）调用的 JSON Schema 描述（让 LLM 能“看见”可用的动作）\n",
        "# - metadata：元数据（可选扩展，用 dict 保存）\n",
        "@dataclass\n",
        "class Prompt:\n",
        "    messages: List[Dict] = field(default_factory=list)\n",
        "    tools: List[Dict] = field(default_factory=list)\n",
        "    metadata: dict = field(default_factory=dict)\n",
        "\n",
        "\n",
        "# generate_response：统一的 LLM 调用入口\n",
        "# - 入参是 Prompt，内部自动根据是否提供 tools 来决定是否启用函数调用能力\n",
        "# - 目标：把模型提供商与主循环解耦；将来切换模型时无需改 Agent 逻辑\n",
        "# - 返回：\n",
        "#   * 无工具时：直接返回助手文本\n",
        "#   * 有工具时：优先解析 tool_calls（并转为 {tool, args} 的 JSON 字符串）\n",
        "#               若无工具调用，则退化为普通文本回复\n",
        "def generate_response(prompt: Prompt) -> str:\n",
        "    \"\"\"调用大语言模型（LLM）生成响应：\n",
        "    - 当未提供 tools（函数调用能力）时，作为普通对话返回文本\n",
        "    - 当提供 tools 时，优先解析函数调用的结构化结果；若无函数调用则退化为普通文本\n",
        "    \"\"\"\n",
        "\n",
        "    messages = prompt.messages\n",
        "    tools = prompt.tools\n",
        "\n",
        "    result = None\n",
        "\n",
        "    if not tools:\n",
        "        # 无工具：普通对话\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",   # 指定使用的模型\n",
        "            messages=messages,  # 发送消息历史\n",
        "            max_tokens=1024   # 限制响应长度\n",
        "        )\n",
        "        result = response.choices[0].message.content\n",
        "    else:\n",
        "        # 有工具：提示模型按函数调用格式返回 tool_calls\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "\n",
        "        if response.choices[0].message.tool_calls:\n",
        "            # 这里仅取第一个工具调用作为最小可运行演示\n",
        "            tool = response.choices[0].message.tool_calls[0]\n",
        "            result = {\n",
        "                \"tool\": tool.function.name,\n",
        "                \"args\": json.loads(tool.function.arguments),\n",
        "            }\n",
        "            # 将 dict 序列化为字符串，便于统一处理与存入记忆\n",
        "            result = json.dumps(result)\n",
        "        else:\n",
        "            # 即使提供了 tools，也可能返回纯文本（例如模型策略判断不调用工具）\n",
        "            result = response.choices[0].message.content\n",
        "\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# Goal：目标对象\n",
        "# - priority：目标优先级（便于排序/裁剪）\n",
        "# - name/description：目标名称和详细说明（同时涵盖“要做什么/如何做”）\n",
        "# 使用 @dataclass 装饰器定义 Goal 为一个不可变的数据类（frozen=True），这意味着其实例一旦创建，其属性值就不能被修改，有助于保证数据安全和可靠性。\n",
        "@dataclass(frozen=True)\n",
        "class Goal:\n",
        "    priority: int\n",
        "    name: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "# Action：动作/工具的抽象\n",
        "# - name：动作名（作为工具名暴露给 LLM）\n",
        "# - function：实际执行的 Python 函数\n",
        "# - description：工具说明，帮助 LLM 选择正确工具\n",
        "# - parameters：JSON Schema（决定 LLM 该如何拼好参数）\n",
        "# - terminal：是否为“终止型”动作（被选中后终止主循环）\n",
        "class Action:\n",
        "    def __init__(self,\n",
        "                 name: str,\n",
        "                 function: Callable,\n",
        "                 description: str,\n",
        "                 parameters: Dict,\n",
        "                 terminal: bool = False):\n",
        "        self.name = name\n",
        "        self.function = function\n",
        "        self.description = description\n",
        "        self.terminal = terminal\n",
        "        self.parameters = parameters\n",
        "\n",
        "    def execute(self, **args) -> Any:\n",
        "        \"\"\"执行该动作所绑定的底层函数，参数通过关键字形式解包传入\"\"\"\n",
        "        # 解包参数并调用底层实现函数\n",
        "        return self.function(**args)\n",
        "\n",
        "\n",
        "# ActionRegistry：动作/工具注册表\n",
        "# - 负责集中管理动作/工具对象，支持按名称检索与批量导出供 AgentLanguage 生成工具Schema\n",
        "class ActionRegistry:\n",
        "    def __init__(self):\n",
        "        self.actions = {}\n",
        "\n",
        "    def register(self, action: Action):\n",
        "        self.actions[action.name] = action\n",
        "\n",
        "    def get_action(self, name: str) -> [Action, None]:\n",
        "        return self.actions.get(name, None)\n",
        "\n",
        "    def get_actions(self) -> List[Action]:\n",
        "        \"\"\"获取所有已注册的动作，按注册顺序返回列表\"\"\"\n",
        "        return list(self.actions.values())\n",
        "\n",
        "\n",
        "# Memory：回合记忆\n",
        "# - items：统一存储“用户/助手/环境”等事件，形成对话历史\n",
        "# - 通过 get_memories 提供最近N条消息给提示构造使用\n",
        "# - 通过 copy_without_system_memories 可过滤掉系统消息（某些场景需要）\n",
        "class Memory:\n",
        "    def __init__(self):\n",
        "        self.items = []  # Basic conversation histor\n",
        "\n",
        "    def add_memory(self, memory: dict):\n",
        "        \"\"\"将一条记忆事件追加到工作记忆，用于后续提示词构造与推理\"\"\"\n",
        "        self.items.append(memory)\n",
        "\n",
        "    def get_memories(self, limit: int = None) -> List[Dict]:\n",
        "        \"\"\"获取用于提示词的对话历史；可通过 limit 限制条数以控制上下文长度\"\"\"\n",
        "        return self.items[:limit]\n",
        "\n",
        "    def copy_without_system_memories(self):\n",
        "        \"\"\"返回一份不包含系统类型（type==system）记忆的副本，用于部分提示场景\"\"\"\n",
        "        filtered_items = [m for m in self.items if m[\"type\"] != \"system\"]\n",
        "        memory = Memory()\n",
        "        memory.items = filtered_items\n",
        "        return memory\n",
        "\n",
        "\n",
        "# Environment：环境层（动作的真实执行者）\n",
        "# - execute_action：捕获执行异常，统一返回结构（是否执行成功/错误/traceback/时间戳）\n",
        "# - format_result：为成功结果补充元数据（时间戳），便于记录与日志化\n",
        "class Environment:\n",
        "    def execute_action(self, action: Action, args: dict) -> dict:\n",
        "        \"\"\"执行指定动作并返回标准化结果；捕获异常并提供错误与追踪信息\"\"\"\n",
        "        try:\n",
        "            result = action.execute(**args)\n",
        "            return self.format_result(result)\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"tool_executed\": False,\n",
        "                \"error\": str(e),\n",
        "                \"traceback\": traceback.format_exc()\n",
        "            }\n",
        "\n",
        "    def format_result(self, result: Any) -> dict:\n",
        "        \"\"\"为执行结果补充元数据（如时间戳）并统一为标准结构\"\"\"\n",
        "        return {\n",
        "            \"tool_executed\": True,\n",
        "            \"result\": result,\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\")\n",
        "        }\n",
        "\n",
        "\n",
        "# AgentLanguage：语言适配层\n",
        "# - 负责把（Goals/Actions/Memory）格式化为 LLM 需要的 Prompt\n",
        "# - 负责从 LLM 的原始输出中解析出“要调用的工具与参数”\n",
        "class AgentLanguage:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        raise NotImplementedError(\"Subclasses must implement this method\")\n",
        "\n",
        "\n",
        "\n",
        "# AgentFunctionCallingActionLanguage：基于“函数调用”范式的语言适配实现\n",
        "# - 将 Goals 拼接为 system 消息\n",
        "# - 将 Memory 规范化映射为 user/assistant 消息\n",
        "# - 将 Actions 转换为符合 OpenAI 函数调用的 tools Schema\n",
        "class AgentFunctionCallingActionLanguage(AgentLanguage):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def format_goals(self, goals: List[Goal]) -> List:\n",
        "        # 把所有目标拼接为一个 system 消息，便于集中表达“要做什么/如何做”\n",
        "        sep = \"\\n-------------------\\n\"\n",
        "        goal_instructions = \"\\n\\n\".join([f\"{goal.name}:{sep}{goal.description}{sep}\" for goal in goals])\n",
        "        return [\n",
        "            {\"role\": \"system\", \"content\": goal_instructions}\n",
        "        ]\n",
        "\n",
        "    def format_memory(self, memory: Memory) -> List:\n",
        "        \"\"\"将 Memory 转换为对话消息格式，供 LLM 上下文使用\"\"\"\n",
        "        # 记忆格式化策略：\n",
        "        # - environment 的输出也作为 assistant 角色加入（让模型能“看到”工具执行结果）\n",
        "        # - user/assistant 原样映射\n",
        "        items = memory.get_memories()\n",
        "        mapped_items = []\n",
        "        for item in items:\n",
        "\n",
        "            content = item.get(\"content\", None)\n",
        "            if not content:\n",
        "                content = json.dumps(item, indent=4)\n",
        "\n",
        "            if item[\"type\"] == \"assistant\":\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            elif item[\"type\"] == \"environment\":\n",
        "                mapped_items.append({\"role\": \"assistant\", \"content\": content})\n",
        "            else:\n",
        "                mapped_items.append({\"role\": \"user\", \"content\": content})\n",
        "\n",
        "        return mapped_items\n",
        "\n",
        "    def format_actions(self, actions: List[Action]) -> [List,List]:\n",
        "        \"\"\"将已注册的动作转换为 OpenAI 函数调用所需的 tools Schema\"\"\"\n",
        "\n",
        "        # 将注册的 Action 转为 OpenAI 函数调用工具的 Schema 数组\n",
        "        tools = [\n",
        "            {\n",
        "                \"type\": \"function\",\n",
        "                \"function\": {\n",
        "                    \"name\": action.name,\n",
        "                    # 描述过长可能无效，限制到 1024 字符\n",
        "                    \"description\": action.description[:1024],\n",
        "                    \"parameters\": action.parameters,\n",
        "                },\n",
        "            } for action in actions\n",
        "        ]\n",
        "\n",
        "        return tools\n",
        "\n",
        "    def construct_prompt(self,\n",
        "                         actions: List[Action],\n",
        "                         environment: Environment,\n",
        "                         goals: List[Goal],\n",
        "                         memory: Memory) -> Prompt:\n",
        "\n",
        "        # 构造最终 Prompt：Goals（system）+ Memory（历史消息）+ Tools（函数Schema）\n",
        "        prompt = []\n",
        "        prompt += self.format_goals(goals)\n",
        "        prompt += self.format_memory(memory)\n",
        "\n",
        "        tools = self.format_actions(actions)\n",
        "\n",
        "        return Prompt(messages=prompt, tools=tools)\n",
        "\n",
        "    def adapt_prompt_after_parsing_error(self,\n",
        "                                         prompt: Prompt,\n",
        "                                         response: str,\n",
        "                                         traceback: str,\n",
        "                                         error: Any,\n",
        "                                         retries_left: int) -> Prompt:\n",
        "        # 解析失败后的“自适应 Prompt”策略（此处保留扩展点，演示版不做修改）\n",
        "        return prompt\n",
        "\n",
        "    def parse_response(self, response: str) -> dict:\n",
        "        \"\"\"将 LLM 的响应解析为结构化格式（优先尝试 JSON 解析，失败则回退为终止工具）\"\"\"\n",
        "\n",
        "        # 期望 LLM 返回 JSON 字符串：{\"tool\": 工具名, \"args\": {...}}\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "\n",
        "        except Exception as e:\n",
        "            # 若无法解析，则将内容作为 message 交给终止工具，友好退出\n",
        "            return {\n",
        "                \"tool\": \"terminate\",\n",
        "                \"args\": {\"message\":response}\n",
        "            }\n",
        "\n",
        "\n",
        "# Agent：智能体主循环\n",
        "# - 维护并协调 G/A/M/E（目标/动作/记忆/环境）\n",
        "# - 统一的 prompt 构造、响应解析、动作执行、记忆更新、终止判断\n",
        "class Agent:\n",
        "    def __init__(self,\n",
        "                 goals: List[Goal],\n",
        "                 agent_language: AgentLanguage,\n",
        "                 action_registry: ActionRegistry,\n",
        "                 generate_response: Callable[[Prompt], str],\n",
        "                 environment: Environment):\n",
        "        \"\"\"\n",
        "        使用核心的 GAME 组件初始化智能体：\n",
        "        - goals：目标与指令集合\n",
        "        - agent_language：语言适配层（提示词构造与解析）\n",
        "        - action_registry：动作注册表（可调用工具）\n",
        "        - generate_response：LLM 调用函数\n",
        "        - environment：动作执行环境\n",
        "        \"\"\"\n",
        "        self.goals = goals\n",
        "        self.generate_response = generate_response\n",
        "        self.agent_language = agent_language\n",
        "        self.actions = action_registry\n",
        "        self.environment = environment\n",
        "\n",
        "    def construct_prompt(self, goals: List[Goal], memory: Memory, actions: ActionRegistry) -> Prompt:\n",
        "        \"\"\"基于当前目标、记忆与动作集合构造提示词（Prompt）\"\"\"\n",
        "        return self.agent_language.construct_prompt(\n",
        "            actions=actions.get_actions(),\n",
        "            environment=self.environment,\n",
        "            goals=goals,\n",
        "            memory=memory\n",
        "        )\n",
        "\n",
        "    def get_action(self, response):\n",
        "        # 解析 LLM 的返回，得到动作名与参数（invocation）\n",
        "        invocation = self.agent_language.parse_response(response)\n",
        "        action = self.actions.get_action(invocation[\"tool\"])\n",
        "        return action, invocation\n",
        "\n",
        "    def should_terminate(self, response: str) -> bool:\n",
        "        # 若当前选择的动作被标记为 terminal，则结束主循环\n",
        "        action_def, _ = self.get_action(response)\n",
        "        return action_def.terminal\n",
        "\n",
        "    def set_current_task(self, memory: Memory, task: str):\n",
        "        # 将用户输入写入记忆，作为本轮起始任务语境\n",
        "        memory.add_memory({\"type\": \"user\", \"content\": task})\n",
        "\n",
        "    def update_memory(self, memory: Memory, response: str, result: dict):\n",
        "        \"\"\"\n",
        "        使用“决策 + 执行结果”更新记忆：\n",
        "        - 将助手的决策（response）作为 assistant 事件存入\n",
        "        - 将环境执行结果（result）序列化为 JSON，作为 environment 事件存入\n",
        "        \"\"\"\n",
        "        # 统一把“助手的决策（response）”与“环境执行结果（result）”写入记忆\n",
        "        new_memories = [\n",
        "            {\"type\": \"assistant\", \"content\": response},\n",
        "            {\"type\": \"environment\", \"content\": json.dumps(result)}\n",
        "        ]\n",
        "        for m in new_memories:\n",
        "            memory.add_memory(m)\n",
        "\n",
        "    def prompt_llm_for_action(self, full_prompt: Prompt) -> str:\n",
        "        # 将 Prompt 发送给 LLM，得到“下一步动作/或文本回复”\n",
        "        response = self.generate_response(full_prompt)\n",
        "        return response\n",
        "\n",
        "    def run(self, user_input: str, memory=None, max_iterations: int = 50) -> Memory:\n",
        "        \"\"\"\n",
        "        执行该智能体的 GAME 主循环，可设置最大迭代次数：\n",
        "        - 每轮：构造 Prompt -> 让 LLM 决策 -> 解析动作 -> 环境执行 -> 写回记忆 -> 终止判断\n",
        "        \"\"\"\n",
        "        # 初始化记忆并写入用户任务\n",
        "        memory = memory or Memory()\n",
        "        self.set_current_task(memory, user_input)\n",
        "\n",
        "        for _ in range(max_iterations):\n",
        "            # 1) 用当前 Goals/Actions/Memory 构造 Prompt\n",
        "            prompt = self.construct_prompt(self.goals, memory, self.actions)\n",
        "\n",
        "            print(\"Agent thinking...\")\n",
        "            # 2) 发送给 LLM，得到“将要调用的动作及其参数”或普通文本\n",
        "            response = self.prompt_llm_for_action(prompt)\n",
        "            print(f\"Agent Decision: {response}\")\n",
        "\n",
        "            # 3) 解析动作与参数\n",
        "            action, invocation = self.get_action(response)\n",
        "\n",
        "            # 4) 在环境中真实执行动作\n",
        "            result = self.environment.execute_action(action, invocation[\"args\"])\n",
        "            print(f\"Action Result: {result}\")\n",
        "\n",
        "            # 5) 将“决策 + 结果”写回记忆，形成闭环\n",
        "            self.update_memory(memory, response, result)\n",
        "\n",
        "            # 6) 终止判断：如果动作为终止型，则跳出循环\n",
        "            if self.should_terminate(response):\n",
        "                break\n",
        "\n",
        "        return memory\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkB14e1duUxg"
      },
      "source": [
        "### OpenAI 函数调用（tools Schema）详解\n",
        "\n",
        "在 Chat Completions API 中，通过 `tools` 字段向模型暴露可调用的函数（工具）。每个工具定义如下：\n",
        "\n",
        "- `type`: 固定为 `function`\n",
        "- `function`:\n",
        "  - `name` (string): 工具名称（小写、下划线风格更稳妥，长度 ≤ 64 常见做法）\n",
        "  - `description` (string): 工具用途的自然语言描述（有助于模型选择正确工具，建议简洁清晰）\n",
        "  - `parameters` (object): 满足 JSON Schema Draft-07 的参数定义，用于指导模型正确组装入参\n",
        "    - `type`: 通常为 `object`\n",
        "    - `properties`: 各字段的类型与描述\n",
        "    - `required`: 必填字段名列表\n",
        "    - `additionalProperties`: 是否允许未声明字段（建议 `false` 以提高鲁棒性）\n",
        "\n",
        "示例（与本笔记 `Action` 转换保持一致）：\n",
        "```json\n",
        "{\n",
        "  \"type\": \"function\",\n",
        "  \"function\": {\n",
        "    \"name\": \"read_project_file\",\n",
        "    \"description\": \"Reads a file from the project.\",\n",
        "    \"parameters\": {\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {\n",
        "        \"name\": { \"type\": \"string\", \"description\": \"The file path to read\" }\n",
        "      },\n",
        "      \"required\": [\"name\"],\n",
        "      \"additionalProperties\": false\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "模型返回时会在 `message.tool_calls` 中给出调用的 `function.name` 与 `function.arguments`（JSON 字符串）。你的代码需：\n",
        "- 解析 `arguments`（`json.loads`）\n",
        "- 路由到本地实现函数执行\n",
        "- 将执行结果写回对话历史（便于下一轮推理）\n",
        "\n",
        "可选高级配置（按实际接口版本支持情况使用）：\n",
        "- `tool_choice`: 强制使用某个工具或允许模型自由选择\n",
        "- `parallel_tool_calls`: 是否允许并行调用（若可用）\n",
        "- `response_format`: 强制 JSON 输出等（如需要结构化）\n",
        "\n",
        "参考文档（官方）：\n",
        "- OpenAI 工具/函数调用总览（Chat Completions）: [Function calling & tools](https://platform.openai.com/docs/guides/function-calling)\n",
        "- JSON Schema 规范（参考）: [JSON Schema](https://json-schema.org/)\n",
        "- Chat Completions 消息与工具调用字段说明: [Chat Completions API](https://platform.openai.com/docs/api-reference/chat)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PC3ncxezoJC",
        "outputId": "85cda07d-82e5-4d4e-e82a-b3c5f159ab1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"list_project_files\", \"args\": {}}\n",
            "Action Result: {'tool_executed': True, 'result': ['mcp_client_deepseek.py', 'weather_server.py'], 'timestamp': '2025-10-30T06:17:08+0000'}\n",
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp_client_deepseek.py\"}}\n",
            "Action Result: {'tool_executed': True, 'result': '\"\"\"\\n# 集成 DeepSeek 的 MCP 客户端\\n#参考官方案例：https://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py\\n\\n\\n# 本模块实现了一个模型上下文协议（MCP）客户端，该客户端使用 DeepSeek 的 API\\n# 来处理查询并与 MCP 工具进行交互。它演示了如何：\\n# 1. 连接到 MCP 服务器\\n# 2. 使用 DeepSeek 的 API 来处理查询\\n# 3. 处理工具调用和响应\\n# 4. 维护一个交互式聊天循环\\n\\n# 所需环境变量：\\n# DEEPSEEK_API_KEY：DeepSeek API 密钥 (格式：sk-xxxx...)\\n# DEEPSEEK_BASE_URL：DeepSeek API 基础 URL (https://api.deepseek.com)\\n# DEEPSEEK_MODEL：DeepSeek 模型名称 (例如 deepseek-chat)\\n\\nAuthor: FlyAIBox\\nDate: 2025.10.11\\n\"\"\"\\n\\nimport json\\nimport asyncio\\nimport logging\\nimport os\\nfrom typing import Optional, Dict, Any, List, Tuple\\nfrom contextlib import AsyncExitStack\\n\\nfrom openai import OpenAI\\nfrom dotenv import load_dotenv\\n\\nfrom mcp import ClientSession, StdioServerParameters\\nfrom mcp.client.stdio import stdio_client\\n\\n# 配置日志记录\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format=\"%(asctime)s - %(levelname)s - %(message)s\"\\n)\\nlogger = logging.getLogger(__name__)\\n\\nload_dotenv()\\n\\n\\nclass Configuration:\\n    \"\"\"配置管理类，负责管理和验证环境变量\"\"\"\\n    \\n    def __init__(self) -> None:\\n        \"\"\"初始化配置并加载环境变量\"\"\"\\n        self.load_env()\\n        self._validate_env()\\n        \\n    @staticmethod\\n    def load_env() -> None:\\n        \"\"\"从.env文件加载环境变量\"\"\"\\n        load_dotenv()\\n        \\n    def _validate_env(self) -> None:\\n        \"\"\"验证必需的环境变量是否存在\"\"\"\\n        required_vars = [\"DEEPSEEK_API_KEY\"]\\n        missing_vars = [var for var in required_vars if not os.getenv(var)]\\n        if missing_vars:\\n            raise ValueError(f\"缺少必需的环境变量: {\\', \\'.join(missing_vars)}\")\\n    \\n    @property\\n    def api_key(self) -> str:\\n        \"\"\"获取 DeepSeek API 密钥\"\"\"\\n        return os.getenv(\"DEEPSEEK_API_KEY\", \"\")\\n    \\n    @property\\n    def base_url(self) -> str:\\n        \"\"\"获取 DeepSeek API 基础 URL\"\"\"\\n        return os.getenv(\"DEEPSEEK_BASE_URL\", \"https://api.deepseek.com\")\\n    \\n    @property\\n    def model(self) -> str:\\n        \"\"\"获取 DeepSeek 模型名称\"\"\"\\n        return os.getenv(\"DEEPSEEK_MODEL\", \"deepseek-chat\")\\n\\nclass Tool:\\n    \"\"\"MCP 工具类，表示一个具有属性的工具\"\"\"\\n    \\n    def __init__(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:\\n        \"\"\"\\n        初始化工具\\n        \\n        Args:\\n            name: 工具名称\\n            description: 工具描述\\n            input_schema: 输入参数模式\\n        \"\"\"\\n        self.name = name\\n        self.description = description\\n        self.input_schema = input_schema\\n        \\n    def to_openai_format(self) -> Dict[str, Any]:\\n        \"\"\"将工具转换为 OpenAI API 格式\"\"\"\\n        return {\\n            \"type\": \"function\",\\n            \"function\": {\\n                \"name\": self.name,\\n                \"description\": self.description,\\n                \"parameters\": self.input_schema\\n            }\\n        }\\n\\nclass MCPServer:\\n    \"\"\"MCP 服务器管理类，处理服务器连接和工具执行\"\"\"\\n    \\n    def __init__(self, server_path: str) -> None:\\n        \"\"\"\\n        初始化服务器管理器\\n        \\n        Args:\\n            server_path: 服务器脚本路径\\n        \"\"\"\\n        self.server_path = server_path\\n        self.session: Optional[ClientSession] = None\\n        self.exit_stack = AsyncExitStack()\\n        self._cleanup_lock = asyncio.Lock()\\n        \\n    async def initialize(self) -> None:\\n        \"\"\"初始化服务器连接，包含重试机制\"\"\"\\n        max_retries = 3\\n        retry_delay = 1.0\\n        \\n        for attempt in range(max_retries):\\n            try:\\n                if not os.path.exists(self.server_path):\\n                    raise FileNotFoundError(f\"找不到服务器文件: {self.server_path}\")\\n                \\n                server_params = StdioServerParameters(\\n                    command=\\'python\\',\\n                    args=[self.server_path],\\n                    env=None\\n                )\\n                \\n                stdio_transport = await self.exit_stack.enter_async_context(\\n                    stdio_client(server_params)\\n                )\\n                stdio, write = stdio_transport\\n                \\n                self.session = await self.exit_stack.enter_async_context(\\n                    ClientSession(stdio, write)\\n                )\\n                await self.session.initialize()\\n                logger.info(\"成功连接到 MCP 服务器\")\\n                break\\n                \\n            except Exception as e:\\n                logger.error(f\"第 {attempt + 1}/{max_retries} 次尝试失败: {str(e)}\")\\n                if attempt < max_retries - 1:\\n                    await asyncio.sleep(retry_delay)\\n                else:\\n                    raise\\n                    \\n    async def list_tools(self) -> List[Tool]:\\n        \"\"\"获取服务器提供的可用工具列表\"\"\"\\n        if not self.session:\\n            raise RuntimeError(\"服务器未初始化\")\\n            \\n        response = await self.session.list_tools()\\n        return [\\n            Tool(tool.name, tool.description, tool.inputSchema)\\n            for tool in response.tools\\n        ]\\n        \\n    async def execute_tool(\\n        self,\\n        tool_name: str,\\n        arguments: Dict[str, Any],\\n        retries: int = 2,\\n        delay: float = 1.0\\n    ) -> Any:\\n        \"\"\"\\n        执行工具，包含重试机制\\n        \\n        Args:\\n            tool_name: 工具名称\\n            arguments: 工具参数\\n            retries: 重试次数\\n            delay: 重试延迟时间（秒）\\n            \\n        Returns:\\n            工具执行结果\\n        \"\"\"\\n        if not self.session:\\n            raise RuntimeError(\"服务器未初始化\")\\n            \\n        for attempt in range(retries):\\n            try:\\n                logger.info(f\"执行工具 {tool_name}，参数: {arguments}\")\\n                result = await self.session.call_tool(tool_name, arguments)\\n                return result\\n                \\n            except Exception as e:\\n                logger.error(f\"工具执行失败 (第 {attempt + 1}/{retries} 次尝试): {str(e)}\")\\n                if attempt < retries - 1:\\n                    await asyncio.sleep(delay)\\n                else:\\n                    raise\\n                    \\n    async def cleanup(self) -> None:\\n        \"\"\"清理服务器资源\"\"\"\\n        async with self._cleanup_lock:\\n            try:\\n                await self.exit_stack.aclose()\\n                self.session = None\\n                logger.info(\"服务器资源清理完成\")\\n            except Exception as e:\\n                logger.error(f\"清理过程中出错: {str(e)}\")\\n\\nclass MCPClient:\\n    \"\"\"MCP 客户端实现，集成了 DeepSeek API\"\"\"\\n    \\n    def __init__(self, config: Configuration) -> None:\\n        \"\"\"\\n        初始化 MCP 客户端\\n        \\n        Args:\\n            config: 配置对象\\n        \"\"\"\\n        self.config = config\\n        self.server: Optional[MCPServer] = None\\n        self.client = OpenAI(\\n            api_key=config.api_key,\\n            base_url=config.base_url\\n        )\\n        \\n    async def initialize(self) -> None:\\n        \"\"\"初始化客户端并连接到服务器\"\"\"\\n        server_path = os.path.join(\\n            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\n            \"server\",\\n            \"weather_server.py\"\\n        )\\n        self.server = MCPServer(server_path)\\n        await self.server.initialize()\\n        \\n\\n    async def process_query(self, query: str) -> str:\\n        \"\"\"\\n        处理用户查询，集成工具调用，支持多轮工具交互\\n\\n        Args:\\n            query: 用户查询字符串\\n\\n        Returns:\\n            处理后的响应结果\\n        \"\"\"\\n        if not self.server:\\n            raise RuntimeError(\"客户端未初始化\")\\n\\n        system_prompt = (\\n            \"You are a helpful assistant specializing in weather information.\\\\n\"\\n            \"You have access to the MCP Weather Server tool with the following functions:\\\\n\"\\n            \"- get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\\\n\"\\n            \"- get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\\\n\"\\n            \"\\\\n\"\\n            \"Core Instructions:\\\\n\"\\n            \"1.  **Carefully analyze the user\\'s request**: Understand all components of the user\\'s query. Determine if the user needs weather warning information, weather forecast information, or both.\\\\n\"\\n            \"2.  **Identify Information Needs**:\\\\n\"\\n            \"    * If the user only asks for warnings (e.g., \\\\\"Are there any warnings in Beijing?\\\\\"), only use `get_weather_warning`.\\\\n\"\\n            \"    * If the user only asks for the forecast (e.g., \\\\\"What\\'s the weather like in Beijing tomorrow?\\\\\"), only use `get_daily_forecast`.\\\\n\"\\n            \"    * **If the user\\'s question includes multiple aspects**, such as asking about **warning status** and also asking **if it\\'s suitable for a certain activity** (which implies a query about future weather, like \\\\\"Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\\\\"), you need to **call both tools sequentially**.\\\\n\"\\n            \"3.  **Call Tools as Needed**:\\\\n\"\\n            \"    * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\\\n\"\\n            \"    * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\\\\"weekend\\\\\", \\\\\"next three days\\\\\", \\\\\"next week\\\\\") or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\\\\"last week\\\\\" or \\\\\"recently\\\\\", interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\\\\"Is it suitable for outdoor activities?\\\\\", you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\\\n\"\\n            \"    * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\\\n\"\\n            \"4.  **Information Integration and Response**:\\\\n\"\\n            \"    * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\\\n\"\\n            \"    * **Completely answer the user\\'s question**: Ensure you answer all parts of the user\\'s query.\\\\n\"\\n            \"    * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\\\\"Currently there are no high temperature warnings, but it\\'s expected to rain this weekend, so it\\'s not very suitable for outdoor activities,\\\\\" or \\\\\"It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\\\\").\\\\n\"\\n            \"5.  **Tool Usage Details**:\\\\n\"\\n            \"    * When using the tools, retain the full context of the user\\'s original question.\\\\n\"\\n            \"    * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\\\\"3 PM\\\\\") into the search query or your response.\\\\n\"\\n            \"    * When city information is needed, if the user provides a city name (e.g., \\\\\"Beijing\\\\\"), use the corresponding `city_id` (e.g., Beijing\\'s city_id might be \\'101010100\\').\\\\n\"\\n        )\\n\\n        messages = [\\n            {\"role\": \"system\", \"content\": system_prompt},\\n            {\"role\": \"user\", \"content\": query}\\n        ]\\n\\n        tools = await self.server.list_tools()\\n        available_tools = [tool.to_openai_format() for tool in tools]\\n\\n        max_tool_turns = 5 # 设置一个最大工具调用回合数，防止无限循环\\n\\n        for _ in range(max_tool_turns):\\n            try:\\n                response = self.client.chat.completions.create(\\n                    model=self.config.model,\\n                    messages=messages,\\n                    tools=available_tools,\\n                    tool_choice=\"auto\" # 允许模型决定是否调用工具\\n                )\\n\\n                content = response.choices[0].message\\n                finish_reason = response.choices[0].finish_reason\\n\\n                if finish_reason == \"stop\":\\n                    # 模型生成了最终的文本回复\\n                    messages.append({\"role\": \"assistant\", \"content\": content.content})\\n                    return content.content\\n                elif finish_reason == \"tool_calls\":\\n                    # 模型预测了工具调用\\n                    messages.append(content.model_dump()) # 将 assistant tool_calls 消息添加到历史\\n\\n                    tool_outputs = []\\n                    for tool_call in content.tool_calls:\\n                        tool_name = tool_call.function.name\\n                        tool_args = json.loads(tool_call.function.arguments)\\n\\n                        try:\\n                            # 执行工具\\n                            result = await self.server.execute_tool(tool_name, tool_args)\\n                            logger.info(f\"工具执行完成: {tool_name}\")\\n                            # 将工具结果添加到 tool_outputs 列表\\n                            tool_outputs.append({\\n                                \"tool_call_id\": tool_call.id,\\n                                # MCP tool results have \\'content\\' which is a list of Parts\\n                                # Assuming text content is the first part\\n                                \"output\": result.content[0].text\\n                            })\\n                        except Exception as tool_exec_error:\\n                            logger.error(f\"工具执行失败 {tool_name}: {str(tool_exec_error)}\")\\n                            # 如果工具执行失败，也需要向模型反馈失败信息\\n                            tool_outputs.append({\\n                                \"tool_call_id\": tool_call.id,\\n                                \"output\": f\"Error executing tool {tool_name}: {str(tool_exec_error)}\"\\n                            })\\n\\n                    # 将所有工具结果作为 tool 消息添加到历史\\n                    # 注意：OpenAI API 要求每个 tool_call_id 对应一个 tool 消息\\n                    # 如果模型预测了多个工具调用，这里需要为每个结果添加 tool 消息\\n                    for output in tool_outputs:\\n                        messages.append({\\n                            \"role\": \"tool\",\\n                            \"content\": output[\"output\"],\\n                            \"tool_call_id\": output[\"tool_call_id\"],\\n                        })\\n\\n                    # 继续下一轮循环，将完整的消息历史发送给模型，让它根据工具结果继续\\n                    logger.info(f\"工具调用回合完成，继续与模型交互...\")\\n                    continue # 继续循环，发送包含工具结果的消息历史给模型\\n                else:\\n                    # 发生未知情况或错误\\n                    logger.error(f\"模型返回意外的 finish_reason: {finish_reason}\")\\n                    return f\"处理你的查询时遇到意外情况 ({finish_reason})\"\\n\\n            except Exception as e:\\n                logger.error(f\"处理查询时出错: {str(e)}\")\\n                return f\"处理你的查询时发生错误: {str(e)}\"\\n\\n        # 如果循环达到最大次数仍未得到最终回复\\n        logger.warning(\"达到最大工具调用回合数，未能获得最终回复\")\\n        return \"抱歉，处理你的请求时出现问题，未能获得最终结果。\"\\n\\n    async def chat_loop(self) -> None:\\n        \"\"\"运行交互式聊天循环\"\"\"\\n        logger.info(\"开始聊天会话...\")\\n        \\n        while True:\\n            try:\\n                query = input(\"\\\\n请输入你的问题 (输入 \\'quit\\' 或 \\'exit\\' 退出): \").strip()\\n                \\n                if query.lower() in [\\'quit\\', \\'exit\\']:\\n                    logger.info(\"结束聊天会话...\")\\n                    break\\n                    \\n                response = await self.process_query(query)\\n                print(f\"\\\\n助手: {response}\")\\n                \\n            except KeyboardInterrupt:\\n                logger.info(\"\\\\n收到键盘中断，结束会话...\")\\n                break\\n            except Exception as e:\\n                logger.error(f\"聊天循环中出错: {str(e)}\")\\n                print(f\"\\\\n发生错误: {str(e)}\")\\n                \\n    async def cleanup(self) -> None:\\n        \"\"\"清理客户端资源\"\"\"\\n        if self.server:\\n            await self.server.cleanup()\\n\\nasync def main() -> None:\\n    \"\"\"主程序入口\"\"\"\\n    try:\\n        config = Configuration()\\n        client = MCPClient(config)\\n        \\n        await client.initialize()\\n        await client.chat_loop()\\n        \\n    except Exception as e:\\n        logger.error(f\"致命错误: {str(e)}\")\\n        raise\\n        \\n    finally:\\n        if \\'client\\' in locals():\\n            await client.cleanup()\\n\\nif __name__ == \"__main__\":\\n    try:\\n        asyncio.run(main())\\n    except KeyboardInterrupt:\\n        logger.info(\"程序被用户终止\")\\n    except Exception as e:\\n        logger.error(f\"程序因错误终止: {str(e)}\")\\n        raise\\n', 'timestamp': '2025-10-30T06:17:09+0000'}\n",
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"read_project_file\", \"args\": {\"name\": \"weather_server.py\"}}\n",
            "Action Result: {'tool_executed': True, 'result': '#!/usr/bin/env python3\\n# -*- coding: utf-8 -*-\\n\\n\"\"\"\\nMCP 服务器可以提供三种主要类型的功能：\\n\\n资源：客户端可以读取的类似文件的数据（例如 API 响应或文件内容）\\n工具：可由 LLM 调用的函数（经用户批准）\\n提示：预先编写的模板，帮助用户完成特定任务\\n\\n######################################\\n\\nMCP 天气服务器\\n\\n提供两个工具：\\n1. get_weather_warning: 获取指定城市ID或经纬度的天气灾害预警\\n2. get_daily_forecast: 获取指定城市ID或经纬度的天气预报\\n\\nAuthor: FlyAIBox\\nDate: 2025.10.11\\n\"\"\"\\n\\nfrom typing import Any, Dict, List, Optional, Union\\nimport asyncio\\nimport httpx\\nimport os\\nfrom urllib.parse import urljoin\\nfrom mcp.server.fastmcp import FastMCP\\nfrom dotenv import load_dotenv\\nfrom pathlib import Path\\n\\n# 加载 .env 文件中的环境变量\\ndotenv_path = Path(__file__).resolve().parents[1] / \\'.env\\'\\nload_dotenv(dotenv_path)\\n\\n# 初始化 FastMCP 服务器\\nmcp = FastMCP(\"weather\",  # 服务器名称\\n              debug=True,  # 启用调试模式，会输出详细日志\\n              host=\"0.0.0.0\") # 监听所有网络接口，允许远程连接\\n\\n# 从环境变量中读取常量\\nQWEATHER_API_BASE = os.getenv(\"QWEATHER_API_BASE\")\\nQWEATHER_API_KEY = os.getenv(\"QWEATHER_API_KEY\")\\n\\ndef _normalize_base_url(raw_base: Optional[str]) -> str:\\n    \"\"\"\\n    确保基础 URL 包含协议并以单个斜杠结尾，兼容 .env 中未写协议的情况\\n    \"\"\"\\n    if not raw_base:\\n        raise RuntimeError(\"未配置 QWEATHER_API_BASE 环境变量\")\\n\\n    base = raw_base.strip()\\n    if not base.startswith((\"http://\", \"https://\")):\\n        base = f\"https://{base.lstrip(\\'/\\')}\"\\n\\n    # urljoin 要求目录风格以斜杠结尾，避免 \\'v7/weather/7d\\' 被覆盖\\n    if not base.endswith(\"/\"):\\n        base = f\"{base}/\"\\n\\n    return base\\n\\ntry:\\n    _QWEATHER_BASE_URL = _normalize_base_url(QWEATHER_API_BASE)\\nexcept RuntimeError as err:\\n    print(f\"[配置错误] {err}\")\\n    _QWEATHER_BASE_URL = None\\n\\nasync def make_qweather_request(endpoint: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:\\n    \"\"\"\\n    向和风天气 API 发送请求\\n    \\n    参数:\\n        endpoint: API 端点路径（不包含基础 URL）\\n        params: API 请求的参数\\n        \\n    返回:\\n        成功时返回 JSON 响应，失败时返回 None\\n    \"\"\"\\n    if not _QWEATHER_BASE_URL:\\n        print(\"QWEATHER_API_BASE 未正确配置，已跳过请求。\")\\n        return None\\n\\n    if not QWEATHER_API_KEY:\\n        print(\"QWEATHER_API_KEY 未设置，已跳过请求。\")\\n        return None\\n\\n    safe_endpoint = endpoint.lstrip(\"/\")\\n    url = urljoin(_QWEATHER_BASE_URL, safe_endpoint)\\n\\n    # 使用 Header 方式认证（和风天气的新版本API）\\n    headers = {\\n        \"X-QW-Api-Key\": QWEATHER_API_KEY\\n    }\\n    \\n    async with httpx.AsyncClient() as client:\\n        try:\\n            print(f\"请求 URL: {url}\")\\n            print(f\"请求参数: {params}\")\\n            response = await client.get(url, params=params, headers=headers, timeout=30.0)\\n            print(f\"响应状态码: {response.status_code}\")\\n            response.raise_for_status()\\n            result = response.json()\\n            print(f\"响应内容: {result}\")\\n            return result\\n        except httpx.HTTPStatusError as e:\\n            print(f\"HTTP 状态错误: {e.response.status_code} - {e.response.text}\")\\n            return None\\n        except Exception as e:\\n            print(f\"API 请求错误: {type(e).__name__}: {e}\")\\n            return None\\n\\ndef format_warning(warning: Dict[str, Any]) -> str:\\n    \"\"\"\\n    将天气预警数据格式化为可读字符串\\n    \\n    参数:\\n        warning: 天气预警数据对象\\n        \\n    返回:\\n        格式化后的预警信息\\n    \"\"\"\\n    return f\"\"\"\\n预警ID: {warning.get(\\'id\\', \\'未知\\')}\\n标题: {warning.get(\\'title\\', \\'未知\\')}\\n发布时间: {warning.get(\\'pubTime\\', \\'未知\\')}\\n开始时间: {warning.get(\\'startTime\\', \\'未知\\')}\\n结束时间: {warning.get(\\'endTime\\', \\'未知\\')}\\n预警类型: {warning.get(\\'typeName\\', \\'未知\\')}\\n预警等级: {warning.get(\\'severity\\', \\'未知\\')} ({warning.get(\\'severityColor\\', \\'未知\\')})\\n发布单位: {warning.get(\\'sender\\', \\'未知\\')}\\n状态: {warning.get(\\'status\\', \\'未知\\')}\\n详细信息: {warning.get(\\'text\\', \\'无详细信息\\')}\\n\"\"\"\\n\\n@mcp.tool()\\nasync def get_weather_warning(location: Union[str, int]) -> str:\\n    \"\"\"\\n    获取指定位置的天气灾害预警\\n    \\n    参数:\\n        location: 城市ID或经纬度坐标（经度,纬度）\\n                例如：\\'101010100\\'（北京）或 \\'116.41,39.92\\'\\n                也可以直接传入数字ID，如 101010100\\n        \\n    返回:\\n        格式化的预警信息字符串\\n    \"\"\"\\n    # 确保 location 为字符串类型\\n    location = str(location)\\n    \\n    params = {\\n        \"location\": location,\\n        \"lang\": \"zh\"\\n    }\\n    \\n    data = await make_qweather_request(\"v7/warning/now\", params)\\n    \\n    if not data:\\n        return \"无法获取预警信息或API请求失败。\"\\n    \\n    if data.get(\"code\") != \"200\":\\n        return f\"API 返回错误: {data.get(\\'code\\')}\"\\n    \\n    warnings = data.get(\"warning\", [])\\n    \\n    if not warnings:\\n        return f\"当前位置 {location} 没有活动预警。\"\\n    \\n    formatted_warnings = [format_warning(warning) for warning in warnings]\\n    return \"\\\\n---\\\\n\".join(formatted_warnings)\\n\\ndef format_daily_forecast(daily: Dict[str, Any]) -> str:\\n    \"\"\"\\n    将天气预报数据格式化为可读字符串\\n    \\n    参数:\\n        daily: 天气预报数据对象\\n        \\n    返回:\\n        格式化后的预报信息\\n    \"\"\"\\n    return f\"\"\"\\n日期: {daily.get(\\'fxDate\\', \\'未知\\')}\\n日出: {daily.get(\\'sunrise\\', \\'未知\\')}  日落: {daily.get(\\'sunset\\', \\'未知\\')}\\n最高温度: {daily.get(\\'tempMax\\', \\'未知\\')}°C  最低温度: {daily.get(\\'tempMin\\', \\'未知\\')}°C\\n白天天气: {daily.get(\\'textDay\\', \\'未知\\')}  夜间天气: {daily.get(\\'textNight\\', \\'未知\\')}\\n白天风向: {daily.get(\\'windDirDay\\', \\'未知\\')} {daily.get(\\'windScaleDay\\', \\'未知\\')}级 ({daily.get(\\'windSpeedDay\\', \\'未知\\')}km/h)\\n夜间风向: {daily.get(\\'windDirNight\\', \\'未知\\')} {daily.get(\\'windScaleNight\\', \\'未知\\')}级 ({daily.get(\\'windSpeedNight\\', \\'未知\\')}km/h)\\n相对湿度: {daily.get(\\'humidity\\', \\'未知\\')}%\\n降水量: {daily.get(\\'precip\\', \\'未知\\')}mm\\n紫外线指数: {daily.get(\\'uvIndex\\', \\'未知\\')}\\n能见度: {daily.get(\\'vis\\', \\'未知\\')}km\\n\"\"\"\\n\\n@mcp.tool()\\nasync def get_daily_forecast(location: Union[str, int], days: int = 3) -> str:\\n    \"\"\"\\n    获取指定位置的天气预报\\n    \\n    参数:\\n        location: 城市ID或经纬度坐标（经度,纬度）\\n                例如：\\'101010100\\'（北京）或 \\'116.41,39.92\\'\\n                也可以直接传入数字ID，如 101010100\\n        days: 预报天数，可选值为 3、7、10、15、30，默认为 3\\n        \\n    返回:\\n        格式化的天气预报字符串\\n    \"\"\"\\n    # 确保 location 为字符串类型\\n    location = str(location)\\n    \\n    # 确保 days 参数有效\\n    valid_days = [3, 7, 10, 15, 30]\\n    if days not in valid_days:\\n        days = 3  # 默认使用3天预报\\n    \\n    params = {\\n        \"location\": location,\\n        \"lang\": \"zh\"\\n    }\\n    # 和风天气API文档 https://dev.qweather.com/docs/api/weather/weather-daily-forecast/\\n    endpoint = f\"v7/weather/{days}d\"\\n    data = await make_qweather_request(endpoint, params)\\n    \\n    if not data:\\n        return \"无法获取天气预报或API请求失败。\"\\n    \\n    if data.get(\"code\") != \"200\":\\n        return f\"API 返回错误: {data.get(\\'code\\')}\"\\n    \\n    daily_forecasts = data.get(\"daily\", [])\\n    \\n    if not daily_forecasts:\\n        return f\"无法获取 {location} 的天气预报数据。\"\\n    \\n    formatted_forecasts = [format_daily_forecast(daily) for daily in daily_forecasts]\\n    return \"\\\\n---\\\\n\".join(formatted_forecasts)\\n\\nif __name__ == \"__main__\":\\n    print(\"正在启动 MCP 天气服务器...\")\\n    print(\"提供工具: get_weather_warning, get_daily_forecast\")\\n    print(\"请确保环境变量 QWEATHER_API_KEY 已设置\")\\n    print(\"使用 Ctrl+C 停止服务器\")\\n    \\n    # 初始化并运行服务器\\n    mcp.run(transport=\\'stdio\\') \\n', 'timestamp': '2025-10-30T06:17:10+0000'}\n",
            "Agent thinking...\n",
            "Agent Decision: {\"tool\": \"terminate\", \"args\": {\"message\": \"# Project Overview\\nThis project is an MCP (Model Context Protocol) client and server implementation integrated with DeepSeek's API to handle weather information queries. The project includes two primary Python scripts built to demonstrate a functional model context protocol capable of interacting with DeepSeek tools.\\n\\n### Components\\n1. **mcp_client_deepseek.py**\\n   - Implements an MCP client using the DeepSeek API.\\n   - Demonstrates how to connect to the MCP server, handle queries, and maintain an interactive chat loop.\\n   - Requires environment variables:\\n     - `DEEPSEEK_API_KEY`: API key for DeepSeek access.\\n     - `DEEPSEEK_BASE_URL`: Base URL for the DeepSeek API.\\n     - `DEEPSEEK_MODEL`: Name of the DeepSeek model (e.g., deepseek-chat).\\n\\n2. **weather_server.py**\\n   - An MCP server providing weather-related tools.\\n   - Offers tools for getting weather warnings and daily forecasts.\\n   - Utilizes the QWeather API, requiring configuration of:\\n     - `QWEATHER_API_BASE`: Base URL for the QWeather API.\\n     - `QWEATHER_API_KEY`: API key for QWeather access.\\n   - These tools include:\\n     - `get_weather_warning`: Fetches weather disaster warnings for a given city ID or coordinates.\\n     - `get_daily_forecast`: Fetches weather forecasts for specified city IDs or coordinates.\\n\\n### Author\\nFlyAIBox\\n\\n### Date\\n2025.10.11\\n\\nThis project requires the correct setup of environment variables to function, especially those related to API access. Please ensure all necessary configurations are completed before running the scripts.\"}}\n",
            "Action Result: {'tool_executed': True, 'result': \"# Project Overview\\nThis project is an MCP (Model Context Protocol) client and server implementation integrated with DeepSeek's API to handle weather information queries. The project includes two primary Python scripts built to demonstrate a functional model context protocol capable of interacting with DeepSeek tools.\\n\\n### Components\\n1. **mcp_client_deepseek.py**\\n   - Implements an MCP client using the DeepSeek API.\\n   - Demonstrates how to connect to the MCP server, handle queries, and maintain an interactive chat loop.\\n   - Requires environment variables:\\n     - `DEEPSEEK_API_KEY`: API key for DeepSeek access.\\n     - `DEEPSEEK_BASE_URL`: Base URL for the DeepSeek API.\\n     - `DEEPSEEK_MODEL`: Name of the DeepSeek model (e.g., deepseek-chat).\\n\\n2. **weather_server.py**\\n   - An MCP server providing weather-related tools.\\n   - Offers tools for getting weather warnings and daily forecasts.\\n   - Utilizes the QWeather API, requiring configuration of:\\n     - `QWEATHER_API_BASE`: Base URL for the QWeather API.\\n     - `QWEATHER_API_KEY`: API key for QWeather access.\\n   - These tools include:\\n     - `get_weather_warning`: Fetches weather disaster warnings for a given city ID or coordinates.\\n     - `get_daily_forecast`: Fetches weather forecasts for specified city IDs or coordinates.\\n\\n### Author\\nFlyAIBox\\n\\n### Date\\n2025.10.11\\n\\nThis project requires the correct setup of environment variables to function, especially those related to API access. Please ensure all necessary configurations are completed before running the scripts.\\nTerminating...\", 'timestamp': '2025-10-30T06:17:15+0000'}\n",
            "[{'type': 'user', 'content': 'Write a README for this project.'}, {'type': 'assistant', 'content': '{\"tool\": \"list_project_files\", \"args\": {}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": [\"mcp_client_deepseek.py\", \"weather_server.py\"], \"timestamp\": \"2025-10-30T06:17:08+0000\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"mcp_client_deepseek.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"\\\\\"\\\\\"\\\\\"\\\\n# \\\\u96c6\\\\u6210 DeepSeek \\\\u7684 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\n#\\\\u53c2\\\\u8003\\\\u5b98\\\\u65b9\\\\u6848\\\\u4f8b\\\\uff1ahttps://github.com/modelcontextprotocol/python-sdk/blob/main/examples/clients/simple-chatbot/mcp_simple_chatbot/main.py\\\\n\\\\n\\\\n# \\\\u672c\\\\u6a21\\\\u5757\\\\u5b9e\\\\u73b0\\\\u4e86\\\\u4e00\\\\u4e2a\\\\u6a21\\\\u578b\\\\u4e0a\\\\u4e0b\\\\u6587\\\\u534f\\\\u8bae\\\\uff08MCP\\\\uff09\\\\u5ba2\\\\u6237\\\\u7aef\\\\uff0c\\\\u8be5\\\\u5ba2\\\\u6237\\\\u7aef\\\\u4f7f\\\\u7528 DeepSeek \\\\u7684 API\\\\n# \\\\u6765\\\\u5904\\\\u7406\\\\u67e5\\\\u8be2\\\\u5e76\\\\u4e0e MCP \\\\u5de5\\\\u5177\\\\u8fdb\\\\u884c\\\\u4ea4\\\\u4e92\\\\u3002\\\\u5b83\\\\u6f14\\\\u793a\\\\u4e86\\\\u5982\\\\u4f55\\\\uff1a\\\\n# 1. \\\\u8fde\\\\u63a5\\\\u5230 MCP \\\\u670d\\\\u52a1\\\\u5668\\\\n# 2. \\\\u4f7f\\\\u7528 DeepSeek \\\\u7684 API \\\\u6765\\\\u5904\\\\u7406\\\\u67e5\\\\u8be2\\\\n# 3. \\\\u5904\\\\u7406\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u548c\\\\u54cd\\\\u5e94\\\\n# 4. \\\\u7ef4\\\\u62a4\\\\u4e00\\\\u4e2a\\\\u4ea4\\\\u4e92\\\\u5f0f\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\n\\\\n# \\\\u6240\\\\u9700\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\uff1a\\\\n# DEEPSEEK_API_KEY\\\\uff1aDeepSeek API \\\\u5bc6\\\\u94a5 (\\\\u683c\\\\u5f0f\\\\uff1ask-xxxx...)\\\\n# DEEPSEEK_BASE_URL\\\\uff1aDeepSeek API \\\\u57fa\\\\u7840 URL (https://api.deepseek.com)\\\\n# DEEPSEEK_MODEL\\\\uff1aDeepSeek \\\\u6a21\\\\u578b\\\\u540d\\\\u79f0 (\\\\u4f8b\\\\u5982 deepseek-chat)\\\\n\\\\nAuthor: FlyAIBox\\\\nDate: 2025.10.11\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\nimport json\\\\nimport asyncio\\\\nimport logging\\\\nimport os\\\\nfrom typing import Optional, Dict, Any, List, Tuple\\\\nfrom contextlib import AsyncExitStack\\\\n\\\\nfrom openai import OpenAI\\\\nfrom dotenv import load_dotenv\\\\n\\\\nfrom mcp import ClientSession, StdioServerParameters\\\\nfrom mcp.client.stdio import stdio_client\\\\n\\\\n# \\\\u914d\\\\u7f6e\\\\u65e5\\\\u5fd7\\\\u8bb0\\\\u5f55\\\\nlogging.basicConfig(\\\\n    level=logging.INFO,\\\\n    format=\\\\\"%(asctime)s - %(levelname)s - %(message)s\\\\\"\\\\n)\\\\nlogger = logging.getLogger(__name__)\\\\n\\\\nload_dotenv()\\\\n\\\\n\\\\nclass Configuration:\\\\n    \\\\\"\\\\\"\\\\\"\\\\u914d\\\\u7f6e\\\\u7ba1\\\\u7406\\\\u7c7b\\\\uff0c\\\\u8d1f\\\\u8d23\\\\u7ba1\\\\u7406\\\\u548c\\\\u9a8c\\\\u8bc1\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u914d\\\\u7f6e\\\\u5e76\\\\u52a0\\\\u8f7d\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n        self.load_env()\\\\n        self._validate_env()\\\\n        \\\\n    @staticmethod\\\\n    def load_env() -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u4ece.env\\\\u6587\\\\u4ef6\\\\u52a0\\\\u8f7d\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\"\\\\\"\\\\\"\\\\n        load_dotenv()\\\\n        \\\\n    def _validate_env(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u9a8c\\\\u8bc1\\\\u5fc5\\\\u9700\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\u662f\\\\u5426\\\\u5b58\\\\u5728\\\\\"\\\\\"\\\\\"\\\\n        required_vars = [\\\\\"DEEPSEEK_API_KEY\\\\\"]\\\\n        missing_vars = [var for var in required_vars if not os.getenv(var)]\\\\n        if missing_vars:\\\\n            raise ValueError(f\\\\\"\\\\u7f3a\\\\u5c11\\\\u5fc5\\\\u9700\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf: {\\', \\'.join(missing_vars)}\\\\\")\\\\n    \\\\n    @property\\\\n    def api_key(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek API \\\\u5bc6\\\\u94a5\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_API_KEY\\\\\", \\\\\"\\\\\")\\\\n    \\\\n    @property\\\\n    def base_url(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek API \\\\u57fa\\\\u7840 URL\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_BASE_URL\\\\\", \\\\\"https://api.deepseek.com\\\\\")\\\\n    \\\\n    @property\\\\n    def model(self) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6 DeepSeek \\\\u6a21\\\\u578b\\\\u540d\\\\u79f0\\\\\"\\\\\"\\\\\"\\\\n        return os.getenv(\\\\\"DEEPSEEK_MODEL\\\\\", \\\\\"deepseek-chat\\\\\")\\\\n\\\\nclass Tool:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u5de5\\\\u5177\\\\u7c7b\\\\uff0c\\\\u8868\\\\u793a\\\\u4e00\\\\u4e2a\\\\u5177\\\\u6709\\\\u5c5e\\\\u6027\\\\u7684\\\\u5de5\\\\u5177\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, name: str, description: str, input_schema: Dict[str, Any]) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316\\\\u5de5\\\\u5177\\\\n        \\\\n        Args:\\\\n            name: \\\\u5de5\\\\u5177\\\\u540d\\\\u79f0\\\\n            description: \\\\u5de5\\\\u5177\\\\u63cf\\\\u8ff0\\\\n            input_schema: \\\\u8f93\\\\u5165\\\\u53c2\\\\u6570\\\\u6a21\\\\u5f0f\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.name = name\\\\n        self.description = description\\\\n        self.input_schema = input_schema\\\\n        \\\\n    def to_openai_format(self) -> Dict[str, Any]:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u5c06\\\\u5de5\\\\u5177\\\\u8f6c\\\\u6362\\\\u4e3a OpenAI API \\\\u683c\\\\u5f0f\\\\\"\\\\\"\\\\\"\\\\n        return {\\\\n            \\\\\"type\\\\\": \\\\\"function\\\\\",\\\\n            \\\\\"function\\\\\": {\\\\n                \\\\\"name\\\\\": self.name,\\\\n                \\\\\"description\\\\\": self.description,\\\\n                \\\\\"parameters\\\\\": self.input_schema\\\\n            }\\\\n        }\\\\n\\\\nclass MCPServer:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u670d\\\\u52a1\\\\u5668\\\\u7ba1\\\\u7406\\\\u7c7b\\\\uff0c\\\\u5904\\\\u7406\\\\u670d\\\\u52a1\\\\u5668\\\\u8fde\\\\u63a5\\\\u548c\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, server_path: str) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316\\\\u670d\\\\u52a1\\\\u5668\\\\u7ba1\\\\u7406\\\\u5668\\\\n        \\\\n        Args:\\\\n            server_path: \\\\u670d\\\\u52a1\\\\u5668\\\\u811a\\\\u672c\\\\u8def\\\\u5f84\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.server_path = server_path\\\\n        self.session: Optional[ClientSession] = None\\\\n        self.exit_stack = AsyncExitStack()\\\\n        self._cleanup_lock = asyncio.Lock()\\\\n        \\\\n    async def initialize(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u670d\\\\u52a1\\\\u5668\\\\u8fde\\\\u63a5\\\\uff0c\\\\u5305\\\\u542b\\\\u91cd\\\\u8bd5\\\\u673a\\\\u5236\\\\\"\\\\\"\\\\\"\\\\n        max_retries = 3\\\\n        retry_delay = 1.0\\\\n        \\\\n        for attempt in range(max_retries):\\\\n            try:\\\\n                if not os.path.exists(self.server_path):\\\\n                    raise FileNotFoundError(f\\\\\"\\\\u627e\\\\u4e0d\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\u6587\\\\u4ef6: {self.server_path}\\\\\")\\\\n                \\\\n                server_params = StdioServerParameters(\\\\n                    command=\\'python\\',\\\\n                    args=[self.server_path],\\\\n                    env=None\\\\n                )\\\\n                \\\\n                stdio_transport = await self.exit_stack.enter_async_context(\\\\n                    stdio_client(server_params)\\\\n                )\\\\n                stdio, write = stdio_transport\\\\n                \\\\n                self.session = await self.exit_stack.enter_async_context(\\\\n                    ClientSession(stdio, write)\\\\n                )\\\\n                await self.session.initialize()\\\\n                logger.info(\\\\\"\\\\u6210\\\\u529f\\\\u8fde\\\\u63a5\\\\u5230 MCP \\\\u670d\\\\u52a1\\\\u5668\\\\\")\\\\n                break\\\\n                \\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u7b2c {attempt + 1}/{max_retries} \\\\u6b21\\\\u5c1d\\\\u8bd5\\\\u5931\\\\u8d25: {str(e)}\\\\\")\\\\n                if attempt < max_retries - 1:\\\\n                    await asyncio.sleep(retry_delay)\\\\n                else:\\\\n                    raise\\\\n                    \\\\n    async def list_tools(self) -> List[Tool]:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u83b7\\\\u53d6\\\\u670d\\\\u52a1\\\\u5668\\\\u63d0\\\\u4f9b\\\\u7684\\\\u53ef\\\\u7528\\\\u5de5\\\\u5177\\\\u5217\\\\u8868\\\\\"\\\\\"\\\\\"\\\\n        if not self.session:\\\\n            raise RuntimeError(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n            \\\\n        response = await self.session.list_tools()\\\\n        return [\\\\n            Tool(tool.name, tool.description, tool.inputSchema)\\\\n            for tool in response.tools\\\\n        ]\\\\n        \\\\n    async def execute_tool(\\\\n        self,\\\\n        tool_name: str,\\\\n        arguments: Dict[str, Any],\\\\n        retries: int = 2,\\\\n        delay: float = 1.0\\\\n    ) -> Any:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u6267\\\\u884c\\\\u5de5\\\\u5177\\\\uff0c\\\\u5305\\\\u542b\\\\u91cd\\\\u8bd5\\\\u673a\\\\u5236\\\\n        \\\\n        Args:\\\\n            tool_name: \\\\u5de5\\\\u5177\\\\u540d\\\\u79f0\\\\n            arguments: \\\\u5de5\\\\u5177\\\\u53c2\\\\u6570\\\\n            retries: \\\\u91cd\\\\u8bd5\\\\u6b21\\\\u6570\\\\n            delay: \\\\u91cd\\\\u8bd5\\\\u5ef6\\\\u8fdf\\\\u65f6\\\\u95f4\\\\uff08\\\\u79d2\\\\uff09\\\\n            \\\\n        Returns:\\\\n            \\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u7ed3\\\\u679c\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        if not self.session:\\\\n            raise RuntimeError(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n            \\\\n        for attempt in range(retries):\\\\n            try:\\\\n                logger.info(f\\\\\"\\\\u6267\\\\u884c\\\\u5de5\\\\u5177 {tool_name}\\\\uff0c\\\\u53c2\\\\u6570: {arguments}\\\\\")\\\\n                result = await self.session.call_tool(tool_name, arguments)\\\\n                return result\\\\n                \\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5931\\\\u8d25 (\\\\u7b2c {attempt + 1}/{retries} \\\\u6b21\\\\u5c1d\\\\u8bd5): {str(e)}\\\\\")\\\\n                if attempt < retries - 1:\\\\n                    await asyncio.sleep(delay)\\\\n                else:\\\\n                    raise\\\\n                    \\\\n    async def cleanup(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6e05\\\\u7406\\\\u670d\\\\u52a1\\\\u5668\\\\u8d44\\\\u6e90\\\\\"\\\\\"\\\\\"\\\\n        async with self._cleanup_lock:\\\\n            try:\\\\n                await self.exit_stack.aclose()\\\\n                self.session = None\\\\n                logger.info(\\\\\"\\\\u670d\\\\u52a1\\\\u5668\\\\u8d44\\\\u6e90\\\\u6e05\\\\u7406\\\\u5b8c\\\\u6210\\\\\")\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u6e05\\\\u7406\\\\u8fc7\\\\u7a0b\\\\u4e2d\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n\\\\nclass MCPClient:\\\\n    \\\\\"\\\\\"\\\\\"MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\u5b9e\\\\u73b0\\\\uff0c\\\\u96c6\\\\u6210\\\\u4e86 DeepSeek API\\\\\"\\\\\"\\\\\"\\\\n    \\\\n    def __init__(self, config: Configuration) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u521d\\\\u59cb\\\\u5316 MCP \\\\u5ba2\\\\u6237\\\\u7aef\\\\n        \\\\n        Args:\\\\n            config: \\\\u914d\\\\u7f6e\\\\u5bf9\\\\u8c61\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        self.config = config\\\\n        self.server: Optional[MCPServer] = None\\\\n        self.client = OpenAI(\\\\n            api_key=config.api_key,\\\\n            base_url=config.base_url\\\\n        )\\\\n        \\\\n    async def initialize(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u521d\\\\u59cb\\\\u5316\\\\u5ba2\\\\u6237\\\\u7aef\\\\u5e76\\\\u8fde\\\\u63a5\\\\u5230\\\\u670d\\\\u52a1\\\\u5668\\\\\"\\\\\"\\\\\"\\\\n        server_path = os.path.join(\\\\n            os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\\\\n            \\\\\"server\\\\\",\\\\n            \\\\\"weather_server.py\\\\\"\\\\n        )\\\\n        self.server = MCPServer(server_path)\\\\n        await self.server.initialize()\\\\n        \\\\n\\\\n    async def process_query(self, query: str) -> str:\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        \\\\u5904\\\\u7406\\\\u7528\\\\u6237\\\\u67e5\\\\u8be2\\\\uff0c\\\\u96c6\\\\u6210\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\uff0c\\\\u652f\\\\u6301\\\\u591a\\\\u8f6e\\\\u5de5\\\\u5177\\\\u4ea4\\\\u4e92\\\\n\\\\n        Args:\\\\n            query: \\\\u7528\\\\u6237\\\\u67e5\\\\u8be2\\\\u5b57\\\\u7b26\\\\u4e32\\\\n\\\\n        Returns:\\\\n            \\\\u5904\\\\u7406\\\\u540e\\\\u7684\\\\u54cd\\\\u5e94\\\\u7ed3\\\\u679c\\\\n        \\\\\"\\\\\"\\\\\"\\\\n        if not self.server:\\\\n            raise RuntimeError(\\\\\"\\\\u5ba2\\\\u6237\\\\u7aef\\\\u672a\\\\u521d\\\\u59cb\\\\u5316\\\\\")\\\\n\\\\n        system_prompt = (\\\\n            \\\\\"You are a helpful assistant specializing in weather information.\\\\\\\\n\\\\\"\\\\n            \\\\\"You have access to the MCP Weather Server tool with the following functions:\\\\\\\\n\\\\\"\\\\n            \\\\\"- get_weather_warning(city_id=None, latitude=None, longitude=None): Retrieves weather disaster warnings for a specified city ID or coordinates.\\\\\\\\n\\\\\"\\\\n            \\\\\"- get_daily_forecast(city_id=None, latitude=None, longitude=None): Retrieves the multi-day weather forecast for a specified city ID or coordinates.\\\\\\\\n\\\\\"\\\\n            \\\\\"\\\\\\\\n\\\\\"\\\\n            \\\\\"Core Instructions:\\\\\\\\n\\\\\"\\\\n            \\\\\"1.  **Carefully analyze the user\\'s request**: Understand all components of the user\\'s query. Determine if the user needs weather warning information, weather forecast information, or both.\\\\\\\\n\\\\\"\\\\n            \\\\\"2.  **Identify Information Needs**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * If the user only asks for warnings (e.g., \\\\\\\\\\\\\"Are there any warnings in Beijing?\\\\\\\\\\\\\"), only use `get_weather_warning`.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * If the user only asks for the forecast (e.g., \\\\\\\\\\\\\"What\\'s the weather like in Beijing tomorrow?\\\\\\\\\\\\\"), only use `get_daily_forecast`.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **If the user\\'s question includes multiple aspects**, such as asking about **warning status** and also asking **if it\\'s suitable for a certain activity** (which implies a query about future weather, like \\\\\\\\\\\\\"Have there been high temperature warnings in Beijing in the last week? Is it suitable for outdoor activities?\\\\\\\\\\\\\"), you need to **call both tools sequentially**.\\\\\\\\n\\\\\"\\\\n            \\\\\"3.  **Call Tools as Needed**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Prioritize getting warning information**: If warning information is needed, first call `get_weather_warning`.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Get the weather forecast**: If the user mentions a specific time period (e.g., \\\\\\\\\\\\\"weekend\\\\\\\\\\\\\", \\\\\\\\\\\\\"next three days\\\\\\\\\\\\\", \\\\\\\\\\\\\"next week\\\\\\\\\\\\\") or asks about activity suitability (which typically concerns the next few days), call `get_daily_forecast` to get the forecast for the corresponding period. For vague phrases like \\\\\\\\\\\\\"last week\\\\\\\\\\\\\" or \\\\\\\\\\\\\"recently\\\\\\\\\\\\\", interpret it as asking about *current* conditions and the *upcoming* few days (covered by the forecast). For questions like \\\\\\\\\\\\\"Is it suitable for outdoor activities?\\\\\\\\\\\\\", you should get the forecast for at least the next 2-3 days (e.g., today, tomorrow, the day after tomorrow, or the upcoming weekend) to support your judgment.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Ensure tool call order**: When multiple tools need to be called, they should be called in a logical sequence. For example, first get the warning, then get the forecast. Wait for one tool to finish executing before deciding whether to call the next tool or generate a response.\\\\\\\\n\\\\\"\\\\n            \\\\\"4.  **Information Integration and Response**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * After obtaining all necessary information (warning, forecast), you **must synthesize and analyze this information**.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Completely answer the user\\'s question**: Ensure you answer all parts of the user\\'s query.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * **Provide advice**: If the user asks about activity suitability, based on the retrieved warning status and forecast information (temperature, weather condition - clear/rainy, wind strength, etc.), provide a clear, data-supported recommendation (e.g., \\\\\\\\\\\\\"Currently there are no high temperature warnings, but it\\'s expected to rain this weekend, so it\\'s not very suitable for outdoor activities,\\\\\\\\\\\\\" or \\\\\\\\\\\\\"It will be sunny for the next few days with no warnings, suitable for outdoor activities.\\\\\\\\\\\\\").\\\\\\\\n\\\\\"\\\\n            \\\\\"5.  **Tool Usage Details**:\\\\\\\\n\\\\\"\\\\n            \\\\\"    * When using the tools, retain the full context of the user\\'s original question.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * Unless explicitly requested by the user, do not insert specific times of day (e.g., \\\\\\\\\\\\\"3 PM\\\\\\\\\\\\\") into the search query or your response.\\\\\\\\n\\\\\"\\\\n            \\\\\"    * When city information is needed, if the user provides a city name (e.g., \\\\\\\\\\\\\"Beijing\\\\\\\\\\\\\"), use the corresponding `city_id` (e.g., Beijing\\'s city_id might be \\'101010100\\').\\\\\\\\n\\\\\"\\\\n        )\\\\n\\\\n        messages = [\\\\n            {\\\\\"role\\\\\": \\\\\"system\\\\\", \\\\\"content\\\\\": system_prompt},\\\\n            {\\\\\"role\\\\\": \\\\\"user\\\\\", \\\\\"content\\\\\": query}\\\\n        ]\\\\n\\\\n        tools = await self.server.list_tools()\\\\n        available_tools = [tool.to_openai_format() for tool in tools]\\\\n\\\\n        max_tool_turns = 5 # \\\\u8bbe\\\\u7f6e\\\\u4e00\\\\u4e2a\\\\u6700\\\\u5927\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u56de\\\\u5408\\\\u6570\\\\uff0c\\\\u9632\\\\u6b62\\\\u65e0\\\\u9650\\\\u5faa\\\\u73af\\\\n\\\\n        for _ in range(max_tool_turns):\\\\n            try:\\\\n                response = self.client.chat.completions.create(\\\\n                    model=self.config.model,\\\\n                    messages=messages,\\\\n                    tools=available_tools,\\\\n                    tool_choice=\\\\\"auto\\\\\" # \\\\u5141\\\\u8bb8\\\\u6a21\\\\u578b\\\\u51b3\\\\u5b9a\\\\u662f\\\\u5426\\\\u8c03\\\\u7528\\\\u5de5\\\\u5177\\\\n                )\\\\n\\\\n                content = response.choices[0].message\\\\n                finish_reason = response.choices[0].finish_reason\\\\n\\\\n                if finish_reason == \\\\\"stop\\\\\":\\\\n                    # \\\\u6a21\\\\u578b\\\\u751f\\\\u6210\\\\u4e86\\\\u6700\\\\u7ec8\\\\u7684\\\\u6587\\\\u672c\\\\u56de\\\\u590d\\\\n                    messages.append({\\\\\"role\\\\\": \\\\\"assistant\\\\\", \\\\\"content\\\\\": content.content})\\\\n                    return content.content\\\\n                elif finish_reason == \\\\\"tool_calls\\\\\":\\\\n                    # \\\\u6a21\\\\u578b\\\\u9884\\\\u6d4b\\\\u4e86\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\n                    messages.append(content.model_dump()) # \\\\u5c06 assistant tool_calls \\\\u6d88\\\\u606f\\\\u6dfb\\\\u52a0\\\\u5230\\\\u5386\\\\u53f2\\\\n\\\\n                    tool_outputs = []\\\\n                    for tool_call in content.tool_calls:\\\\n                        tool_name = tool_call.function.name\\\\n                        tool_args = json.loads(tool_call.function.arguments)\\\\n\\\\n                        try:\\\\n                            # \\\\u6267\\\\u884c\\\\u5de5\\\\u5177\\\\n                            result = await self.server.execute_tool(tool_name, tool_args)\\\\n                            logger.info(f\\\\\"\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5b8c\\\\u6210: {tool_name}\\\\\")\\\\n                            # \\\\u5c06\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u6dfb\\\\u52a0\\\\u5230 tool_outputs \\\\u5217\\\\u8868\\\\n                            tool_outputs.append({\\\\n                                \\\\\"tool_call_id\\\\\": tool_call.id,\\\\n                                # MCP tool results have \\'content\\' which is a list of Parts\\\\n                                # Assuming text content is the first part\\\\n                                \\\\\"output\\\\\": result.content[0].text\\\\n                            })\\\\n                        except Exception as tool_exec_error:\\\\n                            logger.error(f\\\\\"\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5931\\\\u8d25 {tool_name}: {str(tool_exec_error)}\\\\\")\\\\n                            # \\\\u5982\\\\u679c\\\\u5de5\\\\u5177\\\\u6267\\\\u884c\\\\u5931\\\\u8d25\\\\uff0c\\\\u4e5f\\\\u9700\\\\u8981\\\\u5411\\\\u6a21\\\\u578b\\\\u53cd\\\\u9988\\\\u5931\\\\u8d25\\\\u4fe1\\\\u606f\\\\n                            tool_outputs.append({\\\\n                                \\\\\"tool_call_id\\\\\": tool_call.id,\\\\n                                \\\\\"output\\\\\": f\\\\\"Error executing tool {tool_name}: {str(tool_exec_error)}\\\\\"\\\\n                            })\\\\n\\\\n                    # \\\\u5c06\\\\u6240\\\\u6709\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u4f5c\\\\u4e3a tool \\\\u6d88\\\\u606f\\\\u6dfb\\\\u52a0\\\\u5230\\\\u5386\\\\u53f2\\\\n                    # \\\\u6ce8\\\\u610f\\\\uff1aOpenAI API \\\\u8981\\\\u6c42\\\\u6bcf\\\\u4e2a tool_call_id \\\\u5bf9\\\\u5e94\\\\u4e00\\\\u4e2a tool \\\\u6d88\\\\u606f\\\\n                    # \\\\u5982\\\\u679c\\\\u6a21\\\\u578b\\\\u9884\\\\u6d4b\\\\u4e86\\\\u591a\\\\u4e2a\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\uff0c\\\\u8fd9\\\\u91cc\\\\u9700\\\\u8981\\\\u4e3a\\\\u6bcf\\\\u4e2a\\\\u7ed3\\\\u679c\\\\u6dfb\\\\u52a0 tool \\\\u6d88\\\\u606f\\\\n                    for output in tool_outputs:\\\\n                        messages.append({\\\\n                            \\\\\"role\\\\\": \\\\\"tool\\\\\",\\\\n                            \\\\\"content\\\\\": output[\\\\\"output\\\\\"],\\\\n                            \\\\\"tool_call_id\\\\\": output[\\\\\"tool_call_id\\\\\"],\\\\n                        })\\\\n\\\\n                    # \\\\u7ee7\\\\u7eed\\\\u4e0b\\\\u4e00\\\\u8f6e\\\\u5faa\\\\u73af\\\\uff0c\\\\u5c06\\\\u5b8c\\\\u6574\\\\u7684\\\\u6d88\\\\u606f\\\\u5386\\\\u53f2\\\\u53d1\\\\u9001\\\\u7ed9\\\\u6a21\\\\u578b\\\\uff0c\\\\u8ba9\\\\u5b83\\\\u6839\\\\u636e\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u7ee7\\\\u7eed\\\\n                    logger.info(f\\\\\"\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u56de\\\\u5408\\\\u5b8c\\\\u6210\\\\uff0c\\\\u7ee7\\\\u7eed\\\\u4e0e\\\\u6a21\\\\u578b\\\\u4ea4\\\\u4e92...\\\\\")\\\\n                    continue # \\\\u7ee7\\\\u7eed\\\\u5faa\\\\u73af\\\\uff0c\\\\u53d1\\\\u9001\\\\u5305\\\\u542b\\\\u5de5\\\\u5177\\\\u7ed3\\\\u679c\\\\u7684\\\\u6d88\\\\u606f\\\\u5386\\\\u53f2\\\\u7ed9\\\\u6a21\\\\u578b\\\\n                else:\\\\n                    # \\\\u53d1\\\\u751f\\\\u672a\\\\u77e5\\\\u60c5\\\\u51b5\\\\u6216\\\\u9519\\\\u8bef\\\\n                    logger.error(f\\\\\"\\\\u6a21\\\\u578b\\\\u8fd4\\\\u56de\\\\u610f\\\\u5916\\\\u7684 finish_reason: {finish_reason}\\\\\")\\\\n                    return f\\\\\"\\\\u5904\\\\u7406\\\\u4f60\\\\u7684\\\\u67e5\\\\u8be2\\\\u65f6\\\\u9047\\\\u5230\\\\u610f\\\\u5916\\\\u60c5\\\\u51b5 ({finish_reason})\\\\\"\\\\n\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u5904\\\\u7406\\\\u67e5\\\\u8be2\\\\u65f6\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n                return f\\\\\"\\\\u5904\\\\u7406\\\\u4f60\\\\u7684\\\\u67e5\\\\u8be2\\\\u65f6\\\\u53d1\\\\u751f\\\\u9519\\\\u8bef: {str(e)}\\\\\"\\\\n\\\\n        # \\\\u5982\\\\u679c\\\\u5faa\\\\u73af\\\\u8fbe\\\\u5230\\\\u6700\\\\u5927\\\\u6b21\\\\u6570\\\\u4ecd\\\\u672a\\\\u5f97\\\\u5230\\\\u6700\\\\u7ec8\\\\u56de\\\\u590d\\\\n        logger.warning(\\\\\"\\\\u8fbe\\\\u5230\\\\u6700\\\\u5927\\\\u5de5\\\\u5177\\\\u8c03\\\\u7528\\\\u56de\\\\u5408\\\\u6570\\\\uff0c\\\\u672a\\\\u80fd\\\\u83b7\\\\u5f97\\\\u6700\\\\u7ec8\\\\u56de\\\\u590d\\\\\")\\\\n        return \\\\\"\\\\u62b1\\\\u6b49\\\\uff0c\\\\u5904\\\\u7406\\\\u4f60\\\\u7684\\\\u8bf7\\\\u6c42\\\\u65f6\\\\u51fa\\\\u73b0\\\\u95ee\\\\u9898\\\\uff0c\\\\u672a\\\\u80fd\\\\u83b7\\\\u5f97\\\\u6700\\\\u7ec8\\\\u7ed3\\\\u679c\\\\u3002\\\\\"\\\\n\\\\n    async def chat_loop(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u8fd0\\\\u884c\\\\u4ea4\\\\u4e92\\\\u5f0f\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\\"\\\\\"\\\\\"\\\\n        logger.info(\\\\\"\\\\u5f00\\\\u59cb\\\\u804a\\\\u5929\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n        \\\\n        while True:\\\\n            try:\\\\n                query = input(\\\\\"\\\\\\\\n\\\\u8bf7\\\\u8f93\\\\u5165\\\\u4f60\\\\u7684\\\\u95ee\\\\u9898 (\\\\u8f93\\\\u5165 \\'quit\\' \\\\u6216 \\'exit\\' \\\\u9000\\\\u51fa): \\\\\").strip()\\\\n                \\\\n                if query.lower() in [\\'quit\\', \\'exit\\']:\\\\n                    logger.info(\\\\\"\\\\u7ed3\\\\u675f\\\\u804a\\\\u5929\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n                    break\\\\n                    \\\\n                response = await self.process_query(query)\\\\n                print(f\\\\\"\\\\\\\\n\\\\u52a9\\\\u624b: {response}\\\\\")\\\\n                \\\\n            except KeyboardInterrupt:\\\\n                logger.info(\\\\\"\\\\\\\\n\\\\u6536\\\\u5230\\\\u952e\\\\u76d8\\\\u4e2d\\\\u65ad\\\\uff0c\\\\u7ed3\\\\u675f\\\\u4f1a\\\\u8bdd...\\\\\")\\\\n                break\\\\n            except Exception as e:\\\\n                logger.error(f\\\\\"\\\\u804a\\\\u5929\\\\u5faa\\\\u73af\\\\u4e2d\\\\u51fa\\\\u9519: {str(e)}\\\\\")\\\\n                print(f\\\\\"\\\\\\\\n\\\\u53d1\\\\u751f\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n                \\\\n    async def cleanup(self) -> None:\\\\n        \\\\\"\\\\\"\\\\\"\\\\u6e05\\\\u7406\\\\u5ba2\\\\u6237\\\\u7aef\\\\u8d44\\\\u6e90\\\\\"\\\\\"\\\\\"\\\\n        if self.server:\\\\n            await self.server.cleanup()\\\\n\\\\nasync def main() -> None:\\\\n    \\\\\"\\\\\"\\\\\"\\\\u4e3b\\\\u7a0b\\\\u5e8f\\\\u5165\\\\u53e3\\\\\"\\\\\"\\\\\"\\\\n    try:\\\\n        config = Configuration()\\\\n        client = MCPClient(config)\\\\n        \\\\n        await client.initialize()\\\\n        await client.chat_loop()\\\\n        \\\\n    except Exception as e:\\\\n        logger.error(f\\\\\"\\\\u81f4\\\\u547d\\\\u9519\\\\u8bef: {str(e)}\\\\\")\\\\n        raise\\\\n        \\\\n    finally:\\\\n        if \\'client\\' in locals():\\\\n            await client.cleanup()\\\\n\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    try:\\\\n        asyncio.run(main())\\\\n    except KeyboardInterrupt:\\\\n        logger.info(\\\\\"\\\\u7a0b\\\\u5e8f\\\\u88ab\\\\u7528\\\\u6237\\\\u7ec8\\\\u6b62\\\\\")\\\\n    except Exception as e:\\\\n        logger.error(f\\\\\"\\\\u7a0b\\\\u5e8f\\\\u56e0\\\\u9519\\\\u8bef\\\\u7ec8\\\\u6b62: {str(e)}\\\\\")\\\\n        raise\\\\n\", \"timestamp\": \"2025-10-30T06:17:09+0000\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"read_project_file\", \"args\": {\"name\": \"weather_server.py\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"#!/usr/bin/env python3\\\\n# -*- coding: utf-8 -*-\\\\n\\\\n\\\\\"\\\\\"\\\\\"\\\\nMCP \\\\u670d\\\\u52a1\\\\u5668\\\\u53ef\\\\u4ee5\\\\u63d0\\\\u4f9b\\\\u4e09\\\\u79cd\\\\u4e3b\\\\u8981\\\\u7c7b\\\\u578b\\\\u7684\\\\u529f\\\\u80fd\\\\uff1a\\\\n\\\\n\\\\u8d44\\\\u6e90\\\\uff1a\\\\u5ba2\\\\u6237\\\\u7aef\\\\u53ef\\\\u4ee5\\\\u8bfb\\\\u53d6\\\\u7684\\\\u7c7b\\\\u4f3c\\\\u6587\\\\u4ef6\\\\u7684\\\\u6570\\\\u636e\\\\uff08\\\\u4f8b\\\\u5982 API \\\\u54cd\\\\u5e94\\\\u6216\\\\u6587\\\\u4ef6\\\\u5185\\\\u5bb9\\\\uff09\\\\n\\\\u5de5\\\\u5177\\\\uff1a\\\\u53ef\\\\u7531 LLM \\\\u8c03\\\\u7528\\\\u7684\\\\u51fd\\\\u6570\\\\uff08\\\\u7ecf\\\\u7528\\\\u6237\\\\u6279\\\\u51c6\\\\uff09\\\\n\\\\u63d0\\\\u793a\\\\uff1a\\\\u9884\\\\u5148\\\\u7f16\\\\u5199\\\\u7684\\\\u6a21\\\\u677f\\\\uff0c\\\\u5e2e\\\\u52a9\\\\u7528\\\\u6237\\\\u5b8c\\\\u6210\\\\u7279\\\\u5b9a\\\\u4efb\\\\u52a1\\\\n\\\\n######################################\\\\n\\\\nMCP \\\\u5929\\\\u6c14\\\\u670d\\\\u52a1\\\\u5668\\\\n\\\\n\\\\u63d0\\\\u4f9b\\\\u4e24\\\\u4e2a\\\\u5de5\\\\u5177\\\\uff1a\\\\n1. get_weather_warning: \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u7684\\\\u5929\\\\u6c14\\\\u707e\\\\u5bb3\\\\u9884\\\\u8b66\\\\n2. get_daily_forecast: \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\n\\\\nAuthor: FlyAIBox\\\\nDate: 2025.10.11\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\nfrom typing import Any, Dict, List, Optional, Union\\\\nimport asyncio\\\\nimport httpx\\\\nimport os\\\\nfrom urllib.parse import urljoin\\\\nfrom mcp.server.fastmcp import FastMCP\\\\nfrom dotenv import load_dotenv\\\\nfrom pathlib import Path\\\\n\\\\n# \\\\u52a0\\\\u8f7d .env \\\\u6587\\\\u4ef6\\\\u4e2d\\\\u7684\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\ndotenv_path = Path(__file__).resolve().parents[1] / \\'.env\\'\\\\nload_dotenv(dotenv_path)\\\\n\\\\n# \\\\u521d\\\\u59cb\\\\u5316 FastMCP \\\\u670d\\\\u52a1\\\\u5668\\\\nmcp = FastMCP(\\\\\"weather\\\\\",  # \\\\u670d\\\\u52a1\\\\u5668\\\\u540d\\\\u79f0\\\\n              debug=True,  # \\\\u542f\\\\u7528\\\\u8c03\\\\u8bd5\\\\u6a21\\\\u5f0f\\\\uff0c\\\\u4f1a\\\\u8f93\\\\u51fa\\\\u8be6\\\\u7ec6\\\\u65e5\\\\u5fd7\\\\n              host=\\\\\"0.0.0.0\\\\\") # \\\\u76d1\\\\u542c\\\\u6240\\\\u6709\\\\u7f51\\\\u7edc\\\\u63a5\\\\u53e3\\\\uff0c\\\\u5141\\\\u8bb8\\\\u8fdc\\\\u7a0b\\\\u8fde\\\\u63a5\\\\n\\\\n# \\\\u4ece\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\u4e2d\\\\u8bfb\\\\u53d6\\\\u5e38\\\\u91cf\\\\nQWEATHER_API_BASE = os.getenv(\\\\\"QWEATHER_API_BASE\\\\\")\\\\nQWEATHER_API_KEY = os.getenv(\\\\\"QWEATHER_API_KEY\\\\\")\\\\n\\\\ndef _normalize_base_url(raw_base: Optional[str]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u786e\\\\u4fdd\\\\u57fa\\\\u7840 URL \\\\u5305\\\\u542b\\\\u534f\\\\u8bae\\\\u5e76\\\\u4ee5\\\\u5355\\\\u4e2a\\\\u659c\\\\u6760\\\\u7ed3\\\\u5c3e\\\\uff0c\\\\u517c\\\\u5bb9 .env \\\\u4e2d\\\\u672a\\\\u5199\\\\u534f\\\\u8bae\\\\u7684\\\\u60c5\\\\u51b5\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    if not raw_base:\\\\n        raise RuntimeError(\\\\\"\\\\u672a\\\\u914d\\\\u7f6e QWEATHER_API_BASE \\\\u73af\\\\u5883\\\\u53d8\\\\u91cf\\\\\")\\\\n\\\\n    base = raw_base.strip()\\\\n    if not base.startswith((\\\\\"http://\\\\\", \\\\\"https://\\\\\")):\\\\n        base = f\\\\\"https://{base.lstrip(\\'/\\')}\\\\\"\\\\n\\\\n    # urljoin \\\\u8981\\\\u6c42\\\\u76ee\\\\u5f55\\\\u98ce\\\\u683c\\\\u4ee5\\\\u659c\\\\u6760\\\\u7ed3\\\\u5c3e\\\\uff0c\\\\u907f\\\\u514d \\'v7/weather/7d\\' \\\\u88ab\\\\u8986\\\\u76d6\\\\n    if not base.endswith(\\\\\"/\\\\\"):\\\\n        base = f\\\\\"{base}/\\\\\"\\\\n\\\\n    return base\\\\n\\\\ntry:\\\\n    _QWEATHER_BASE_URL = _normalize_base_url(QWEATHER_API_BASE)\\\\nexcept RuntimeError as err:\\\\n    print(f\\\\\"[\\\\u914d\\\\u7f6e\\\\u9519\\\\u8bef] {err}\\\\\")\\\\n    _QWEATHER_BASE_URL = None\\\\n\\\\nasync def make_qweather_request(endpoint: str, params: Dict[str, Any]) -> Optional[Dict[str, Any]]:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u5411\\\\u548c\\\\u98ce\\\\u5929\\\\u6c14 API \\\\u53d1\\\\u9001\\\\u8bf7\\\\u6c42\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        endpoint: API \\\\u7aef\\\\u70b9\\\\u8def\\\\u5f84\\\\uff08\\\\u4e0d\\\\u5305\\\\u542b\\\\u57fa\\\\u7840 URL\\\\uff09\\\\n        params: API \\\\u8bf7\\\\u6c42\\\\u7684\\\\u53c2\\\\u6570\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u6210\\\\u529f\\\\u65f6\\\\u8fd4\\\\u56de JSON \\\\u54cd\\\\u5e94\\\\uff0c\\\\u5931\\\\u8d25\\\\u65f6\\\\u8fd4\\\\u56de None\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    if not _QWEATHER_BASE_URL:\\\\n        print(\\\\\"QWEATHER_API_BASE \\\\u672a\\\\u6b63\\\\u786e\\\\u914d\\\\u7f6e\\\\uff0c\\\\u5df2\\\\u8df3\\\\u8fc7\\\\u8bf7\\\\u6c42\\\\u3002\\\\\")\\\\n        return None\\\\n\\\\n    if not QWEATHER_API_KEY:\\\\n        print(\\\\\"QWEATHER_API_KEY \\\\u672a\\\\u8bbe\\\\u7f6e\\\\uff0c\\\\u5df2\\\\u8df3\\\\u8fc7\\\\u8bf7\\\\u6c42\\\\u3002\\\\\")\\\\n        return None\\\\n\\\\n    safe_endpoint = endpoint.lstrip(\\\\\"/\\\\\")\\\\n    url = urljoin(_QWEATHER_BASE_URL, safe_endpoint)\\\\n\\\\n    # \\\\u4f7f\\\\u7528 Header \\\\u65b9\\\\u5f0f\\\\u8ba4\\\\u8bc1\\\\uff08\\\\u548c\\\\u98ce\\\\u5929\\\\u6c14\\\\u7684\\\\u65b0\\\\u7248\\\\u672cAPI\\\\uff09\\\\n    headers = {\\\\n        \\\\\"X-QW-Api-Key\\\\\": QWEATHER_API_KEY\\\\n    }\\\\n    \\\\n    async with httpx.AsyncClient() as client:\\\\n        try:\\\\n            print(f\\\\\"\\\\u8bf7\\\\u6c42 URL: {url}\\\\\")\\\\n            print(f\\\\\"\\\\u8bf7\\\\u6c42\\\\u53c2\\\\u6570: {params}\\\\\")\\\\n            response = await client.get(url, params=params, headers=headers, timeout=30.0)\\\\n            print(f\\\\\"\\\\u54cd\\\\u5e94\\\\u72b6\\\\u6001\\\\u7801: {response.status_code}\\\\\")\\\\n            response.raise_for_status()\\\\n            result = response.json()\\\\n            print(f\\\\\"\\\\u54cd\\\\u5e94\\\\u5185\\\\u5bb9: {result}\\\\\")\\\\n            return result\\\\n        except httpx.HTTPStatusError as e:\\\\n            print(f\\\\\"HTTP \\\\u72b6\\\\u6001\\\\u9519\\\\u8bef: {e.response.status_code} - {e.response.text}\\\\\")\\\\n            return None\\\\n        except Exception as e:\\\\n            print(f\\\\\"API \\\\u8bf7\\\\u6c42\\\\u9519\\\\u8bef: {type(e).__name__}: {e}\\\\\")\\\\n            return None\\\\n\\\\ndef format_warning(warning: Dict[str, Any]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u5c06\\\\u5929\\\\u6c14\\\\u9884\\\\u8b66\\\\u6570\\\\u636e\\\\u683c\\\\u5f0f\\\\u5316\\\\u4e3a\\\\u53ef\\\\u8bfb\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        warning: \\\\u5929\\\\u6c14\\\\u9884\\\\u8b66\\\\u6570\\\\u636e\\\\u5bf9\\\\u8c61\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u540e\\\\u7684\\\\u9884\\\\u8b66\\\\u4fe1\\\\u606f\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    return f\\\\\"\\\\\"\\\\\"\\\\n\\\\u9884\\\\u8b66ID: {warning.get(\\'id\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u6807\\\\u9898: {warning.get(\\'title\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u53d1\\\\u5e03\\\\u65f6\\\\u95f4: {warning.get(\\'pubTime\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u5f00\\\\u59cb\\\\u65f6\\\\u95f4: {warning.get(\\'startTime\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u7ed3\\\\u675f\\\\u65f6\\\\u95f4: {warning.get(\\'endTime\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u9884\\\\u8b66\\\\u7c7b\\\\u578b: {warning.get(\\'typeName\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u9884\\\\u8b66\\\\u7b49\\\\u7ea7: {warning.get(\\'severity\\', \\'\\\\u672a\\\\u77e5\\')} ({warning.get(\\'severityColor\\', \\'\\\\u672a\\\\u77e5\\')})\\\\n\\\\u53d1\\\\u5e03\\\\u5355\\\\u4f4d: {warning.get(\\'sender\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u72b6\\\\u6001: {warning.get(\\'status\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u8be6\\\\u7ec6\\\\u4fe1\\\\u606f: {warning.get(\\'text\\', \\'\\\\u65e0\\\\u8be6\\\\u7ec6\\\\u4fe1\\\\u606f\\')}\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\n@mcp.tool()\\\\nasync def get_weather_warning(location: Union[str, int]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u4f4d\\\\u7f6e\\\\u7684\\\\u5929\\\\u6c14\\\\u707e\\\\u5bb3\\\\u9884\\\\u8b66\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        location: \\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u5750\\\\u6807\\\\uff08\\\\u7ecf\\\\u5ea6,\\\\u7eac\\\\u5ea6\\\\uff09\\\\n                \\\\u4f8b\\\\u5982\\\\uff1a\\'101010100\\'\\\\uff08\\\\u5317\\\\u4eac\\\\uff09\\\\u6216 \\'116.41,39.92\\'\\\\n                \\\\u4e5f\\\\u53ef\\\\u4ee5\\\\u76f4\\\\u63a5\\\\u4f20\\\\u5165\\\\u6570\\\\u5b57ID\\\\uff0c\\\\u5982 101010100\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u7684\\\\u9884\\\\u8b66\\\\u4fe1\\\\u606f\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    # \\\\u786e\\\\u4fdd location \\\\u4e3a\\\\u5b57\\\\u7b26\\\\u4e32\\\\u7c7b\\\\u578b\\\\n    location = str(location)\\\\n    \\\\n    params = {\\\\n        \\\\\"location\\\\\": location,\\\\n        \\\\\"lang\\\\\": \\\\\"zh\\\\\"\\\\n    }\\\\n    \\\\n    data = await make_qweather_request(\\\\\"v7/warning/now\\\\\", params)\\\\n    \\\\n    if not data:\\\\n        return \\\\\"\\\\u65e0\\\\u6cd5\\\\u83b7\\\\u53d6\\\\u9884\\\\u8b66\\\\u4fe1\\\\u606f\\\\u6216API\\\\u8bf7\\\\u6c42\\\\u5931\\\\u8d25\\\\u3002\\\\\"\\\\n    \\\\n    if data.get(\\\\\"code\\\\\") != \\\\\"200\\\\\":\\\\n        return f\\\\\"API \\\\u8fd4\\\\u56de\\\\u9519\\\\u8bef: {data.get(\\'code\\')}\\\\\"\\\\n    \\\\n    warnings = data.get(\\\\\"warning\\\\\", [])\\\\n    \\\\n    if not warnings:\\\\n        return f\\\\\"\\\\u5f53\\\\u524d\\\\u4f4d\\\\u7f6e {location} \\\\u6ca1\\\\u6709\\\\u6d3b\\\\u52a8\\\\u9884\\\\u8b66\\\\u3002\\\\\"\\\\n    \\\\n    formatted_warnings = [format_warning(warning) for warning in warnings]\\\\n    return \\\\\"\\\\\\\\n---\\\\\\\\n\\\\\".join(formatted_warnings)\\\\n\\\\ndef format_daily_forecast(daily: Dict[str, Any]) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u5c06\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6570\\\\u636e\\\\u683c\\\\u5f0f\\\\u5316\\\\u4e3a\\\\u53ef\\\\u8bfb\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        daily: \\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6570\\\\u636e\\\\u5bf9\\\\u8c61\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u540e\\\\u7684\\\\u9884\\\\u62a5\\\\u4fe1\\\\u606f\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    return f\\\\\"\\\\\"\\\\\"\\\\n\\\\u65e5\\\\u671f: {daily.get(\\'fxDate\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u65e5\\\\u51fa: {daily.get(\\'sunrise\\', \\'\\\\u672a\\\\u77e5\\')}  \\\\u65e5\\\\u843d: {daily.get(\\'sunset\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u6700\\\\u9ad8\\\\u6e29\\\\u5ea6: {daily.get(\\'tempMax\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u00b0C  \\\\u6700\\\\u4f4e\\\\u6e29\\\\u5ea6: {daily.get(\\'tempMin\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u00b0C\\\\n\\\\u767d\\\\u5929\\\\u5929\\\\u6c14: {daily.get(\\'textDay\\', \\'\\\\u672a\\\\u77e5\\')}  \\\\u591c\\\\u95f4\\\\u5929\\\\u6c14: {daily.get(\\'textNight\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u767d\\\\u5929\\\\u98ce\\\\u5411: {daily.get(\\'windDirDay\\', \\'\\\\u672a\\\\u77e5\\')} {daily.get(\\'windScaleDay\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u7ea7 ({daily.get(\\'windSpeedDay\\', \\'\\\\u672a\\\\u77e5\\')}km/h)\\\\n\\\\u591c\\\\u95f4\\\\u98ce\\\\u5411: {daily.get(\\'windDirNight\\', \\'\\\\u672a\\\\u77e5\\')} {daily.get(\\'windScaleNight\\', \\'\\\\u672a\\\\u77e5\\')}\\\\u7ea7 ({daily.get(\\'windSpeedNight\\', \\'\\\\u672a\\\\u77e5\\')}km/h)\\\\n\\\\u76f8\\\\u5bf9\\\\u6e7f\\\\u5ea6: {daily.get(\\'humidity\\', \\'\\\\u672a\\\\u77e5\\')}%\\\\n\\\\u964d\\\\u6c34\\\\u91cf: {daily.get(\\'precip\\', \\'\\\\u672a\\\\u77e5\\')}mm\\\\n\\\\u7d2b\\\\u5916\\\\u7ebf\\\\u6307\\\\u6570: {daily.get(\\'uvIndex\\', \\'\\\\u672a\\\\u77e5\\')}\\\\n\\\\u80fd\\\\u89c1\\\\u5ea6: {daily.get(\\'vis\\', \\'\\\\u672a\\\\u77e5\\')}km\\\\n\\\\\"\\\\\"\\\\\"\\\\n\\\\n@mcp.tool()\\\\nasync def get_daily_forecast(location: Union[str, int], days: int = 3) -> str:\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    \\\\u83b7\\\\u53d6\\\\u6307\\\\u5b9a\\\\u4f4d\\\\u7f6e\\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\n    \\\\n    \\\\u53c2\\\\u6570:\\\\n        location: \\\\u57ce\\\\u5e02ID\\\\u6216\\\\u7ecf\\\\u7eac\\\\u5ea6\\\\u5750\\\\u6807\\\\uff08\\\\u7ecf\\\\u5ea6,\\\\u7eac\\\\u5ea6\\\\uff09\\\\n                \\\\u4f8b\\\\u5982\\\\uff1a\\'101010100\\'\\\\uff08\\\\u5317\\\\u4eac\\\\uff09\\\\u6216 \\'116.41,39.92\\'\\\\n                \\\\u4e5f\\\\u53ef\\\\u4ee5\\\\u76f4\\\\u63a5\\\\u4f20\\\\u5165\\\\u6570\\\\u5b57ID\\\\uff0c\\\\u5982 101010100\\\\n        days: \\\\u9884\\\\u62a5\\\\u5929\\\\u6570\\\\uff0c\\\\u53ef\\\\u9009\\\\u503c\\\\u4e3a 3\\\\u30017\\\\u300110\\\\u300115\\\\u300130\\\\uff0c\\\\u9ed8\\\\u8ba4\\\\u4e3a 3\\\\n        \\\\n    \\\\u8fd4\\\\u56de:\\\\n        \\\\u683c\\\\u5f0f\\\\u5316\\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u5b57\\\\u7b26\\\\u4e32\\\\n    \\\\\"\\\\\"\\\\\"\\\\n    # \\\\u786e\\\\u4fdd location \\\\u4e3a\\\\u5b57\\\\u7b26\\\\u4e32\\\\u7c7b\\\\u578b\\\\n    location = str(location)\\\\n    \\\\n    # \\\\u786e\\\\u4fdd days \\\\u53c2\\\\u6570\\\\u6709\\\\u6548\\\\n    valid_days = [3, 7, 10, 15, 30]\\\\n    if days not in valid_days:\\\\n        days = 3  # \\\\u9ed8\\\\u8ba4\\\\u4f7f\\\\u75283\\\\u5929\\\\u9884\\\\u62a5\\\\n    \\\\n    params = {\\\\n        \\\\\"location\\\\\": location,\\\\n        \\\\\"lang\\\\\": \\\\\"zh\\\\\"\\\\n    }\\\\n    # \\\\u548c\\\\u98ce\\\\u5929\\\\u6c14API\\\\u6587\\\\u6863 https://dev.qweather.com/docs/api/weather/weather-daily-forecast/\\\\n    endpoint = f\\\\\"v7/weather/{days}d\\\\\"\\\\n    data = await make_qweather_request(endpoint, params)\\\\n    \\\\n    if not data:\\\\n        return \\\\\"\\\\u65e0\\\\u6cd5\\\\u83b7\\\\u53d6\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6216API\\\\u8bf7\\\\u6c42\\\\u5931\\\\u8d25\\\\u3002\\\\\"\\\\n    \\\\n    if data.get(\\\\\"code\\\\\") != \\\\\"200\\\\\":\\\\n        return f\\\\\"API \\\\u8fd4\\\\u56de\\\\u9519\\\\u8bef: {data.get(\\'code\\')}\\\\\"\\\\n    \\\\n    daily_forecasts = data.get(\\\\\"daily\\\\\", [])\\\\n    \\\\n    if not daily_forecasts:\\\\n        return f\\\\\"\\\\u65e0\\\\u6cd5\\\\u83b7\\\\u53d6 {location} \\\\u7684\\\\u5929\\\\u6c14\\\\u9884\\\\u62a5\\\\u6570\\\\u636e\\\\u3002\\\\\"\\\\n    \\\\n    formatted_forecasts = [format_daily_forecast(daily) for daily in daily_forecasts]\\\\n    return \\\\\"\\\\\\\\n---\\\\\\\\n\\\\\".join(formatted_forecasts)\\\\n\\\\nif __name__ == \\\\\"__main__\\\\\":\\\\n    print(\\\\\"\\\\u6b63\\\\u5728\\\\u542f\\\\u52a8 MCP \\\\u5929\\\\u6c14\\\\u670d\\\\u52a1\\\\u5668...\\\\\")\\\\n    print(\\\\\"\\\\u63d0\\\\u4f9b\\\\u5de5\\\\u5177: get_weather_warning, get_daily_forecast\\\\\")\\\\n    print(\\\\\"\\\\u8bf7\\\\u786e\\\\u4fdd\\\\u73af\\\\u5883\\\\u53d8\\\\u91cf QWEATHER_API_KEY \\\\u5df2\\\\u8bbe\\\\u7f6e\\\\\")\\\\n    print(\\\\\"\\\\u4f7f\\\\u7528 Ctrl+C \\\\u505c\\\\u6b62\\\\u670d\\\\u52a1\\\\u5668\\\\\")\\\\n    \\\\n    # \\\\u521d\\\\u59cb\\\\u5316\\\\u5e76\\\\u8fd0\\\\u884c\\\\u670d\\\\u52a1\\\\u5668\\\\n    mcp.run(transport=\\'stdio\\') \\\\n\", \"timestamp\": \"2025-10-30T06:17:10+0000\"}'}, {'type': 'assistant', 'content': '{\"tool\": \"terminate\", \"args\": {\"message\": \"# Project Overview\\\\nThis project is an MCP (Model Context Protocol) client and server implementation integrated with DeepSeek\\'s API to handle weather information queries. The project includes two primary Python scripts built to demonstrate a functional model context protocol capable of interacting with DeepSeek tools.\\\\n\\\\n### Components\\\\n1. **mcp_client_deepseek.py**\\\\n   - Implements an MCP client using the DeepSeek API.\\\\n   - Demonstrates how to connect to the MCP server, handle queries, and maintain an interactive chat loop.\\\\n   - Requires environment variables:\\\\n     - `DEEPSEEK_API_KEY`: API key for DeepSeek access.\\\\n     - `DEEPSEEK_BASE_URL`: Base URL for the DeepSeek API.\\\\n     - `DEEPSEEK_MODEL`: Name of the DeepSeek model (e.g., deepseek-chat).\\\\n\\\\n2. **weather_server.py**\\\\n   - An MCP server providing weather-related tools.\\\\n   - Offers tools for getting weather warnings and daily forecasts.\\\\n   - Utilizes the QWeather API, requiring configuration of:\\\\n     - `QWEATHER_API_BASE`: Base URL for the QWeather API.\\\\n     - `QWEATHER_API_KEY`: API key for QWeather access.\\\\n   - These tools include:\\\\n     - `get_weather_warning`: Fetches weather disaster warnings for a given city ID or coordinates.\\\\n     - `get_daily_forecast`: Fetches weather forecasts for specified city IDs or coordinates.\\\\n\\\\n### Author\\\\nFlyAIBox\\\\n\\\\n### Date\\\\n2025.10.11\\\\n\\\\nThis project requires the correct setup of environment variables to function, especially those related to API access. Please ensure all necessary configurations are completed before running the scripts.\"}}'}, {'type': 'environment', 'content': '{\"tool_executed\": true, \"result\": \"# Project Overview\\\\nThis project is an MCP (Model Context Protocol) client and server implementation integrated with DeepSeek\\'s API to handle weather information queries. The project includes two primary Python scripts built to demonstrate a functional model context protocol capable of interacting with DeepSeek tools.\\\\n\\\\n### Components\\\\n1. **mcp_client_deepseek.py**\\\\n   - Implements an MCP client using the DeepSeek API.\\\\n   - Demonstrates how to connect to the MCP server, handle queries, and maintain an interactive chat loop.\\\\n   - Requires environment variables:\\\\n     - `DEEPSEEK_API_KEY`: API key for DeepSeek access.\\\\n     - `DEEPSEEK_BASE_URL`: Base URL for the DeepSeek API.\\\\n     - `DEEPSEEK_MODEL`: Name of the DeepSeek model (e.g., deepseek-chat).\\\\n\\\\n2. **weather_server.py**\\\\n   - An MCP server providing weather-related tools.\\\\n   - Offers tools for getting weather warnings and daily forecasts.\\\\n   - Utilizes the QWeather API, requiring configuration of:\\\\n     - `QWEATHER_API_BASE`: Base URL for the QWeather API.\\\\n     - `QWEATHER_API_KEY`: API key for QWeather access.\\\\n   - These tools include:\\\\n     - `get_weather_warning`: Fetches weather disaster warnings for a given city ID or coordinates.\\\\n     - `get_daily_forecast`: Fetches weather forecasts for specified city IDs or coordinates.\\\\n\\\\n### Author\\\\nFlyAIBox\\\\n\\\\n### Date\\\\n2025.10.11\\\\n\\\\nThis project requires the correct setup of environment variables to function, especially those related to API access. Please ensure all necessary configurations are completed before running the scripts.\\\\nTerminating...\", \"timestamp\": \"2025-10-30T06:17:15+0000\"}'}]\n"
          ]
        }
      ],
      "source": [
        "# =============================== 示例：最小可运行 Agent ===============================\n",
        "# 1) 定义智能体目标（Goals）：\n",
        "#    - 读取项目中的每个文件\n",
        "#    - 当已读取完毕时调用 terminate，并在消息中提供 README 的内容（示例环境如为空目录会直接终止）\n",
        "goals = [\n",
        "    Goal(priority=1, name=\"Gather Information\", description=\"Read each file in the project\"),\n",
        "    Goal(priority=1, name=\"Terminate\", description=\"Call the terminate call when you have read all the files \"\n",
        "                                                   \"and provide the content of the README in the terminate message\")\n",
        "]\n",
        "\n",
        "# 2) 指定语言适配器（基于函数调用的 Prompt/解析策略）\n",
        "agent_language = AgentFunctionCallingActionLanguage()\n",
        "\n",
        "# 3) 实现底层动作：读取文件\n",
        "def read_project_file(name: str) -> str:\n",
        "    with open(name, \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# 4) 实现底层动作：列出当前目录下的 .py 文件（最小示例）\n",
        "def list_project_files() -> List[str]:\n",
        "    return sorted([file for file in os.listdir(\".\") if file.endswith(\".py\")])\n",
        "\n",
        "\n",
        "# 5) 注册动作：将 Python 函数“暴露”为可被 LLM 选择的工具\n",
        "action_registry = ActionRegistry()\n",
        "action_registry.register(Action(\n",
        "    name=\"list_project_files\",\n",
        "    function=list_project_files,\n",
        "    description=\"Lists all files in the project.\",\n",
        "    parameters={},\n",
        "    terminal=False\n",
        "))\n",
        "action_registry.register(Action(\n",
        "    name=\"read_project_file\",\n",
        "    function=read_project_file,\n",
        "    description=\"Reads a file from the project.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"name\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": [\"name\"]\n",
        "    },\n",
        "    terminal=False\n",
        "))\n",
        "action_registry.register(Action(\n",
        "    name=\"terminate\",\n",
        "    function=lambda message: f\"{message}\\nTerminating...\",\n",
        "    description=\"Terminates the session and prints the message to the user.\",\n",
        "    parameters={\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"message\": {\"type\": \"string\"}\n",
        "        },\n",
        "        \"required\": []\n",
        "    },\n",
        "    terminal=True\n",
        "))\n",
        "\n",
        "# 6) 准备环境（负责真实执行动作并返回标准化结果）\n",
        "environment = Environment()\n",
        "\n",
        "# 7) 构建 Agent 实例（组装 G/A/M/E 与 LLM 响应函数）\n",
        "agent = Agent(goals, agent_language, action_registry, generate_response, environment)\n",
        "\n",
        "# 8) 运行智能体（输入一个自然语言任务），内部会进入循环直到触发终止或达到最大轮数\n",
        "user_input = \"Write a README for this project.\"\n",
        "final_memory = agent.run(user_input)\n",
        "\n",
        "# 9) 输出最终的记忆（包含用户任务、助手决策、环境执行结果等）\n",
        "print(final_memory.get_memories())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 将智能体运行结果以 Markdown 形式美化展示\n",
        "from IPython.display import display, Markdown\n",
        "import json\n",
        "\n",
        "def _format_env_result(env_json_str: str) -> str:\n",
        "    try:\n",
        "        obj = json.loads(env_json_str)\n",
        "    except Exception:\n",
        "        return env_json_str\n",
        "    if isinstance(obj, dict) and obj.get(\"tool_executed\") is True:\n",
        "        result = obj.get(\"result\", \"\")\n",
        "        ts = obj.get(\"timestamp\", \"\")\n",
        "        if isinstance(result, list):\n",
        "            body = \"\\n\".join([f\"- {x}\" for x in result])\n",
        "            return f\"执行成功（{ts}）\\n\\n可用文件列表：\\n{body}\"\n",
        "        if isinstance(result, str):\n",
        "            if result.strip().startswith(\"# \") or \"\\n## \" in result:\n",
        "                return f\"执行成功（{ts}）\\n\\n生成内容：\\n\\n```markdown\\n{result}\\n```\"\n",
        "            return f\"执行成功（{ts}）\\n\\n```text\\n{result}\\n```\"\n",
        "        return f\"执行成功（{ts}）\\n\\n```json\\n{json.dumps(result, ensure_ascii=False, indent=2)}\\n```\"\n",
        "    if isinstance(obj, dict) and obj.get(\"tool_executed\") is False:\n",
        "        err = obj.get(\"error\", \"\")\n",
        "        tb = obj.get(\"traceback\", \"\")\n",
        "        return f\"执行失败\\n\\n错误：`{err}`\\n\\n<details><summary>Traceback</summary>\\n\\n```text\\n{tb}\\n```\\n\\n</details>\"\n",
        "    return f\"```json\\n{json.dumps(obj, ensure_ascii=False, indent=2)}\\n```\"\n",
        "\n",
        "md_lines = [\n",
        "    \"# 智能体执行报告\",\n",
        "]\n",
        "\n",
        "if 'final_memory' not in globals():\n",
        "    display(Markdown(\"> 未检测到 final_memory 变量，请先运行上方示例执行智能体。\"))\n",
        "else:\n",
        "    memories = final_memory.get_memories()\n",
        "\n",
        "    # 概览\n",
        "    md_lines.append(\"## 概览\")\n",
        "    md_lines.append(f\"- 总事件数：{len(memories)}\")\n",
        "\n",
        "    # 逐步展示\n",
        "    md_lines.append(\"\\n## 交互明细\\n\")\n",
        "    for idx, item in enumerate(memories, 1):\n",
        "        typ = item.get(\"type\", \"unknown\")\n",
        "        content = item.get(\"content\", \"\")\n",
        "        if typ == \"user\":\n",
        "            md_lines.append(f\"### 步骤 {idx} · 用户输入\")\n",
        "            md_lines.append(f\"> {content}\")\n",
        "        elif typ == \"assistant\":\n",
        "            md_lines.append(f\"### 步骤 {idx} · 助手决策（工具调用）\")\n",
        "            try:\n",
        "                call = json.loads(content)\n",
        "                tool = call.get(\"tool\", \"?\")\n",
        "                args = call.get(\"args\", {})\n",
        "                md_lines.append(f\"- 工具：`{tool}`\")\n",
        "                md_lines.append(\"- 参数：\")\n",
        "                md_lines.append(f\"```json\\n{json.dumps(args, ensure_ascii=False, indent=2)}\\n```\")\n",
        "            except Exception:\n",
        "                md_lines.append(\"- 文本回复：\")\n",
        "                md_lines.append(f\"```text\\n{content}\\n```\")\n",
        "        elif typ == \"environment\":\n",
        "            md_lines.append(f\"### 步骤 {idx} · 环境执行结果\")\n",
        "            md_lines.append(_format_env_result(content))\n",
        "        else:\n",
        "            md_lines.append(f\"### 步骤 {idx} · 其他\")\n",
        "            md_lines.append(f\"```text\\n{content}\\n```\")\n",
        "\n",
        "    # 摘取 README 内容（若存在 terminate 消息）\n",
        "    md_lines.append(\"\\n## 生成的 README（若已终止并返回）\\n\")\n",
        "    readme_blocks = []\n",
        "    for item in memories[::-1]:\n",
        "        if item.get(\"type\") == \"environment\":\n",
        "            try:\n",
        "                obj = json.loads(item[\"content\"]) if isinstance(item.get(\"content\"), str) else item[\"content\"]\n",
        "                if obj.get(\"tool_executed\") and isinstance(obj.get(\"result\"), str) and obj[\"result\"].lstrip().startswith(\"# \"):\n",
        "                    readme_blocks.append(obj[\"result\"].replace(\"\\nTerminating...\", \"\").strip())\n",
        "                    break\n",
        "            except Exception:\n",
        "                pass\n",
        "    if readme_blocks:\n",
        "        md_lines.append(\"```markdown\\n\" + readme_blocks[0] + \"\\n```\")\n",
        "    else:\n",
        "        md_lines.append(\"> 本次执行未生成 README 内容或未调用终止工具。\")\n",
        "\n",
        "    display(Markdown(\"\\n\\n\".join(md_lines)))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
