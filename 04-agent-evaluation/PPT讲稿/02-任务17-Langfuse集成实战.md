# ä»»åŠ¡17ï¼šLangfuseé›†æˆå®æˆ˜ - è¿½è¸ªOpenAIå’ŒLangChainè°ƒç”¨

**æ—¶é•¿ï¼š** 30åˆ†é’Ÿ  
**éš¾åº¦ï¼š** â­â­â­â˜†â˜†  
**äº¤ä»˜æˆæœï¼š** åŸºç¡€é›†æˆä»£ç å’Œè¿½è¸ªæ–¹æ¡ˆ

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šOpenAI SDKé›†æˆ (15åˆ†é’Ÿ)

### ğŸ¯ é›†æˆç›®æ ‡

**åªéœ€æ”¹ä¸€è¡Œä»£ç ï¼Œå°±èƒ½è·å¾—å®Œæ•´çš„å¯è§‚æµ‹æ€§ï¼**

```python
# åŸæ¥çš„å¯¼å…¥æ–¹å¼
# from openai import OpenAI

# æ–°çš„å¯¼å…¥æ–¹å¼ï¼ˆè‡ªåŠ¨é›†æˆLangfuseï¼‰
from langfuse.openai import openai
```

### ğŸš€ å¿«é€Ÿå¼€å§‹

#### æ­¥éª¤1ï¼šå®‰è£…ä¾èµ–

```bash
pip install langfuse==3.3.0 openai==1.107.0
```

#### æ­¥éª¤2ï¼šé…ç½®ç¯å¢ƒå˜é‡

```python
import os
import getpass

# OpenAIé…ç½®
os.environ["OPENAI_API_KEY"] = getpass.getpass("OPENAI_API_KEY: ")
os.environ["OPENAI_BASE_URL"] = "https://api.openai.com/v1"  # å¯é€‰

# Langfuseé…ç½®
os.environ["LANGFUSE_PUBLIC_KEY"] = getpass.getpass("LANGFUSE_PUBLIC_KEY: ")
os.environ["LANGFUSE_SECRET_KEY"] = getpass.getpass("LANGFUSE_SECRET_KEY: ")
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com"
```

#### æ­¥éª¤3ï¼šä½¿ç”¨é›†æˆåçš„OpenAIå®¢æˆ·ç«¯

```python
from langfuse.openai import openai

# å®Œå…¨å…¼å®¹åŸç”ŸOpenAI SDKçš„æ‰€æœ‰åŠŸèƒ½
completion = openai.chat.completions.create(
    # ğŸ“ Langfuseç‰¹æœ‰å‚æ•°ï¼šç»™è¿™æ¬¡è°ƒç”¨èµ·ä¸ªåå­—
    name="calculator-demo",
    
    # ğŸ¤– æ ‡å‡†OpenAIå‚æ•°
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªç²¾ç¡®çš„è®¡ç®—å™¨"},
        {"role": "user", "content": "1 + 1 = "}
    ],
    temperature=0,
    
    # ğŸ·ï¸ Langfuseå…ƒæ•°æ®ï¼šè‡ªå®šä¹‰æ ‡ç­¾å’Œåˆ†ç±»
    metadata={
        "task_type": "calculator",
        "difficulty": "easy",
        "user_id": "demo_user"
    }
)

print(f"è®¡ç®—ç»“æœ: {completion.choices[0].message.content}")
```

**è¾“å‡ºï¼š**
```
è®¡ç®—ç»“æœ: 2
```

### ğŸ“Š åœ¨Langfuseä¸­æŸ¥çœ‹

è®¿é—® https://cloud.langfuse.com/traces

**ä½ å°†çœ‹åˆ°ï¼š**
1. **Traceä¿¡æ¯**
   - name: "calculator-demo"
   - user_id: "demo_user"
   - æ‰§è¡Œæ—¶é—´å’Œå»¶è¿Ÿ

2. **Generationè¯¦æƒ…**
   - model: "gpt-4o"
   - input tokens: ~25
   - output tokens: ~1
   - cost: ~$0.0001

3. **è‡ªå®šä¹‰å…ƒæ•°æ®**
   - task_type: "calculator"
   - difficulty: "easy"

---

### ğŸ–¼ï¸ å›¾åƒåˆ†æç¤ºä¾‹

```python
# å¤šæ¨¡æ€è°ƒç”¨åŒæ ·é€‚ç”¨
completion = openai.chat.completions.create(
    name="image-analysis-demo",
    model="gpt-4o",
    messages=[
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªå›¾åƒåˆ†æAIï¼Œæè¿°å›¾åƒçš„ä¸»è¦å†…å®¹ã€‚"
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "è¿™å¼ å›¾ç‰‡æç»˜äº†ä»€ä¹ˆï¼Ÿ"
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": "https://example.com/image.jpg"
                    }
                }
            ]
        }
    ],
    metadata={
        "task_type": "image_analysis",
        "image_source": "user_upload"
    }
)

print(f"åˆ†æç»“æœ: {completion.choices[0].message.content}")
```

---

### ğŸŒŠ æµå¼è¾“å‡ºè¿½è¸ª

```python
# æµå¼è¾“å‡ºä¹Ÿèƒ½å®Œæ•´è¿½è¸ª
print("AIå›å¤ï¼š", end="")

completion = openai.chat.completions.create(
    name="streaming-joke-demo",
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½å¹½é»˜çš„å–œå‰§æ¼”å‘˜"},
        {"role": "user", "content": "è®²ä¸€ä¸ªå…³äºç¨‹åºå‘˜çš„ç¬‘è¯"}
    ],
    stream=True,  # å¼€å¯æµå¼è¾“å‡º
    metadata={"streaming": True}
)

for chunk in completion:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)

print("\nâœ… å®Œæˆ!")
```

**Langfuseä¼šè‡ªåŠ¨ï¼š**
- æ”¶é›†æ‰€æœ‰æµå¼ç‰‡æ®µ
- è®¡ç®—æ€»tokenæ•°
- è®°å½•å®Œæ•´çš„è¾“å‡ºå†…å®¹

---

### âš¡ å¼‚æ­¥è°ƒç”¨è¿½è¸ª

```python
from langfuse.openai import AsyncOpenAI
import asyncio

async_client = AsyncOpenAI()

async def async_calculation():
    completion = await async_client.chat.completions.create(
        name="async-calculator-demo",
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯è®¡ç®—å™¨"},
            {"role": "user", "content": "100 + 200 = "}
        ],
        temperature=0,
        metadata={"concurrency": "high"}
    )
    return completion.choices[0].message.content

# è¿è¡Œå¼‚æ­¥å‡½æ•°
result = await async_calculation()
print(f"å¼‚æ­¥è®¡ç®—ç»“æœ: {result}")
```

---

### ğŸ”§ å‡½æ•°è°ƒç”¨ï¼ˆFunction Callingï¼‰

```python
from pydantic import BaseModel
from typing import List
import json

# å®šä¹‰å‡½æ•°è¿”å›å€¼ç»“æ„
class SolutionSteps(BaseModel):
    title: str
    steps: List[str]

# ä½¿ç”¨å‡½æ•°è°ƒç”¨
response = openai.chat.completions.create(
    name="function-calling-demo",
    model="gpt-4o",
    messages=[
        {"role": "user", "content": "å¦‚ä½•åšç•ªèŒ„ç‚’è›‹ï¼Ÿ"}
    ],
    functions=[
        {
            "name": "get_recipe_steps",
            "description": "è¿”å›èœè°±çš„åˆ¶ä½œæ­¥éª¤",
            "parameters": SolutionSteps.model_json_schema()
        }
    ],
    function_call={"name": "get_recipe_steps"}
)

# è§£æå‡½æ•°è°ƒç”¨ç»“æœ
function_args = json.loads(
    response.choices[0].message.function_call.arguments
)
print(f"èœè°±ï¼š{function_args['title']}")
for i, step in enumerate(function_args['steps'], 1):
    print(f"{i}. {step}")
```

**Langfuseä¸­çš„å‡½æ•°è°ƒç”¨è¿½è¸ªï¼š**
- è®°å½•å‡½æ•°å®šä¹‰
- è®°å½•å‡½æ•°å‚æ•°
- è®°å½•å‡½æ•°è¿”å›å€¼
- æ ‡è®°ä¸ºç‰¹æ®Šçš„Generationç±»å‹

---

### ğŸ·ï¸ é«˜çº§åŠŸèƒ½ï¼šè‡ªå®šä¹‰å…ƒæ•°æ®

```python
completion = openai.chat.completions.create(
    name="advanced-metadata-demo",
    model="gpt-4o",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯åŠ©æ‰‹"},
        {"role": "user", "content": "ä½ å¥½"}
    ],
    metadata={
        # Langfuseç‰¹æ®Šå­—æ®µ
        "langfuse_session_id": "session_123",  # ä¼šè¯ID
        "langfuse_user_id": "user_456",        # ç”¨æˆ·ID
        "langfuse_tags": ["greeting", "test"], # æ ‡ç­¾åˆ—è¡¨
        
        # è‡ªå®šä¹‰å­—æ®µ
        "environment": "production",
        "version": "v1.0.0",
        "feature": "chat"
    }
)
```

**åœ¨Langfuseä¸­çš„å¥½å¤„ï¼š**
- æŒ‰session/userèšåˆåˆ†æ
- æŒ‰tagsè¿‡æ»¤å’Œæœç´¢
- æŒ‰è‡ªå®šä¹‰å­—æ®µåˆ†ç»„ç»Ÿè®¡

---

### ğŸ“‹ å°†å¤šæ¬¡è°ƒç”¨å½’å¹¶åˆ°åŒä¸€ä¸ªTrace

```python
from langfuse import observe
from langfuse.openai import openai

@observe()  # è£…é¥°å™¨åˆ›å»ºé¡¶å±‚trace
def multi_step_task(country: str):
    # ç¬¬ä¸€æ­¥ï¼šæŸ¥è¯¢é¦–éƒ½
    capital_response = openai.chat.completions.create(
        name="get-capital",
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯åœ°ç†è€å¸ˆ"},
            {"role": "user", "content": f"{country}çš„é¦–éƒ½æ˜¯ï¼Ÿ"}
        ]
    )
    capital = capital_response.choices[0].message.content
    
    # ç¬¬äºŒæ­¥ï¼šå†™å…³äºé¦–éƒ½çš„è¯—
    poem_response = openai.chat.completions.create(
        name="write-poem",
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯è¯—äºº"},
            {"role": "user", "content": f"å†™ä¸€é¦–å…³äº{capital}çš„è¯—"}
        ],
        temperature=0.9
    )
    
    return poem_response.choices[0].message.content

# è°ƒç”¨
result = multi_step_task("ä¸­å›½")
print(result)
```

**Langfuseä¸­çš„å±‚çº§ç»“æ„ï¼š**
```
Trace: multi_step_task
â”œâ”€â”€ Span: get-capital (Generation)
â”‚   â”œâ”€â”€ input: "ä¸­å›½çš„é¦–éƒ½æ˜¯ï¼Ÿ"
â”‚   â””â”€â”€ output: "åŒ—äº¬"
â””â”€â”€ Span: write-poem (Generation)
    â”œâ”€â”€ input: "å†™ä¸€é¦–å…³äºåŒ—äº¬çš„è¯—"
    â””â”€â”€ output: "..."
```

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šLangChainé›†æˆ (15åˆ†é’Ÿ)

### ğŸ¯ ä¸ºä»€ä¹ˆè¦é›†æˆLangChainï¼Ÿ

**LangChainçš„ä¼˜åŠ¿ï¼š**
- ğŸ”— é“¾å¼è°ƒç”¨ï¼ˆChainï¼‰
- ğŸ¤– æ™ºèƒ½ä½“ï¼ˆAgentï¼‰
- ğŸ› ï¸ å·¥å…·é›†æˆï¼ˆToolsï¼‰
- ğŸ’¾ è®°å¿†ç®¡ç†ï¼ˆMemoryï¼‰
- ğŸ“š æ–‡æ¡£æ£€ç´¢ï¼ˆRAGï¼‰

**é›†æˆLangfuseçš„ä»·å€¼ï¼š**
- è¿½è¸ªå¤æ‚çš„é“¾å¼è°ƒç”¨
- ç›‘æ§Agentçš„å†³ç­–è¿‡ç¨‹
- åˆ†æå·¥å…·ä½¿ç”¨æƒ…å†µ
- ä¼˜åŒ–RAGç³»ç»Ÿæ€§èƒ½

---

### ğŸš€ LangChainé›†æˆæ­¥éª¤

#### æ­¥éª¤1ï¼šå®‰è£…ä¾èµ–

```bash
pip install langfuse==3.3.0 \
            langchain==0.3.27 \
            langchain-openai==0.3.31
```

#### æ­¥éª¤2ï¼šåˆå§‹åŒ–Langfuseå›è°ƒå¤„ç†å™¨

```python
from langfuse.langchain import CallbackHandler

# åˆå§‹åŒ–å›è°ƒå¤„ç†å™¨
langfuse_handler = CallbackHandler()

# ä½¿ç”¨æ–¹å¼ï¼šåœ¨è°ƒç”¨æ—¶æ·»åŠ configå‚æ•°
# chain.invoke(..., config={"callbacks": [langfuse_handler]})
```

---

### ğŸ“ ç¤ºä¾‹1ï¼šç®€å•çš„LangChain LCEL

**LCEL = LangChain Expression Languageï¼ˆLangChainè¡¨è¾¾å¼è¯­è¨€ï¼‰**

```python
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langfuse.langchain import CallbackHandler

# åˆå§‹åŒ–å›è°ƒ
langfuse_handler = CallbackHandler()

# åˆ›å»ºæç¤ºæ¨¡æ¿
prompt = ChatPromptTemplate.from_template("{person}æ¥è‡ªå“ªåº§åŸå¸‚ï¼Ÿ")

# åˆ›å»ºæ¨¡å‹
model = ChatOpenAI(model="gpt-4o", temperature=0)

# åˆ›å»ºè¾“å‡ºè§£æå™¨
output_parser = StrOutputParser()

# æ„å»ºé“¾
chain = prompt | model | output_parser

# æ‰§è¡Œé“¾å¹¶è¿½è¸ª
result = chain.invoke(
    {"person": "è‹ä¸œå¡"},
    config={"callbacks": [langfuse_handler]}
)

print(f"ç»“æœ: {result}")  # è¾“å‡º: çœ‰å±±æˆ–æ­å·
```

**Langfuseä¸­çš„è¿½è¸ªç»“æ„ï¼š**
```
Trace
â”œâ”€â”€ Span: ChatPromptTemplate
â”‚   â”œâ”€â”€ input: {"person": "è‹ä¸œå¡"}
â”‚   â””â”€â”€ output: "è‹ä¸œå¡æ¥è‡ªå“ªåº§åŸå¸‚ï¼Ÿ"
â”œâ”€â”€ Span: ChatOpenAI (Generation)
â”‚   â”œâ”€â”€ input: "è‹ä¸œå¡æ¥è‡ªå“ªåº§åŸå¸‚ï¼Ÿ"
â”‚   â”œâ”€â”€ output: "çœ‰å±±"
â”‚   â”œâ”€â”€ tokens: 35
â”‚   â””â”€â”€ cost: $0.0002
â””â”€â”€ Span: StrOutputParser
    â”œâ”€â”€ input: AIMessage(content="çœ‰å±±")
    â””â”€â”€ output: "çœ‰å±±"
```

---

### ğŸ”— ç¤ºä¾‹2ï¼šå¤æ‚çš„å¤šæ­¥éª¤é“¾

```python
from operator import itemgetter
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser

langfuse_handler = CallbackHandler()

# ç¬¬ä¸€ä¸ªæç¤ºï¼šæŸ¥è¯¢åŸå¸‚
prompt1 = ChatPromptTemplate.from_template("{person}æ¥è‡ªå“ªåº§åŸå¸‚ï¼Ÿ")

# ç¬¬äºŒä¸ªæç¤ºï¼šæŸ¥è¯¢å›½å®¶
prompt2 = ChatPromptTemplate.from_template(
    "åŸå¸‚{city}ä½äºå“ªä¸ªå›½å®¶ï¼Ÿè¯·ç”¨{language}å›ç­”"
)

model = ChatOpenAI(model="gpt-4o", temperature=0)

# ç¬¬ä¸€ä¸ªé“¾ï¼šäººå â†’ åŸå¸‚
chain1 = prompt1 | model | StrOutputParser()

# ç¬¬äºŒä¸ªé“¾ï¼šåŸå¸‚ + è¯­è¨€ â†’ å›½å®¶
chain2 = (
    {
        "city": chain1,  # ä½¿ç”¨chain1çš„è¾“å‡º
        "language": itemgetter("language")  # ä»è¾“å…¥æå–language
    }
    | prompt2
    | model
    | StrOutputParser()
)

# æ‰§è¡Œå¤æ‚é“¾
result = chain2.invoke(
    {"person": "è«è¨€", "language": "è‹±æ–‡"},
    config={"callbacks": [langfuse_handler]}
)

print(f"ç»“æœ: {result}")
```

**Langfuseä¸­çš„åµŒå¥—è¿½è¸ªï¼š**
```
Trace
â”œâ”€â”€ Span: chain2
â”‚   â”œâ”€â”€ Span: chain1
â”‚   â”‚   â”œâ”€â”€ prompt1: "è«è¨€æ¥è‡ªå“ªåº§åŸå¸‚ï¼Ÿ"
â”‚   â”‚   â”œâ”€â”€ model: "é«˜å¯†"
â”‚   â”‚   â””â”€â”€ parser: "é«˜å¯†"
â”‚   â”œâ”€â”€ Span: RunnableParallel
â”‚   â”‚   â”œâ”€â”€ city: "é«˜å¯†" (from chain1)
â”‚   â”‚   â””â”€â”€ language: "è‹±æ–‡" (from input)
â”‚   â”œâ”€â”€ Span: prompt2
â”‚   â”‚   â””â”€â”€ "åŸå¸‚é«˜å¯†ä½äºå“ªä¸ªå›½å®¶ï¼Ÿè¯·ç”¨è‹±æ–‡å›ç­”"
â”‚   â”œâ”€â”€ Span: model (Generation)
â”‚   â”‚   â””â”€â”€ "China"
â”‚   â””â”€â”€ Span: parser
â”‚       â””â”€â”€ "China"
```

---

### ğŸƒ ç¤ºä¾‹3ï¼šRunnableæ–¹æ³•è¿½è¸ª

```python
from langfuse.langchain import CallbackHandler

langfuse_handler = CallbackHandler()

# å‡è®¾chainå·²å®šä¹‰ï¼ˆåŒä¸Šï¼‰
chain = prompt | model | StrOutputParser()

# 1. åŒæ­¥è°ƒç”¨
result = chain.invoke(
    {"person": "é²è¿…"},
    config={"callbacks": [langfuse_handler]}
)

# 2. å¼‚æ­¥è°ƒç”¨
result = await chain.ainvoke(
    {"person": "å·´é‡‘"},
    config={"callbacks": [langfuse_handler]}
)

# 3. æ‰¹å¤„ç†
results = chain.batch([
    {"person": "è€èˆ"},
    {"person": "èŒ…ç›¾"}
], config={"callbacks": [langfuse_handler]})

# 4. æµå¼è¾“å‡º
for chunk in chain.stream(
    {"person": "é’±é’Ÿä¹¦"},
    config={"callbacks": [langfuse_handler]}
):
    print(chunk, end="", flush=True)
```

**æ¯ç§æ–¹æ³•éƒ½ä¼šåœ¨Langfuseä¸­åˆ›å»ºç‹¬ç«‹çš„Traceã€‚**

---

### ğŸ“š ç¤ºä¾‹4ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰

```python
from langchain_community.document_loaders import TextLoader
from langchain_chroma import Chroma
from langchain_text_splitters import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import RetrievalQA
from langfuse.langchain import CallbackHandler

langfuse_handler = CallbackHandler()

# 1. åŠ è½½æ–‡æ¡£
loader = TextLoader("state_of_the_union.txt")
documents = loader.load()

# 2. åˆ†å‰²æ–‡æ¡£
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# 3. åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()
docsearch = Chroma.from_documents(texts, embeddings)

# 4. åˆ›å»ºæ£€ç´¢é—®ç­”é“¾
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=docsearch.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# 5. æ‰§è¡Œé—®ç­”å¹¶è¿½è¸ª
result = qa_chain.invoke(
    "ç¾å›½å›½æƒ…å’¨æ–‡çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ",
    config={"callbacks": [langfuse_handler]}
)

print(f"ç­”æ¡ˆ: {result['result']}")
print(f"æ¥æº: {len(result['source_documents'])}ä¸ªæ–‡æ¡£")
```

**Langfuseä¸­çš„RAGè¿½è¸ªï¼š**
```
Trace: RetrievalQA
â”œâ”€â”€ Span: Retriever
â”‚   â”œâ”€â”€ input: "ç¾å›½å›½æƒ…å’¨æ–‡çš„æ ¸å¿ƒä¸»é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ"
â”‚   â”œâ”€â”€ output: [Document1, Document2, Document3]
â”‚   â””â”€â”€ latency: 0.2s
â”œâ”€â”€ Span: StuffDocumentsChain
â”‚   â”œâ”€â”€ input: query + documents
â”‚   â””â”€â”€ Span: LLMChain
â”‚       â”œâ”€â”€ Span: PromptTemplate
â”‚       â”‚   â””â”€â”€ å°†é—®é¢˜å’Œæ–‡æ¡£ç»„åˆæˆæç¤º
â”‚       â”œâ”€â”€ Span: ChatOpenAI (Generation)
â”‚       â”‚   â”œâ”€â”€ tokens: 1500
â”‚       â”‚   â”œâ”€â”€ cost: $0.003
â”‚       â”‚   â””â”€â”€ latency: 2.5s
â”‚       â””â”€â”€ output: "æ ¸å¿ƒä¸»é¢˜æ˜¯..."
```

---

## ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“

### OpenAI SDKé›†æˆ
```
âœ… åªéœ€æ”¹ä¸€è¡Œå¯¼å…¥ä»£ç 
âœ… å®Œå…¨å…¼å®¹åŸç”ŸSDK
âœ… è‡ªåŠ¨è¿½è¸ªæ‰€æœ‰è°ƒç”¨
âœ… æ”¯æŒæµå¼ã€å¼‚æ­¥ã€å‡½æ•°è°ƒç”¨
```

### LangChainé›†æˆ
```
âœ… ä½¿ç”¨CallbackHandler
âœ… åœ¨configä¸­æ·»åŠ callbacks
âœ… è¿½è¸ªå¤æ‚çš„é“¾å¼è°ƒç”¨
âœ… æ”¯æŒæ‰€æœ‰Runnableæ–¹æ³•
```

### è¿½è¸ªæ•°æ®çš„ä»·å€¼
```
ğŸ“Š å¯è§†åŒ–æ‰§è¡Œæµç¨‹
â±ï¸ è¯†åˆ«æ€§èƒ½ç“¶é¢ˆ
ğŸ’° ç²¾ç¡®è®¡ç®—æˆæœ¬
ğŸ› å¿«é€Ÿå®šä½é—®é¢˜
```

---

## ğŸ¯ å®æˆ˜ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šåŸºç¡€é›†æˆ
ä¸ºä½ çš„ç°æœ‰OpenAIè°ƒç”¨æ·»åŠ Langfuseè¿½è¸ªã€‚

### ç»ƒä¹ 2ï¼šLangChainé“¾
åˆ›å»ºä¸€ä¸ªåŒ…å«è‡³å°‘3ä¸ªæ­¥éª¤çš„LangChainï¼Œå¹¶è¿½è¸ªæ‰§è¡Œè¿‡ç¨‹ã€‚

### ç»ƒä¹ 3ï¼šæ€§èƒ½åˆ†æ
è¿è¡ŒåŒä¸€ä¸ªæç¤º10æ¬¡ï¼Œåœ¨Langfuseä¸­åˆ†æå»¶è¿Ÿåˆ†å¸ƒã€‚

### ç»ƒä¹ 4ï¼šæˆæœ¬ä¼˜åŒ–
å¯¹æ¯”gpt-4oå’Œgpt-3.5-turboçš„æˆæœ¬å·®å¼‚ã€‚

---

**ä¸‹ä¸€èŠ‚ï¼šä»»åŠ¡18 - è¿½è¸ªLangGraphå¤šè§’è‰²æ™ºèƒ½ä½“**


