{
      "cells": [
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "# vLLM\n",
                        "\n",
                        "推荐您尝试使用 [vLLM](https://github.com/vllm-project/vllm) 来部署 Qwen 模型。它易于使用，且具备极高的服务吞吐量。其核心特性包括 PagedAttention（高效管理注意力键值内存）、连续批处理（continuous batching）以及优化的 CUDA 内核等。\n",
                        "\n",
                        "欲深入了解 vLLM，请参阅[论文](https://arxiv.org/abs/2309.06180)和[官方文档](https://docs.vllm.ai/)。\n",
                        "\n",
                        "**部署环境说明**：\n",
                        "- 平台：Google Colab\n",
                        "- 操作系统：Ubuntu 22.04\n",
                        "- GPU：Tesla T4 (15GB 显存)\n",
                        "- 目标受众：智能体中高级学习者，关注高性能推理部署"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## 环境设置 (Environment Setup)\n",
                        "\n",
                        "默认情况下，您可以在纯净环境中使用 pip 安装 `vllm`："
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!pip install \"vllm>=0.8.5\""
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "**注意**：`vllm` 对 `torch` 及其 CUDA 版本有严格的依赖要求。如果遇到问题，请查阅[官方安装文档](https://docs.vllm.ai/en/latest/getting_started/installation.html)。"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## API 服务 (API Service)\n",
                        "\n",
                        "使用 vLLM 构建兼容 OpenAI 的 API 服务非常便捷。默认端口为 `http://localhost:8000`。\n",
                        "\n",
                        "针对 Colab 的 T4 GPU 环境，我们将部署 **AWQ 量化版本** 的 `Qwen/Qwen3-8B-AWQ` 模型。"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "import time\n",
                        "import requests\n",
                        "import os\n",
                        "\n",
                        "# 启动 vLLM 服务器（后台运行）\n",
                        "# 适配 T4 环境的关键参数：\n",
                        "# --quantization awq: 显式指定使用 AWQ 量化后端\n",
                        "# --dtype float16: 配合量化使用\n",
                        "# --max-model-len 32768: 根据显存情况可能需要调整，这里保持默认尝试\n",
                        "print(\"正在启动 vLLM 服务器... (预计需要几分钟下载和加载模型)\")\n",
                        "\n",
                        "# 注意：vLLM 可能会占用大量显存，若遇到 OOM，请尝试减小 max-model-len，例如 --max-model-len 8192\n",
                        "get_ipython().system_raw('nohup vllm serve Qwen/Qwen3-8B-AWQ --port 8000 --quantization awq --dtype float16 > vllm.log 2>&1 &')\n",
                        "\n",
                        "# 等待服务器就绪\n",
                        "def wait_for_server(url, timeout=900):\n",
                        "    start_time = time.time()\n",
                        "    print(\"等待服务就绪...\", end=\"\")\n",
                        "    while time.time() - start_time < timeout:\n",
                        "        try:\n",
                        "            response = requests.get(f\"{url}/health\")\n",
                        "            if response.status_code == 200:\n",
                        "                print(\"\\n服务器已启动并就绪！\")\n",
                        "                return True\n",
                        "        except requests.ConnectionError:\n",
                        "            pass\n",
                        "        time.sleep(10)\n",
                        "        print(\".\", end=\"\", flush=True)\n",
                        "    print(\"\\n服务器启动超时。请运行 `!cat vllm.log` 查看错误日志。\")\n",
                        "    return False\n",
                        "\n",
                        "wait_for_server(\"http://127.0.0.1:8000\")"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 基本用法 (Basic Usage)\n",
                        "\n",
                        "使用 `curl` 与 Qwen 进行交互："
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!curl http://localhost:8000/v1/chat/completions \\\n",
                        "  -H \"Content-Type: application/json\" \\\n",
                        "  -d '{\n",
                        "  \"model\": \"Qwen/Qwen3-8B-AWQ\",\n",
                        "  \"messages\": [\n",
                        "    {\"role\": \"user\", \"content\": \"请简要介绍一下大型语言模型。\"}\n",
                        "  ],\n",
                        "  \"temperature\": 0.6,\n",
                        "  \"top_p\": 0.95,\n",
                        "  \"top_k\": 20,\n",
                        "  \"max_tokens\": 1024\n",
                        "}'"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 思考模式与非思考模式\n",
                        "\n",
                        "与 SGLang 类似，vLLM 也支持通过 `chat_template_kwargs` 控制 Qwen3 的思考行为。以下是禁用思考的示例："
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "!curl http://localhost:8000/v1/chat/completions \\\n",
                        "  -H \"Content-Type: application/json\" \\\n",
                        "  -d '{\n",
                        "  \"model\": \"Qwen/Qwen3-8B-AWQ\",\n",
                        "  \"messages\": [\n",
                        "    {\"role\": \"user\", \"content\": \"请简要介绍一下大型语言模型。\"}\n",
                        "  ],\n",
                        "  \"temperature\": 0.7,\n",
                        "  \"top_p\": 0.8,\n",
                        "  \"top_k\": 20,\n",
                        "  \"max_tokens\": 1024,\n",
                        "  \"presence_penalty\": 1.5,\n",
                        "  \"chat_template_kwargs\": {\"enable_thinking\": false}\n",
                        "}'"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "### 解析思考内容 (Parsing Thinking Content)\n",
                        "\n",
                        "vLLM 支持解析思考内容。\n",
                        "- 启动参数：`--enable-reasoning --reasoning-parser deepseek_r1` (或 v0.9.0+ 的 `qwen3`)\n",
                        "- 注意：vLLM 0.8.5 中，`enable_thinking=False` 可能不兼容此功能。建议在 v0.9.0+ 版本中使用。\n",
                        "\n",
                        "### 常见问题 (FAQ)：OOM (显存溢出)\n",
                        "\n",
                        "在 Colab 等显存有限的环境中，OOM 是常见问题。建议尝试以下参数进行优化：\n",
                        "1. `--max-model-len`: 减小上下文长度。例如设置为 `8192` 或更小，以减少显存占用。\n",
                        "2. `--gpu-memory-utilization`: vLLM 默认占用 90% 显存。如果遇到 OOM，可以尝试微调此参数。\n"
                  ]
            },
            {
                  "cell_type": "markdown",
                  "metadata": {},
                  "source": [
                        "## Python 库使用 (Python Library)\n",
                        "\n",
                        "vLLM 也可以作为 Python 库直接使用，方便离线批处理。\n",
                        "\n",
                        "**重要提示**：在 Colab 单卡 T4 环境下，同时运行 vLLM 服务器和 Python 库实例会导致 **OOM**。在大约运行下面的代码之前，**必须**先停止上面的后台服务器。"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "# 首先停止后台的 vLLM 服务器\n",
                        "!pkill -f vllm\n",
                        "print(\"vLLM 服务器已停止。现在可以运行 Python 库示例。\")\n",
                        "# 稍微等待显存释放\n",
                        "time.sleep(5)"
                  ]
            },
            {
                  "cell_type": "code",
                  "execution_count": null,
                  "metadata": {},
                  "outputs": [],
                  "source": [
                        "from transformers import AutoTokenizer\n",
                        "from vllm import LLM, SamplingParams\n",
                        "import torch\n",
                        "\n",
                        "# 清理缓存以确保有足够显存\n",
                        "torch.cuda.empty_cache()\n",
                        "\n",
                        "# 初始化分词器\n",
                        "try:\n",
                        "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-8B-AWQ\")\n",
                        "\n",
                        "    # 配置采样参数\n",
                        "    # 请根据显存调整 max_tokens\n",
                        "    sampling_params = SamplingParams(temperature=0.6, top_p=0.95, top_k=20, max_tokens=1024)\n",
                        "\n",
                        "    # 初始化 vLLM 引擎\n",
                        "    # 显式指定 quantization='awq' 以加载 AWQ 模型\n",
                        "    llm = LLM(model=\"Qwen/Qwen3-8B-AWQ\", quantization=\"awq\", dtype=\"float16\", max_model_len=8192)\n",
                        "\n",
                        "    # 准备输入\n",
                        "    prompt = \"请简要介绍一下大型语言模型。\"\n",
                        "    messages = [\n",
                        "        {\"role\": \"user\", \"content\": prompt}\n",
                        "    ]\n",
                        "    \n",
                        "    # 应用聊天模板\n",
                        "    text = tokenizer.apply_chat_template(\n",
                        "        messages,\n",
                        "        tokenize=False,\n",
                        "        add_generation_prompt=True,\n",
                        "        # enable_thinking=True, # 如果模型支持且库版本支持\n",
                        "    )\n",
                        "\n",
                        "    # 生成输出\n",
                        "    outputs = llm.generate([text], sampling_params)\n",
                        "\n",
                        "    # 打印输出\n",
                        "    for output in outputs:\n",
                        "        prompt = output.prompt\n",
                        "        generated_text = output.outputs[0].text\n",
                        "        print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n",
                        "        \n",
                        "except Exception as e:\n",
                        "    print(f\"运行出错 (可能是 OOM): {e}\")"
                  ]
            }
      ],
      "metadata": {
            "kernelspec": {
                  "display_name": "Python 3",
                  "language": "python",
                  "name": "python3"
            },
            "language_info": {
                  "codemirror_mode": {
                        "name": "ipython",
                        "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.10.12"
            }
      },
      "nbformat": 4,
      "nbformat_minor": 5
}