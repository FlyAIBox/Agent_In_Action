{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangGraph è¿›é˜¶æ¦‚å¿µï¼šä¸­é—´ä»¶ (Middleware) & äººæœºäº¤äº’ (Human-in-the-Loop)\n",
    "\n",
    "æ¬¢è¿æ¥åˆ° LangGraph è¿›é˜¶è¯¾ç¨‹ï¼æœ¬ Notebook å»ºç«‹åœ¨ langgraph_1.x_01 çš„åŸºç¡€ä¹‹ä¸Šï¼Œä»‹ç»æ„å»ºç”Ÿäº§çº§ Agent çš„ä¸¤ä¸ªå¼ºå¤§æ¨¡å¼ã€‚\n",
    "\n",
    "**ä½ å°†å­¦åˆ°ï¼š**\n",
    "- **Human-in-the-Loop (äººæœºäº¤äº’)** - æš‚åœ Agent ä»¥è¿›è¡Œäººå·¥å®¡æŸ¥å’Œæ‰¹å‡†\n",
    "- **Middleware (ä¸­é—´ä»¶)** - åœ¨æ‰§è¡Œçš„å…³é”®ç‚¹ä¿®æ”¹ Agent çš„è¡Œä¸º\n",
    "- **Tool Review (å·¥å…·å®¡æŸ¥)** - ä¸ºæ•æ„Ÿå·¥å…·æ·»åŠ å®¡æ‰¹å·¥ä½œæµ\n",
    "- **Dynamic Behavior (åŠ¨æ€è¡Œä¸º)** - æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´ Agent çš„å“åº”\n",
    "\n",
    "**å…ˆå†³æ¡ä»¶ï¼š** å®Œæˆ `langgraph_1.x_01.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "> **æ³¨æ„ï¼š** è¿™äº›æ¨¡å¼å¯¹äºç”Ÿäº§çº§ Agent è‡³å…³é‡è¦ï¼Œ **å› ä¸ºåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå®‰å…¨æ€§ã€åˆè§„æ€§å’Œç”¨æˆ·æ§åˆ¶æ˜¯å¿…ä¸å¯å°‘çš„** ã€‚LangGraph 1.0 å¼•å…¥äº† `interrupt` å’Œ `Command` ç­‰æ–°åŸè¯­ï¼Œè®©è¿™äº›æ¨¡å¼çš„å®ç°å˜å¾—æ›´åŠ ç›´è§‚å’Œå¼ºå¤§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph 1.0 å‡çº§æŒ‡å— - äººæœºäº¤äº’ä¸æ§åˆ¶æµ\n",
    "\n",
    "åœ¨ LangGraph 1.0 ä¸­ï¼Œ**Human-in-the-Loop** å’Œ **æ§åˆ¶æµ** å‘ç”Ÿäº†é‡å¤§å˜åŒ–ï¼Œå˜å¾—æ›´åŠ åŠ¨æ€å’Œçµæ´»ã€‚\n",
    "\n",
    "**ä¸»è¦åŒºåˆ«ï¼š**\n",
    "\n",
    "#### 1. ä¸­æ–­æœºåˆ¶ï¼š`interrupt()` vs é™æ€é…ç½®\n",
    "- **æ—§ç‰ˆ (0.x)**: éœ€è¦åœ¨ç¼–è¯‘ Graph æ—¶é€šè¿‡ `interrupt_before=[\"node_name\"]` é™æ€æŒ‡å®šåœ¨å“ªåœã€‚\n",
    "- **æ–°ç‰ˆ (1.0)**: å¼•å…¥äº†åŠ¨æ€ **`interrupt()`** å‡½æ•°ã€‚ä½ å¯ä»¥åœ¨ä»»ä½•èŠ‚ç‚¹æˆ–å·¥å…·å†…éƒ¨è°ƒç”¨å®ƒã€‚è¿™å…è®¸åŸºäºé€»è¾‘ï¼ˆå¦‚â€œç½®ä¿¡åº¦ä½æ—¶æ‰ä¸­æ–­â€ï¼‰åŠ¨æ€å†³å®šæ˜¯å¦æš‚åœã€‚\n",
    "\n",
    "#### 2. çŠ¶æ€æ›´æ–°ä¸å¯¼èˆªï¼š`Command` å¯¹è±¡\n",
    "- **æ—§ç‰ˆ (0.x)**: èŠ‚ç‚¹è¿”å›çŠ¶æ€æ›´æ–°ï¼Œè·¯ç”±ç”±å•ç‹¬çš„æ¡ä»¶è¾¹å‡½æ•°å¤„ç†ã€‚\n",
    "- **æ–°ç‰ˆ (1.0)**: å¼•å…¥ **`Command`** å¯¹è±¡ã€‚ä¸€ä¸ªèŠ‚ç‚¹å¯ä»¥åŒæ—¶è¿”å›çŠ¶æ€æ›´æ–° (`update`) å’Œè·¯ç”±æŒ‡ä»¤ (`goto`)ï¼Œæå¤§ç®€åŒ–äº†å¤æ‚é€»è¾‘çš„ç¼–å†™ã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹çš„â€œç¬¬ 1 éƒ¨åˆ†â€å’Œâ€œç¬¬ 2 éƒ¨åˆ†â€å°†è¯¦ç»†æ¼”ç¤ºè¿™äº›æ–°ç‰¹æ€§çš„ç”¨æ³•ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‰ç½®éƒ¨åˆ†ï¼šç¯å¢ƒé…ç½®å’Œæ¨¡å‹é…ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ›å»ºlangchain1.x ç¯å¢ƒï¼ˆåœ¨shellä¸­æ‰§è¡Œï¼‰\n",
    "```\n",
    "# åˆ›å»ºlangchain1.x ç¯å¢ƒ\n",
    "conda create -n langchain1.x python=3.10.18 -y\n",
    "\n",
    "# ç¡®ä¿åœ¨ langchain1.x ç¯å¢ƒä¸­\n",
    "conda activate langchain1.x\n",
    "\n",
    "# å®‰è£… ipykernel\n",
    "pip install ipykernel\n",
    "\n",
    "# å°†ç¯å¢ƒæ³¨å†Œä¸º Jupyter å†…æ ¸\n",
    "python -m ipykernel install --user --name=langchain1.x --display-name=\"Python (langchain1.x)\"\n",
    "\n",
    "# éªŒè¯å†…æ ¸å®‰è£…\n",
    "jupyter kernelspec list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”§ ç¯å¢ƒé…ç½®å’Œæ£€æŸ¥\n",
    "\n",
    "#### æ¦‚è¿°\n",
    "\n",
    "æœ¬æ•™ç¨‹éœ€è¦ç‰¹å®šçš„ç¯å¢ƒé…ç½®ä»¥ç¡®ä¿æœ€ä½³å­¦ä¹ ä½“éªŒã€‚ä»¥ä¸‹é…ç½®å°†å¸®åŠ©ä½ ï¼š\n",
    "\n",
    "- ä½¿ç”¨ç»Ÿä¸€çš„condaç¯å¢ƒï¼šæ¿€æ´»ç»Ÿä¸€çš„å­¦ä¹ ç¯å¢ƒ\n",
    "- é€šè¿‡å›½å†…é•œåƒæºå¿«é€Ÿå®‰è£…ä¾èµ–ï¼šé…ç½®pipä½¿ç”¨æ¸…åé•œåƒæº\n",
    "- åŠ é€Ÿæ¨¡å‹ä¸‹è½½ï¼šè®¾ç½®HuggingFaceé•œåƒä»£ç†\n",
    "- æ£€æŸ¥ç³»ç»Ÿé…ç½®ï¼šæ£€æŸ¥ç¡¬ä»¶å’Œè½¯ä»¶é…ç½®\n",
    "\n",
    "#### é…ç½®\n",
    "\n",
    "- **æ‰€éœ€ç¯å¢ƒåŠå…¶ä¾èµ–å·²ç»éƒ¨ç½²å¥½**\n",
    "- åœ¨`Notebook`å³ä¸Šè§’é€‰æ‹©`jupyterå†…æ ¸`ä¸º`python(langchain1.x)`ï¼Œå³å¯æ‰§è¡Œä¸‹æ–¹ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda ç¯å¢ƒæ£€æŸ¥æŠ¥å‘Š (ä»…é’ˆå¯¹å½“å‰ Bash å­è¿›ï¿½ï¿½) ==\n",
      "=========================================\n",
      "âœ… å½“å‰å•å…ƒæ ¼å·²æˆåŠŸæ¿€æ´»åˆ° langchain1.x ç¯å¢ƒã€‚\n",
      "âœ… æ­£åœ¨ä½¿ç”¨çš„ç¯å¢ƒè·¯å¾„: /root/miniconda3/envs/langchain1.x\n",
      "\n",
      "ğŸ’¡ æç¤º: åç»­çš„Pythonå•å…ƒæ ¼å°†ä½¿ç”¨Notebookå½“å‰é€‰æ‹©çš„Jupyterå†…æ ¸ã€‚\n",
      "   å¦‚æœéœ€è¦åç»­å•å…ƒæ ¼ä¹Ÿä½¿ç”¨æ­¤ç¯å¢ƒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:\n",
      "   1. æ£€ï¿½ï¿½ï¿½ Notebook å³ä¸Šè§’æ˜¯å¦å·²é€‰æ‹© 'python(langchain1.x)'ã€‚\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. æ¿€æ´» conda ç¯å¢ƒ (ä»…å¯¹å½“å‰å•å…ƒæ ¼æœ‰æ•ˆ)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate langchain1.x\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda ç¯å¢ƒæ£€æŸ¥æŠ¥å‘Š (ä»…é’ˆå¯¹å½“å‰ Bash å­è¿›ç¨‹) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. æ£€æŸ¥å½“å‰æ¿€æ´»çš„ç¯å¢ƒ\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"langchain1.x\" ]; then\n",
    "    echo \"âœ… å½“å‰å•å…ƒæ ¼å·²æˆåŠŸæ¿€æ´»åˆ° langchain1.x ç¯å¢ƒã€‚\"\n",
    "    echo \"âœ… æ­£åœ¨ä½¿ç”¨çš„ç¯å¢ƒè·¯å¾„: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"ğŸ’¡ æç¤º: åç»­çš„Pythonå•å…ƒæ ¼å°†ä½¿ç”¨Notebookå½“å‰é€‰æ‹©çš„Jupyterå†…æ ¸ã€‚\"\n",
    "    echo \"   å¦‚æœéœ€è¦åç»­å•å…ƒæ ¼ä¹Ÿä½¿ç”¨æ­¤ç¯å¢ƒï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ:\"\n",
    "    echo \"   1. æ£€æŸ¥ Notebook å³ä¸Šè§’æ˜¯å¦å·²é€‰æ‹© 'python(langchain1.x)'ã€‚\"\n",
    "else\n",
    "    echo \"âŒ æ¿€æ´»å¤±è´¥æˆ–ç¯å¢ƒåç§°ä¸åŒ¹é…ã€‚å½“å‰ç¯å¢ƒ: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸ ä¸¥é‡æç¤º: å»ºè®®å°† Notebook çš„ Jupyter **å†…æ ¸ (Kernel)** åˆ‡æ¢ä¸º 'python(langchain1.x)'ã€‚\"\n",
    "    echo \"   (é€šå¸¸ä½äº Notebook å³ä¸Šè§’æˆ– 'å†…æ ¸' èœå•ä¸­)\"\n",
    "    echo \"\"\n",
    "    echo \"ğŸ“š å¤‡ç”¨æ–¹æ³• (ä¸æ¨è): å¦‚æœæ— æ³•åˆ‡æ¢å†…æ ¸ï¼Œåˆ™å¿…é¡»åœ¨**æ¯ä¸ª**ä»£ç å•å…ƒæ ¼çš„å¤´éƒ¨é‡å¤ä»¥ä¸‹å‘½ä»¤:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# å¿…é¡»åœ¨æ¯ä¸ªå•å…ƒæ ¼éƒ½æ‰§è¡Œ\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate langchain1.x\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” ç¯å¢ƒä¿¡æ¯æ‰“å°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: pandas==2.2.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from pandas==2.2.2) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### ç¯å¢ƒä¿¡æ¯\n",
      "| é¡¹ç›®         | ä¿¡æ¯                                                                               |\n",
      "|:-------------|:-----------------------------------------------------------------------------------|\n",
      "| æ“ä½œç³»ç»Ÿ     | Linux Ubuntu 22.04.4 LTS                                                           |\n",
      "| CPU ä¿¡æ¯     | 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (1 physical cores, 4 logical cores) |\n",
      "| å†…å­˜ä¿¡æ¯     | 5.75 GB (Available: 4.24 GB)                                                       |\n",
      "| GPU ä¿¡æ¯     | No GPU found (nvidia-smi not found)                                                |\n",
      "| CUDA ä¿¡æ¯    | CUDA not found                                                                     |\n",
      "| Python ç‰ˆæœ¬  | 3.10.18                                                                            |\n",
      "| Conda ç‰ˆæœ¬   | conda 24.4.0                                                                       |\n",
      "| ç‰©ç†ç£ç›˜ç©ºé—´ | Total: 145.49 GB, Used: 57.45 GB, Free: 81.81 GB                                   |\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” ç¯å¢ƒä¿¡æ¯æ£€æŸ¥è„šæœ¬\n",
    "#\n",
    "# æœ¬è„šæœ¬çš„ä½œç”¨ï¼š\n",
    "# 1. å®‰è£… pandas åº“ç”¨äºæ•°æ®è¡¨æ ¼å±•ç¤º\n",
    "# 2. æ£€æŸ¥ç³»ç»Ÿçš„å„é¡¹é…ç½®ä¿¡æ¯\n",
    "# 3. ç”Ÿæˆè¯¦ç»†çš„ç¯å¢ƒæŠ¥å‘Šè¡¨æ ¼\n",
    "#\n",
    "# å¯¹äºåˆå­¦è€…æ¥è¯´ï¼Œè¿™ä¸ªæ­¥éª¤å¸®åŠ©ä½ ï¼š\n",
    "# - äº†è§£å½“å‰è¿è¡Œç¯å¢ƒçš„ç¡¬ä»¶é…ç½®\n",
    "# - ç¡®è®¤æ˜¯å¦æ»¡è¶³æ¨¡å‹è¿è¡Œçš„æœ€ä½è¦æ±‚\n",
    "# - å­¦ä¹ å¦‚ä½•é€šè¿‡ä»£ç è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "\n",
    "# å®‰è£… pandas åº“ - ç”¨äºåˆ›å»ºå’Œå±•ç¤ºæ•°æ®è¡¨æ ¼\n",
    "# pandas æ˜¯ Python ä¸­æœ€æµè¡Œçš„æ•°æ®å¤„ç†å’Œåˆ†æåº“\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # å¯¼å…¥ platform æ¨¡å—ä»¥è·å–ç³»ç»Ÿä¿¡æ¯\n",
    "import os # å¯¼å…¥ os æ¨¡å—ä»¥ä¸æ“ä½œç³»ç»Ÿäº¤äº’\n",
    "import subprocess # å¯¼å…¥ subprocess æ¨¡å—ä»¥è¿è¡Œå¤–éƒ¨å‘½ä»¤\n",
    "import pandas as pd # å¯¼å…¥ pandas æ¨¡å—ï¼Œé€šå¸¸ç”¨äºæ•°æ®å¤„ç†ï¼Œè¿™é‡Œç”¨äºåˆ›å»ºè¡¨æ ¼\n",
    "import shutil # å¯¼å…¥ shutil æ¨¡å—ä»¥è·å–ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "# è·å– CPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ ¸å¿ƒæ•°é‡\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # åˆå§‹åŒ– CPU ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # å¦‚æœæ˜¯ Windows ç³»ç»Ÿ\n",
    "        cpu_info = platform.processor() # ä½¿ç”¨ platform.processor() è·å– CPU ä¿¡æ¯\n",
    "        try:\n",
    "            # è·å– Windows ä¸Šçš„æ ¸å¿ƒæ•°é‡ (éœ€è¦ WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # å¦‚æœ WMI ä¸å¯ç”¨ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # æ›´æ–° PATH ç¯å¢ƒå˜é‡\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/cpuinfo æ–‡ä»¶è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # æŸ¥æ‰¾ä»¥ 'model name'å¼€å¤´çš„è¡Œ\n",
    "                        if not cpu_info: # åªè·å–ç¬¬ä¸€ä¸ª model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # æŸ¥æ‰¾ä»¥ 'cpu cores' å¼€å¤´çš„è¡Œ\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # æŸ¥æ‰¾ä»¥ 'processor' å¼€å¤´çš„è¡Œ\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # è¿”å› CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "\n",
    "\n",
    "# è·å–å†…å­˜ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # åˆå§‹åŒ–å†…å­˜ä¿¡æ¯å­—ç¬¦ä¸²\n",
    "    if platform.system() == \"Windows\":\n",
    "        # åœ¨ Windows ä¸Šä¸å®¹æ˜“é€šè¿‡æ ‡å‡†åº“è·å–ï¼Œéœ€è¦å¤–éƒ¨åº“æˆ– PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # è®¾ç½®æç¤ºä¿¡æ¯\n",
    "    elif platform.system() == \"Darwin\": # å¦‚æœæ˜¯ macOS ç³»ç»Ÿ\n",
    "        # åœ¨ macOS ä¸Šä½¿ç”¨ sysctl å‘½ä»¤è·å–å†…å­˜å¤§å°\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # è¿è¡Œ sysctl å‘½ä»¤\n",
    "        stdout, stderr = process.communicate() # è·å–æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # è§£æè¾“å‡ºï¼Œè·å–å†…å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰\n",
    "        mem_gb = mem_bytes / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    else:  # Linux ç³»ç»Ÿ\n",
    "        try:\n",
    "            # åœ¨ Linux ä¸Šè¯»å– /proc/meminfo æ–‡ä»¶è·å–å†…å­˜ä¿¡æ¯\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # æŸ¥æ‰¾ä»¥ 'MemTotal' å¼€å¤´çš„è¡Œ\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–æ€»å†…å­˜ï¼ˆKBï¼‰\n",
    "                    elif line.startswith('MemAvailable'): # æŸ¥æ‰¾ä»¥ 'MemAvailable' å¼€å¤´çš„è¡Œ\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # è§£æè¡Œï¼Œè·å–å¯ç”¨å†…å­˜ï¼ˆKBï¼‰\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # è½¬æ¢ä¸º GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡ºæ€»å†…å­˜\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # æ·»åŠ å¯ç”¨å†…å­˜ä¿¡æ¯\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # å¦‚æœè¯»å–æ–‡ä»¶å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "    return mem_info # è¿”å›å†…å­˜ä¿¡æ¯\n",
    "\n",
    "# è·å– GPU ä¿¡æ¯çš„å‡½æ•°ï¼ŒåŒ…æ‹¬æ˜¾å­˜\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvidia-smi è·å– NVIDIA GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # è§£æè¾“å‡ºï¼Œè·å– GPU åç§°å’Œæ˜¾å­˜\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # æ ¼å¼åŒ– GPU ä¿¡æ¯\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # è¿”å› GPU ä¿¡æ¯æˆ–æç¤ºä¿¡æ¯\n",
    "        else:\n",
    "             # å°è¯•ä½¿ç”¨ lshw è·å–å…¶ä»– GPU ä¿¡æ¯ (éœ€è¦å®‰è£… lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "                     # ç®€å•è§£æè¾“å‡ºä¸­çš„ product åç§°å’Œæ˜¾å­˜\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # æ·»åŠ æœ€åä¸€ä¸ª GPU çš„ä¿¡æ¯\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # å¦‚æœæ‰¾åˆ° GPU ä½†ä¿¡æ¯æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # å¦‚æœä¸¤ä¸ªå‘½ä»¤éƒ½æ‰¾ä¸åˆ° GPUï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # å¦‚æœæ‰¾ä¸åˆ° lshw å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # å¦‚æœæ‰¾ä¸åˆ° nvidia-smi å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "\n",
    "# è·å– CUDA ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ nvcc --version è·å– CUDA ç‰ˆæœ¬\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # æŸ¥æ‰¾åŒ…å« 'release' çš„è¡Œ\n",
    "                    return line.split('release ')[1].split(',')[0] # è§£æè¡Œï¼Œæå–ç‰ˆæœ¬å·\n",
    "        return \"CUDA not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° CUDA æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # å¦‚æœæ‰¾ä¸åˆ° nvcc å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å– Python ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_python_version():\n",
    "    return platform.python_version() # è·å– Python ç‰ˆæœ¬\n",
    "\n",
    "# è·å– Conda ç‰ˆæœ¬çš„å‡½æ•°\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨ conda --version è·å– Conda ç‰ˆæœ¬\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            return result.stdout.strip() # è¿”å› Conda ç‰ˆæœ¬\n",
    "        return \"Conda not found or version not parsed\" # å¦‚æœæ‰¾ä¸åˆ° Conda æˆ–ç‰ˆæœ¬æ— æ³•è§£æï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # å¦‚æœæ‰¾ä¸åˆ° conda å‘½ä»¤ï¼Œè®¾ç½®æç¤ºä¿¡æ¯\n",
    "\n",
    "# è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯çš„å‡½æ•°\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # è·å–æ ¹ç›®å½•çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ\n",
    "        total_gb = total / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        used_gb = used / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        free_gb = free / (1024**3) # è½¬æ¢ä¸º GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # æ ¼å¼åŒ–è¾“å‡º\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # å¦‚æœè·å–ä¿¡æ¯å‡ºé”™ï¼Œè®¾ç½®é”™è¯¯ä¿¡æ¯\n",
    "\n",
    "# è·å–ç¯å¢ƒä¿¡æ¯\n",
    "os_name = platform.system() # è·å–æ“ä½œç³»ç»Ÿåç§°\n",
    "os_version = platform.release() # è·å–æ“ä½œç³»ç»Ÿç‰ˆæœ¬\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # åœ¨ Linux ä¸Šå°è¯•è·å–å‘è¡Œç‰ˆå’Œç‰ˆæœ¬\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # å¦‚æœå‘½ä»¤æˆåŠŸæ‰§è¡Œ\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # æŸ¥æ‰¾åŒ…å« 'Description:' çš„è¡Œ\n",
    "                    os_version = line.split('Description:')[1].strip() # æå–æè¿°ä¿¡æ¯ä½œä¸ºç‰ˆæœ¬\n",
    "                    break # æ‰¾åˆ°åé€€å‡ºå¾ªç¯\n",
    "                elif 'Release:' in line: # æŸ¥æ‰¾åŒ…å« 'Release:' çš„è¡Œ\n",
    "                     os_version = line.split('Release:')[1].strip() # æå–ç‰ˆæœ¬å·\n",
    "                     # å°è¯•è·å– codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # å°† codename æ·»åŠ åˆ°ç‰ˆæœ¬ä¿¡æ¯ä¸­\n",
    "                     except:\n",
    "                         pass # å¦‚æœè·å– codename å¤±è´¥åˆ™å¿½ç•¥\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release å¯èƒ½æœªå®‰è£…ï¼Œå¿½ç•¥é”™è¯¯\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # ç»„åˆå®Œæ•´çš„æ“ä½œç³»ç»Ÿä¿¡æ¯\n",
    "cpu_info = get_cpu_info() # è°ƒç”¨å‡½æ•°è·å– CPU ä¿¡æ¯å’Œæ ¸å¿ƒæ•°é‡\n",
    "memory_info = get_memory_info() # è°ƒç”¨å‡½æ•°è·å–å†…å­˜ä¿¡æ¯\n",
    "gpu_info = get_gpu_info() # è°ƒç”¨å‡½æ•°è·å– GPU ä¿¡æ¯å’Œæ˜¾å­˜\n",
    "cuda_version = get_cuda_version() # è°ƒç”¨å‡½æ•°è·å– CUDA ç‰ˆæœ¬\n",
    "python_version = get_python_version() # è°ƒç”¨å‡½æ•°è·å– Python ç‰ˆæœ¬\n",
    "conda_version = get_conda_version() # è°ƒç”¨å‡½æ•°è·å– Conda ç‰ˆæœ¬\n",
    "disk_info = get_disk_space() # è°ƒç”¨å‡½æ•°è·å–ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "\n",
    "\n",
    "# åˆ›å»ºç”¨äºå­˜å‚¨æ•°æ®çš„å­—å…¸\n",
    "env_data = {\n",
    "    \"é¡¹ç›®\": [ # é¡¹ç›®åç§°åˆ—è¡¨\n",
    "        \"æ“ä½œç³»ç»Ÿ\",\n",
    "        \"CPU ä¿¡æ¯\",\n",
    "        \"å†…å­˜ä¿¡æ¯\",\n",
    "        \"GPU ä¿¡æ¯\",\n",
    "        \"CUDA ä¿¡æ¯\",\n",
    "        \"Python ç‰ˆæœ¬\",\n",
    "        \"Conda ç‰ˆæœ¬\",\n",
    "        \"ç‰©ç†ç£ç›˜ç©ºé—´\" # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´\n",
    "    ],\n",
    "    \"ä¿¡æ¯\": [ # å¯¹åº”çš„ä¿¡æ¯åˆ—è¡¨\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # æ·»åŠ ç‰©ç†ç£ç›˜ç©ºé—´ä¿¡æ¯\n",
    "    ]\n",
    "}\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# æ‰“å°è¡¨æ ¼\n",
    "print(\"### ç¯å¢ƒä¿¡æ¯\") # æ‰“å°æ ‡é¢˜\n",
    "print(df.to_markdown(index=False)) # å°† DataFrame è½¬æ¢ä¸º Markdown æ ¼å¼å¹¶æ‰“å°ï¼Œä¸åŒ…å«ç´¢å¼•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ 0 éƒ¨åˆ†: è®¾ç½®ä¸å®‰è£…\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å¿…è¦çš„ Python åŒ…å¹¶è®¾ç½®ç¯å¢ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: langchain==1.1.3 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (1.1.3)\n",
      "Requirement already satisfied: langchain-openai==1.1.1 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: python-dotenv==1.2.1 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (1.2.1)\n",
      "Requirement already satisfied: langgraph==1.0.4 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (1.0.4)\n",
      "Requirement already satisfied: sqlalchemy==2.0.44 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (2.0.44)\n",
      "Requirement already satisfied: requests==2.32.5 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (2.32.5)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.1.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain==1.1.3) (1.1.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain==1.1.3) (2.12.5)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langgraph==1.0.4) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langgraph==1.0.4) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langgraph==1.0.4) (0.2.14)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langgraph==1.0.4) (3.6.0)\n",
      "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-openai==1.1.1) (2.9.0)\n",
      "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-openai==1.1.1) (0.12.0)\n",
      "Requirement already satisfied: greenlet>=1 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from sqlalchemy==2.0.44) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from sqlalchemy==2.0.44) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from requests==2.32.5) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from requests==2.32.5) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from requests==2.32.5) (2.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from requests==2.32.5) (2025.11.12)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (0.4.56)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (25.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (9.1.2)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (0.12.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph==1.0.4) (1.12.0)\n",
      "Requirement already satisfied: httpx>=0.25.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph==1.0.4) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.10.1 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph==1.0.4) (3.11.5)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.1.2->langchain==1.1.3) (0.25.0)\n",
      "Requirement already satisfied: anyio in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph==1.0.4) (4.12.0)\n",
      "Requirement already satisfied: httpcore==1.* in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph==1.0.4) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph==1.0.4) (0.16.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai==1.1.1) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai==1.1.1) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai==1.1.1) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from openai<3.0.0,>=1.109.1->langchain-openai==1.1.1) (4.67.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph==1.0.4) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.1.3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.1.3) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain==1.1.3) (0.4.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /root/miniconda3/envs/langchain1.x/lib/python3.10/site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai==1.1.1) (2025.11.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# å®‰è£…æ‰€éœ€çš„åŒ…\n",
    "!pip install langchain==1.1.3 \\\n",
    "langchain-openai==1.1.1 \\\n",
    "python-dotenv==1.2.1 \\\n",
    "langgraph==1.0.4 \\\n",
    "sqlalchemy==2.0.44 \\\n",
    "requests==2.32.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é…ç½®æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥å¿…è¦çš„æ¨¡å—\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    è®¾ç½®ç¯å¢ƒå˜é‡çš„è¾…åŠ©å‡½æ•°\n",
    "    å¦‚æœç¯å¢ƒå˜é‡ä¸å­˜åœ¨ï¼Œåˆ™æç¤ºç”¨æˆ·è¾“å…¥\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "# sk-AGnnT4BAKJBA22uYA32fF98e9d1645739916347057D2A14f\n",
    "\n",
    "# è®¾ç½® OpenAI API å¯†é’¥ \n",
    "# è¿™æ˜¯ä½¿ç”¨ OpenAI æ¨¡å‹æ‰€å¿…éœ€çš„\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# è®¾ç½® OpenAI APIä»£ç†åœ°å€ (ä¾‹å¦‚ï¼šhttps://api.apiyi.com/v1ï¼‰\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "æ¨¡å‹åˆå§‹åŒ–æ–‡ä»¶\n",
    "\n",
    "æ­¤æ–‡ä»¶é…ç½®åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨çš„ LLM (å¤§è¯­è¨€æ¨¡å‹)ã€‚\n",
    "\n",
    "é»˜è®¤é…ç½®:\n",
    "- é»˜è®¤æä¾›å•†æ˜¯ OpenAI (ä½¿ç”¨ o3-mini æ¨¡å‹)\n",
    "- ä½ ä¹Ÿå¯ä»¥é€šè¿‡å–æ¶ˆæ³¨é‡Šç›¸åº”çš„è¡Œæ¥åˆ‡æ¢åˆ° Anthropic\n",
    "\n",
    "å¤‡é€‰æä¾›å•†:\n",
    "è¦ä½¿ç”¨ä¸åŒçš„ LLM æä¾›å•†ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œ:\n",
    "1. æ³¨é‡Šæ‰ä¸‹æ–¹çš„ \"Default Models\" (é»˜è®¤æ¨¡å‹) éƒ¨åˆ†\n",
    "2. å–æ¶ˆæ³¨é‡Šä½ æƒ³è¦çš„æä¾›å•†éƒ¨åˆ†:\n",
    "   - Azure OpenAI: éœ€è¦è®¾ç½® AZURE_OPENAI_API_KEY å’Œ AZURE_OPENAI_ENDPOINT\n",
    "   - AWS Bedrock: éœ€è¦è®¾ç½® AWS å‡­è¯å’Œé…ç½®\n",
    "   - Google Vertex AI: éœ€è¦è®¾ç½® GOOGLE_APPLICATION_CREDENTIALS\n",
    "3. æŒ‰ç…§æ¯ä¸ªéƒ¨åˆ†å†…çš„è®¾ç½®è¯´æ˜è¿›è¡Œæ“ä½œ\n",
    "\"\"\"\n",
    "\n",
    "# \"\"\"é»˜è®¤æ¨¡å‹ (Default Models)\"\"\"\n",
    "from langchain.chat_models import init_chat_model\n",
    "# åˆå§‹åŒ–èŠå¤©æ¨¡å‹ï¼Œè¿™é‡Œä½¿ç”¨ OpenAI çš„ o3-mini\n",
    "model = init_chat_model(\"openai:o3-mini\")\n",
    "\n",
    "# ä½¿ç”¨ Anthropic æ›¿ä»£ OpenAI\n",
    "# model = init_chat_model(\"anthropic:claude-haiku-4-5\")\n",
    "\n",
    "\n",
    "# \"\"\"AZURE OpenAI Version\"\"\"\n",
    "# from langchain_openai import AzureChatOpenAI\n",
    "# # from langchain_anthropic import ChatAnthropic\n",
    "# # from langchain_google_vertexai import ChatVertexAI\n",
    "# from azure.identity import InteractiveBrowserCredential\n",
    "\n",
    "# credential = InteractiveBrowserCredential()\n",
    "\n",
    "# def get_token():\n",
    "#     token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "#     return token.token\n",
    "\n",
    "# For AzureOpenAI, make sure you set AZURE_OPENAI_API_KEY and AZURE_OPENAI_ENDPOINT\n",
    "\n",
    "# Azure OpenAI: Using Environment Variables\n",
    "# AZURE_OPENAI_GPT_4O = AzureChatOpenAI(\n",
    "#     azure_deployment=\"gpt-4o\",\n",
    "#     streaming=True\n",
    "# )\n",
    "\n",
    "# Azure OpenAI: Using Azure AD\n",
    "# AZURE_OPENAI_GPT_4O = AzureChatOpenAI(\n",
    "#     api_version=\"2024-03-01-preview\",\n",
    "#     azure_endpoint=\"https://deployment.openai.azure.com/\",\n",
    "#     azure_deployment=\"gpt-4o\",\n",
    "#     azure_ad_token_provider=get_token\n",
    "# )\n",
    "\n",
    "\n",
    "# \"\"\"Bedrock Version\"\"\"\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain_aws import ChatBedrockConverse\n",
    "# import os\n",
    "\n",
    "# load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "\n",
    "# AWS_ACCESS_KEY_ID=os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "# AWS_SECRET_ACCESS_KEY=os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "# AWS_REGION_NAME=os.getenv(\"AWS_REGION_NAME\")\n",
    "# AWS_MODEL_ARN=os.getenv(\"AWS_MODEL_ARN\")\n",
    "\n",
    "# model = ChatBedrockConverse(\n",
    "#     aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "#     aws_secret_access_key=AWS_SECRET_ACCESS_KEY, \n",
    "#     region_name=AWS_REGION_NAME,\n",
    "#     provider=\"anthropic\",\n",
    "#     model_id=AWS_MODEL_ARN\n",
    "# )\n",
    "\n",
    "\n",
    "# \"\"\"Google Vertex AI version\"\"\"\n",
    "# Make sure you have your vertex ai credentials setup and your GOOGLE_APPLICATION_CREDENTIALS are pointing to the JSON file. \n",
    "\n",
    "# import os\n",
    "# from pathlib import Path\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain.chat_models import init_chat_model\n",
    "\n",
    "# # Find project root and load .env\n",
    "# # Use __file__ to get the location of this file, then go up two directories to project root\n",
    "# project_root = Path.cwd().parent.parent\n",
    "# load_dotenv(dotenv_path=project_root / \".env\", override=True)\n",
    "\n",
    "# # Fix credentials path to absolute\n",
    "# if \"GOOGLE_APPLICATION_CREDENTIALS\" in os.environ:\n",
    "#     cred_path = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "#     if not os.path.isabs(cred_path):\n",
    "#         os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = str(project_root / cred_path.lstrip(\"./\"))\n",
    "\n",
    "# # Create model\n",
    "# model = init_chat_model(\"google_vertexai:gemini-2.5-flash\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¸¸ç”¨æ–¹æ³•é…ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.pool import StaticPool\n",
    "\n",
    "def show_graph(graph, xray=False):\n",
    "    \"\"\"å±•ç¤º LangGraph çš„ Mermaid æµç¨‹å›¾ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ ASCII å›¾è¡¨ä½œä¸ºåå¤‡ã€‚\n",
    "    \n",
    "    å‚æ•°:\n",
    "        graph: æ‹¥æœ‰ get_graph() æ–¹æ³•çš„ LangGraph å¯¹è±¡\n",
    "        xray: æ˜¯å¦å±•ç¤ºå›¾çš„å†…éƒ¨ç»“æ„ (ä¾‹å¦‚å­å›¾)\n",
    "    \"\"\"\n",
    "    from IPython.display import Image\n",
    "    try:\n",
    "        return Image(graph.get_graph(xray=xray).draw_mermaid_png())\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  å›¾ç‰‡æ¸²æŸ“å¤±è´¥: {e}\")\n",
    "        print(\"\\nğŸ“Š æ”¹ä¸ºå±•ç¤º ASCII å›¾è¡¨:\\n\")\n",
    "        ascii_diagram = graph.get_graph(xray=xray).draw_ascii()\n",
    "        print(ascii_diagram)\n",
    "        return None\n",
    "\n",
    "def get_engine_for_chinook_db():\n",
    "    \"\"\"ä¸‹è½½ SQL æ–‡ä»¶ï¼Œå¡«å……å†…å­˜æ•°æ®åº“ï¼Œå¹¶åˆ›å»º SQLAlchemy å¼•æ“ã€‚\"\"\"\n",
    "    url = \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sql\"\n",
    "    response = requests.get(url)\n",
    "    sql_script = response.text\n",
    "\n",
    "    connection = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
    "    connection.executescript(sql_script)\n",
    "    return create_engine(\n",
    "        \"sqlite://\",\n",
    "        creator=lambda: connection,\n",
    "        poolclass=StaticPool,\n",
    "        connect_args={\"check_same_thread\": False},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿½ç•¥ LangSmith ä½¿ç”¨ UUID v7 çš„è­¦å‘Šï¼ˆåœ¨ç®€å•çš„æ— çº¿ç¨‹ ID ç¤ºä¾‹ä¸­ï¼‰\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='LangSmith now uses UUID v7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ 1 éƒ¨åˆ†: ä½¿ç”¨ Interrupts å®ç°äººæœºäº¤äº’ (Human-in-the-Loop)\n",
    "\n",
    "### é—®é¢˜èƒŒæ™¯\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨æ„å»ºä¸€ä¸ªå¯ä»¥å‘é€ç”µå­é‚®ä»¶æˆ–è¿›è¡Œç”µå­å•†åŸä¸‹å•è´­ä¹°çš„ Agentã€‚ä½ è‚¯å®šä¸å¸Œæœ›å®ƒè‡ªåŠ¨æ‰§è¡Œè¿™äº›æ“ä½œ â€”â€” ä½ å¸Œæœ›å…ˆç»è¿‡äººå·¥æ‰¹å‡†ï¼\n",
    "\n",
    "**Human-in-the-Loop** å…è®¸ä½ ï¼š\n",
    "- æš‚åœæ‰§è¡Œä»¥è¿›è¡Œå®¡æŸ¥\n",
    "- æ‰¹å‡†ã€æ‹’ç»æˆ–ç¼–è¾‘åŠ¨ä½œ\n",
    "- ä¸ºæ•æ„Ÿæ“ä½œæ·»åŠ å®‰å…¨æ§åˆ¶\n",
    "\n",
    "### å·¥ä½œåŸç† (LangGraph 1.0 æ–°ç‰¹æ€§)\n",
    "\n",
    "LangGraph 1.0 å¼•å…¥äº†åŠ¨æ€ `interrupt` å‡½æ•°ï¼Œæ›¿ä»£äº†æ—§ç‰ˆè¾ƒä¸ºé™æ€çš„é…ç½®æ–¹å¼ã€‚\n",
    "\n",
    "1. Agent é‡åˆ° `interrupt()` å‡½æ•° - æ‰§è¡Œæš‚åœ\n",
    "2. ç³»ç»Ÿå°†ä¿¡æ¯å±•ç¤ºç»™äººç±»\n",
    "3. äººç±»æä¾›è¾“å…¥ï¼ˆæ‰¹å‡†/æ‹’ç»/ç¼–è¾‘ï¼‰\n",
    "4. Agent ä½¿ç”¨ `Command(resume=...)` æ¢å¤æ‰§è¡Œï¼Œå¹¶å°†äººç±»è¾“å…¥ä½œä¸º `interrupt()` çš„è¿”å›å€¼ä¼ å›"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹ 1: ç®€å•çš„å®¡æ‰¹å·¥ä½œæµ\n",
    "\n",
    "è®©æˆ‘ä»¬ä»ä¸€ä¸ªç®€å•çš„ä¾‹å­å¼€å§‹â€”â€”åœ¨å‘é€ç”µå­é‚®ä»¶ä¹‹å‰è¯·æ±‚æ‰¹å‡†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·¥å…·åˆ›å»ºæˆåŠŸï¼\n",
      "å·¥å…·åç§°: send_email\n",
      "å·¥å…·æè¿°: å‘é€é‚®ä»¶ç»™æ¥æ”¶è€…ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import interrupt\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def send_email(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"å‘é€é‚®ä»¶ç»™æ¥æ”¶è€…ã€‚\"\"\"\n",
    "    \n",
    "    # æš‚åœä»¥ç­‰å¾…äººå·¥æ‰¹å‡†\n",
    "    # interrupt æ¥æ”¶çš„å‚æ•°ä¼šä½œä¸ºæš‚åœæ—¶çš„ä¿¡æ¯è¿”å›ç»™å‰ç«¯/ç”¨æˆ·\n",
    "    # ç¨‹åºä¼šåœ¨è¿™é‡ŒæŒ‚èµ·ï¼Œç›´åˆ°æ”¶åˆ° resume æŒ‡ä»¤\n",
    "    approval = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"message\": \"æ‚¨ç¡®å®šè¦å‘é€è¿™å°é‚®ä»¶å—ï¼Ÿ\"\n",
    "    })\n",
    "    \n",
    "    # æ¢å¤æ‰§è¡Œåï¼Œapproval å˜é‡å°†åŒ…å« resume æ—¶ä¼ å…¥çš„æ•°æ®\n",
    "    if approval.get(\"approved\"): # å¦‚æœé€šè¿‡æ‰¹å‡†ï¼ˆæ ¹æ®æˆ‘ä»¬å®šä¹‰çš„åè®®ï¼‰\n",
    "        # åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¿™é‡Œä¼šå®é™…å‘é€é‚®ä»¶\n",
    "        return f\"é‚®ä»¶å·²å‘é€è‡³ {to}ï¼Œä¸»é¢˜ä¸º '{subject}'\"\n",
    "    else:\n",
    "        return \"ç”¨æˆ·å–æ¶ˆäº†é‚®ä»¶å‘é€\"\n",
    "\n",
    "# æµ‹è¯•å·¥å…·å®šä¹‰\n",
    "print(\"å·¥å…·åˆ›å»ºæˆåŠŸï¼\")\n",
    "print(f\"å·¥å…·åç§°: {send_email.name}\")\n",
    "print(f\"å·¥å…·æè¿°: {send_email.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åˆ›å»ºå¸¦æœ‰ Human-in-the-Loop çš„ Agent\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä½¿ç”¨æ­¤å·¥å…·çš„ Agentã€‚**åˆ‡è®°ï¼š** Interrupts å¿…é¡»é…åˆ Checkpointer ä½¿ç”¨ï¼Œå¦åˆ™æ— æ³•ä¿å­˜æš‚åœæ—¶çš„çŠ¶æ€ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# åˆ›å»º Checkpointer ç”¨äºæŒä¹…åŒ–\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# åˆ›å»ºå¸¦æœ‰é‚®ä»¶å·¥å…·çš„ Agent\n",
    "agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[send_email],\n",
    "    system_prompt=\"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„é‚®ä»¶åŠ©æ‰‹ã€‚å½“è¢«è¦æ±‚å‘é€é‚®ä»¶æ—¶ï¼Œè¯·ä½¿ç”¨ send_email å·¥å…·ã€‚\",\n",
    "    checkpointer=checkpointer  # å¿…é¡»é¡¹ï¼šç”¨äº interrupts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¿è¡Œç›´åˆ°ä¸­æ–­ (Running Until Interrupt)\n",
    "\n",
    "è®©æˆ‘ä»¬è¿è¡Œ Agentï¼Œçœ‹çœ‹å®ƒå¦‚ä½•æš‚åœä»¥ç­‰å¾…æ‰¹å‡†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent å·²æš‚åœç­‰å¾…æ‰¹å‡†\n",
      "\n",
      "ä¸­æ–­è¯¦æƒ…ï¼š\n",
      "  æ”¶ä»¶äºº: alice@example.com\n",
      "  ä¸»é¢˜: æ˜å¤©å¼€ä¼š\n",
      "  å†…å®¹: æˆ‘ä»¬ä¸‹åˆ3ç‚¹è§é¢ã€‚\n",
      "  æ¶ˆæ¯: æ‚¨ç¡®å®šè¦å‘é€è¿™å°é‚®ä»¶å—ï¼Ÿ\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "from langsmith import uuid7\n",
    "\n",
    "# ä¸ºæ­¤å¯¹è¯åˆ›å»ºå”¯ä¸€çº¿ç¨‹\n",
    "config = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "# è¿è¡Œ Agent\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"ç»™ alice@example.com å‘é€ä¸€å°é‚®ä»¶ï¼Œä¸»é¢˜æ˜¯ 'æ˜å¤©å¼€ä¼š'ï¼Œå†…å®¹æ˜¯ 'æˆ‘ä»¬ä¸‹åˆ3ç‚¹è§é¢ã€‚'\")]\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦è§¦å‘äº† interrupt\n",
    "# åœ¨ LangGraph 1.0 ä¸­ï¼Œå¦‚æœå›  interrupt æš‚åœï¼Œç»“æœä¸­ä¼šåŒ…å« __interrupt__ é”®\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"Agent å·²æš‚åœç­‰å¾…æ‰¹å‡†\\n\")\n",
    "\n",
    "    # è·å– interrupt çš„è¯¦ç»†ä¿¡æ¯\n",
    "    interrupt_info = result[\"__interrupt__\"][0]\n",
    "\n",
    "    print(\"ä¸­æ–­è¯¦æƒ…ï¼š\")\n",
    "    print(f\"  æ”¶ä»¶äºº: {interrupt_info.value['to']}\")\n",
    "    print(f\"  ä¸»é¢˜: {interrupt_info.value['subject']}\")\n",
    "    print(f\"  å†…å®¹: {interrupt_info.value['body']}\")\n",
    "    print(f\"  æ¶ˆæ¯: {interrupt_info.value['message']}\")\n",
    "else:\n",
    "    print(\"Agent æ‰§è¡Œå®Œæˆï¼Œæœªè§¦å‘ä¸­æ–­\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰¹å‡†å¹¶æ¢å¤æ‰§è¡Œ (Resuming with Approval)\n",
    "\n",
    "ç°åœ¨è®©æˆ‘ä»¬æ‰¹å‡†è¿™å°é‚®ä»¶ï¼Œå¹¶è®© Agent ç»§ç»­è¿è¡Œã€‚\n",
    "æˆ‘ä»¬ä½¿ç”¨ `Command` å¯¹è±¡å¹¶ä¼ å…¥ `resume` å‚æ•°ã€‚è¿™ä¸ªå€¼ä¼šç›´æ¥æˆä¸º `interrupt()` å‡½æ•°åœ¨å·¥å…·å†…éƒ¨çš„è¿”å›å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç»ˆå“åº”ï¼š\n",
      "é‚®ä»¶å·²å‘é€è‡³ alice@example.comï¼Œä¸»é¢˜ä¸º 'æ˜å¤©å¼€ä¼š'ï¼Œå†…å®¹æ˜¯ 'æˆ‘ä»¬ä¸‹åˆ3ç‚¹è§é¢ã€‚'\n"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command\n",
    "\n",
    "# æ¢å¤æ‰§è¡Œå¹¶ç»™äºˆæ‰¹å‡†\n",
    "# è¿™é‡Œçš„ {\"approved\": True} å°†ä¼šæ˜¯å·¥å…·ä¸­ approval å˜é‡çš„å€¼\n",
    "result = agent.invoke(\n",
    "    Command(resume={\"approved\": True}),\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# æ‰“å°æœ€ç»ˆå“åº”\n",
    "print(\"æœ€ç»ˆå“åº”ï¼š\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç»ƒä¹ ï¼šå°è¯•æ‹’ç»é‚®ä»¶\n",
    "\n",
    "å†æ¬¡è¿è¡Œä¸Šé¢çš„æ­¥éª¤ï¼Œä½†è¿™æ¬¡é€šè¿‡ä¼ å…¥ `{\"approved\": False}` æ¥æ‹’ç»é‚®ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç»ˆå“åº”ï¼š\n",
      "é‚®ä»¶å‘é€å·²å–æ¶ˆã€‚å¦‚æœéœ€è¦é‡æ–°å‘é€é‚®ä»¶ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ä¸ºæ‹’ç»ç¤ºä¾‹åˆ›å»ºä¸€ä¸ªæ–°çº¿ç¨‹\n",
    "config_2 = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "# è¿è¡Œç›´åˆ°ä¸­æ–­\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"ç»™ bob@example.com å‘é€ä¸€å°é‚®ä»¶ï¼Œå†…å®¹æ˜¯ 'ä½ å¥½ï¼'\")]\n",
    "    },\n",
    "    config=config_2\n",
    ")\n",
    "\n",
    "# æ¢å¤å¹¶æ‹’ç»\n",
    "result = agent.invoke(\n",
    "    Command(resume={\"approved\": False}),  # æ‹’ç»é‚®ä»¶\n",
    "    config=config_2\n",
    ")\n",
    "\n",
    "print(\"æœ€ç»ˆå“åº”ï¼š\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content=\"ç»™ bob@example.com å‘é€ä¸€å°é‚®ä»¶ï¼Œå†…å®¹æ˜¯ 'ä½ å¥½ï¼'\", additional_kwargs={}, response_metadata={}, id='5e5991f2-faaa-42d5-ab60-74d89977e0aa'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 307, 'prompt_tokens': 94, 'total_tokens': 401, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'o3-mini', 'system_fingerprint': 'fp_52e4403cb8', 'id': 'chatcmpl-Cl8hysE3D7ZTMze0biSyN6qwdBmpY', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b071c-da2f-7110-87d3-4fb7a0b8411f-0', tool_calls=[{'name': 'send_email', 'args': {'to': 'bob@example.com', 'subject': 'é—®å€™', 'body': 'ä½ å¥½ï¼'}, 'id': 'call_ZhwLnofAKspfJEUQklT9cV90', 'type': 'tool_call'}], usage_metadata={'input_tokens': 94, 'output_tokens': 307, 'total_tokens': 401, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}), ToolMessage(content='ç”¨æˆ·å–æ¶ˆäº†é‚®ä»¶å‘é€', name='send_email', id='147a976b-c8a1-4184-aafb-5cf008670bb7', tool_call_id='call_ZhwLnofAKspfJEUQklT9cV90'), AIMessage(content='é‚®ä»¶å‘é€å·²å–æ¶ˆã€‚å¦‚æœéœ€è¦é‡æ–°å‘é€é‚®ä»¶ï¼Œè¯·å‘Šè¯‰æˆ‘ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 153, 'prompt_tokens': 135, 'total_tokens': 288, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'o3-mini', 'system_fingerprint': 'fp_52e4403cb8', 'id': 'chatcmpl-Cl8hMf7Ktp3eB7a0LoTWwUYtAXDLn', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b071c-f1c9-72c2-a7b1-0f252fcdad2f-0', usage_metadata={'input_tokens': 135, 'output_tokens': 153, 'total_tokens': 288, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}})]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ 2 éƒ¨åˆ†: è¿›é˜¶æ¨¡å¼ - æ‰§è¡Œå‰å…ˆç¼–è¾‘ (Edit Before Execution)\n",
    "\n",
    "æœ‰æ—¶ä½ ä¸ä»…ä»…æƒ³æ‰¹å‡†æˆ–æ‹’ç»ï¼Œè¿˜æƒ³**ç¼–è¾‘**å·¥å…·è°ƒç”¨çš„å‚æ•°ã€‚è®©æˆ‘ä»¬å‡çº§æˆ‘ä»¬çš„å·¥å…·ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def send_email_v2(to: str, subject: str, body: str) -> str:\n",
    "    \"\"\"å‘é€é‚®ä»¶ç»™æ¥æ”¶è€…ã€‚\"\"\"\n",
    "    \n",
    "    # æš‚åœç­‰å¾…äººå·¥å®¡æŸ¥\n",
    "    response = interrupt({\n",
    "        \"action\": \"send_email\",\n",
    "        \"to\": to,\n",
    "        \"subject\": subject,\n",
    "        \"body\": body,\n",
    "        \"message\": \"è¯·å®¡æŸ¥è¿™å°é‚®ä»¶ã€‚æ‚¨å¯ä»¥æ‰¹å‡†ã€æ‹’ç»æˆ–ç¼–è¾‘å®ƒã€‚\"\n",
    "    })\n",
    "    \n",
    "    # å¤„ç†ä¸åŒçš„å“åº”ç±»å‹\n",
    "    if response[\"type\"] == \"approve\":\n",
    "        return f\"é‚®ä»¶å·²å‘é€è‡³ {to}ï¼Œä¸»é¢˜ä¸º '{subject}'\"\n",
    "\n",
    "    elif response[\"type\"] == \"reject\":\n",
    "        return \"é‚®ä»¶å·²å–æ¶ˆ\"\n",
    "\n",
    "    elif response[\"type\"] == \"edit\":\n",
    "        # æ›´æ–°ä¸ºç¼–è¾‘åçš„å€¼\n",
    "        to = response.get(\"to\", to)\n",
    "        subject = response.get(\"subject\", subject)\n",
    "        body = response.get(\"body\", body)\n",
    "        return f\"\"\"Email sent with edits:\n",
    "                To: {to}\n",
    "                Subject: {subject}\n",
    "                Body: {body}\"\"\"\n",
    "    \n",
    "    return \"æœªçŸ¥å“åº”\"\n",
    "\n",
    "# åˆ›å»ºä½¿ç”¨å¢å¼ºå·¥å…·çš„æ–° Agent\n",
    "agent_v2 = create_agent(\n",
    "    model=model,\n",
    "    tools=[send_email_v2],\n",
    "    system_prompt=\"ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„é‚®ä»¶åŠ©æ‰‹ã€‚\",\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æš‚åœç­‰å¾…å®¡æŸ¥...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œå¹¶ç¼–è¾‘é‚®ä»¶\n",
    "config_3 = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "# è¿è¡Œç›´åˆ°ä¸­æ–­\n",
    "result = agent_v2.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"ç»™ team@example.com å‘é€ä¸€å°å…³äºä¼šè®®çš„é‚®ä»¶\")]\n",
    "    },\n",
    "    config=config_3\n",
    ")\n",
    "\n",
    "print(\"æš‚åœç­‰å¾…å®¡æŸ¥...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä¿®æ”¹é‚®ä»¶ä¸»é¢˜ï¼Œå°†å…¶æ ‡è®°ä¸º URGENT (ç´§æ€¥) ä¼šè®®ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æœ€ç»ˆå“åº”ï¼š\n",
      "é‚®ä»¶å·²æˆåŠŸå‘é€ç»™ team@example.comã€‚\n"
     ]
    }
   ],
   "source": [
    "# æ¢å¤å¹¶å¸¦å…¥ç¼–è¾‘åçš„å†…å®¹\n",
    "result = agent_v2.invoke(\n",
    "    Command(resume={\n",
    "        \"type\": \"edit\",\n",
    "        \"subject\": \"ç´§æ€¥ï¼šä»Šå¤©ä¸‹åˆ2ç‚¹å¼€ä¼š\",  # æˆ‘ä»¬ä¿®æ”¹äº†é‚®ä»¶ä¸»é¢˜\n",
    "        \"body\": \"è¿™æ˜¯ç¼–è¾‘åçš„é‚®ä»¶å†…å®¹ï¼ŒåŒ…å«æ›´å¤šç»†èŠ‚ã€‚\"\n",
    "    }),\n",
    "    config=config_3\n",
    ")\n",
    "\n",
    "print(\"æœ€ç»ˆå“åº”ï¼š\")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='ç»™ team@example.com å‘é€ä¸€å°å…³äºä¼šè®®çš„é‚®ä»¶', additional_kwargs={}, response_metadata={}, id='3e8f547f-72fc-4f06-ac91-4db684448115'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 80, 'total_tokens': 402, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 192, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'o3-mini', 'system_fingerprint': 'fp_52e4403cb8', 'id': 'chatcmpl-Cl8kf9ikg92RcXKO5GtEqfc7a4d1c', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b071f-5877-7733-a1e4-844b029f87fb-0', tool_calls=[{'name': 'send_email_v2', 'args': {'body': 'å¤§å®¶å¥½ï¼Œ\\n\\næˆ‘æƒ³å°±å³å°†å¬å¼€çš„ä¼šè®®ä¸å¤§å®¶æ²Ÿé€šä¸€äº›äº‹é¡¹ã€‚ä¼šè®®çš„æ—¶é—´ã€åœ°ç‚¹ä»¥åŠè®®ç¨‹å®‰æ’ç­‰ç»†èŠ‚ç›®å‰æ­£åœ¨ç¡®è®¤ä¸­ï¼Œå¾…ç¡®å®šåæˆ‘ä¼šç¬¬ä¸€æ—¶é—´ä¸å¤§å®¶åˆ†äº«ã€‚\\n\\nå¦‚æœå¤§å®¶æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·éšæ—¶ä¸æˆ‘è”ç³»ã€‚\\n\\nè°¢è°¢å¤§å®¶çš„é…åˆï¼ŒæœŸå¾…åœ¨ä¼šè®®ä¸­è§åˆ°å„ä½ã€‚\\n\\næ­¤è‡´\\næ•¬ç¤¼', 'subject': 'å…³äºä¼šè®®çš„é€šçŸ¥', 'to': 'team@example.com'}, 'id': 'call_MHNPCEVuAWtxO94rgOAVLhER', 'type': 'tool_call'}], usage_metadata={'input_tokens': 80, 'output_tokens': 322, 'total_tokens': 402, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 192}}), ToolMessage(content='Email sent with edits:\\n                To: team@example.com\\n                Subject: ç´§æ€¥ï¼šä»Šå¤©ä¸‹åˆ2ç‚¹å¼€ä¼š\\n                Body: è¿™æ˜¯ç¼–è¾‘åçš„é‚®ä»¶å†…å®¹ï¼ŒåŒ…å«æ›´å¤šç»†èŠ‚ã€‚', name='send_email_v2', id='45ac30da-989b-4c2f-8923-1bd7aee746e5', tool_call_id='call_MHNPCEVuAWtxO94rgOAVLhER'), AIMessage(content='é‚®ä»¶å·²æˆåŠŸå‘é€ç»™ team@example.comã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 149, 'prompt_tokens': 250, 'total_tokens': 399, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}, 'input_tokens': 0, 'output_tokens': 0, 'input_tokens_details': None}, 'model_provider': 'openai', 'model_name': 'o3-mini', 'system_fingerprint': 'fp_52e4403cb8', 'id': 'chatcmpl-2025sosA7S8i4iDzhZ9DOs0Lhn456', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b071f-6c06-7e41-9e40-1e036a325c44-0', usage_metadata={'input_tokens': 250, 'output_tokens': 149, 'total_tokens': 399, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}})]\n"
     ]
    }
   ],
   "source": [
    "print(result[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ 3 éƒ¨åˆ†: ä¸­é—´ä»¶ (Middleware) ç®€ä»‹\n",
    "\n",
    "**Middleware (ä¸­é—´ä»¶)** æä¾›äº†å¯¹ Agent å¾ªç¯ (Loop) çš„ç»†ç²’åº¦æ§åˆ¶ã€‚å®ƒå¯ä»¥è®©ä½ ï¼š\n",
    "- åœ¨æ¨¡å‹è°ƒç”¨å‰åæ£€æŸ¥çŠ¶æ€\n",
    "- åŠ¨æ€ä¿®æ”¹æ¨¡å‹è¯·æ±‚\n",
    "- åœ¨æ‰§è¡Œçš„å…³é”®ç‚¹æ·»åŠ è‡ªå®šä¹‰é€»è¾‘\n",
    "\n",
    "### Agent å¾ªç¯ (The Agent Loop)\n",
    "\n",
    "```\n",
    "Input --> [before_model] --> [wrap_model_call] --> Model --> [after_model] --> Tools --> ...\n",
    "```\n",
    "\n",
    "Middleware ä¼šæŒ‚è½½åˆ°è¿™ä¸ªå¾ªç¯ä¸­ï¼š\n",
    "- **`before_model`** - åœ¨æ¨¡å‹æ‰§è¡Œå‰è¿è¡Œï¼Œå¯ä»¥æ›´æ–°çŠ¶æ€\n",
    "- **`wrap_model_call`** - åŒ…è£…æ¨¡å‹è°ƒç”¨ï¼Œæ§åˆ¶æ¨¡å‹ä½•æ—¶/å¦‚ä½•è¢«è°ƒç”¨ï¼ˆä¾‹å¦‚é‡è¯•ã€ç¼“å­˜ï¼‰\n",
    "- **`after_model`** - åœ¨æ¨¡å‹æ‰§è¡Œåï¼Œæ‰§è¡Œå·¥å…·å‰è¿è¡Œ\n",
    "\n",
    "### ä¸¤ç§ Hook é£æ ¼\n",
    "\n",
    "**Node-style hooks (èŠ‚ç‚¹å¼é’©å­)** æŒ‰é¡ºåºè¿è¡Œï¼š\n",
    "- `before_agent`, `before_model`, `after_model`, `after_agent`\n",
    "- é€‚åˆç”¨äºæ—¥å¿—è®°å½•ã€éªŒè¯ã€çŠ¶æ€æ›´æ–°\n",
    "\n",
    "**Wrap-style hooks (åŒ…è£…å¼é’©å­)** æ‹¦æˆªæ‰§è¡Œï¼š\n",
    "- `wrap_model_call`, `wrap_tool_call`\n",
    "- å®Œå…¨æ§åˆ¶å¤„ç†ç¨‹åºçš„è°ƒç”¨\n",
    "- é€‚åˆç”¨äºé‡è¯•é€»è¾‘ã€ç¼“å­˜ã€æ•°æ®è½¬æ¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹ 1: åŠ¨æ€ç³»ç»Ÿæç¤ºè¯ (Dynamic System Prompt)\n",
    "\n",
    "è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªä¸­é—´ä»¶ï¼Œæ ¹æ®ç”¨æˆ·çš„è§’è‰²åŠ¨æ€æ›´æ”¹ç³»ç»Ÿæç¤ºè¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "from typing import TypedDict\n",
    "\n",
    "# å®šä¹‰ä¸Šä¸‹æ–‡ Schema\n",
    "class Context(TypedDict):\n",
    "    user_role: str\n",
    "\n",
    "# ä½¿ç”¨è£…é¥°å™¨åˆ›å»ºä¸­é—´ä»¶\n",
    "@dynamic_prompt\n",
    "def dynamic_prompt_middleware(request: ModelRequest) -> str:\n",
    "    \"\"\"æ ¹æ®ç”¨æˆ·è§’è‰²è°ƒæ•´ç³»ç»Ÿæç¤ºè¯ã€‚\"\"\"\n",
    "    \n",
    "    # ä»è¿è¡Œæ—¶ä¸Šä¸‹æ–‡ä¸­è·å– user_role\n",
    "    user_role = request.runtime.context.get(\"user_role\", \"general\")\n",
    "    \n",
    "    if user_role == \"expert\":\n",
    "        return \"ä½ æ˜¯ä¸€åé¢å‘ä¸“å®¶çš„ AI åŠ©æ‰‹ã€‚è¯·æä¾›åŒ…å«ä»£ç ç¤ºä¾‹çš„è¯¦ç»†æŠ€æœ¯è§£ç­”ã€‚\"\n",
    "    elif user_role == \"beginner\":\n",
    "        return \"ä½ æ˜¯ä¸€åé¢å‘åˆå­¦è€…çš„ AI åŠ©æ‰‹ã€‚è¯·é€šä¿—æ˜“æ‡‚åœ°è§£é‡Šæ¦‚å¿µï¼Œé¿å…ä½¿ç”¨è¡Œè¯ã€‚\"\n",
    "    else:\n",
    "        return \"ä½ æ˜¯ä¸€åä¹äºåŠ©äººçš„ AI åŠ©æ‰‹ã€‚\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def explain_concept(concept: str) -> str:\n",
    "    \"\"\"è§£é‡Šä¸€ä¸ªç¼–ç¨‹æ¦‚å¿µã€‚\"\"\"\n",
    "    explanations = {\n",
    "        \"async\": \"å¼‚æ­¥ç¼–ç¨‹å…è®¸ä»£ç åœ¨ä¸é˜»å¡çš„æƒ…å†µä¸‹è¿è¡Œã€‚\",\n",
    "        \"recursion\": \"é€’å½’æ˜¯æŒ‡å‡½æ•°è°ƒç”¨è‡ªèº«çš„æƒ…å†µã€‚\"\n",
    "    }\n",
    "    return explanations.get(concept.lower(), \"æœªæ‰¾åˆ°è¯¥æ¦‚å¿µã€‚\")\n",
    "\n",
    "# åˆ›å»ºå¸¦æœ‰ä¸­é—´ä»¶çš„ Agent\n",
    "agent_with_middleware = create_agent(\n",
    "    model=model,\n",
    "    tools=[explain_concept],\n",
    "    middleware=[dynamic_prompt_middleware],\n",
    "    context_schema=Context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æµ‹è¯•ä¸åŒçš„ç”¨æˆ·è§’è‰²\n",
    "\n",
    "è®©æˆ‘ä»¬çœ‹çœ‹ Agent å¦‚ä½•æ ¹æ®ç”¨æˆ·è§’è‰²åšå‡ºä¸åŒçš„ååº”ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ä¸“å®¶ç”¨æˆ·\n",
      "==================================================\n",
      "å¼‚æ­¥ç¼–ç¨‹æ˜¯ä¸€ç§å…è®¸ç¨‹åºåœ¨ç­‰å¾…æŸäº›è€—æ—¶æ“ä½œï¼ˆå¦‚ç½‘ç»œè¯·æ±‚ã€ç£ç›˜ I/O æˆ–è®¡æ—¶å™¨ä»»åŠ¡ï¼‰å®Œæˆæ—¶ï¼Œç»§ç»­æ‰§è¡Œå…¶ä»–ä»»åŠ¡çš„ç¼–ç¨‹æ¨¡å¼ã€‚è¿™ç§æ–¹å¼å¯ä»¥æ˜¾è‘—æé«˜ç¨‹åºçš„å“åº”æ€§å’Œèµ„æºåˆ©ç”¨ç‡ï¼Œé¿å…ç¨‹åºé•¿æ—¶é—´é˜»å¡åœ¨å•ä¸ªæ“ä½œä¸Šã€‚\n",
      "\n",
      "ä¸‹é¢è¯¦ç»†ä»‹ç»å‡ ç§å¸¸è§çš„å¼‚æ­¥ç¼–ç¨‹æŠ€æœ¯åŠå…¶ç¤ºä¾‹ä»£ç ï¼š\n",
      "\n",
      "1. å›è°ƒå‡½æ•°ï¼ˆCallback Functionï¼‰  \n",
      "   å›è°ƒå‡½æ•°æ˜¯ä¸€ç§æœ€åŸºæœ¬çš„å¼‚æ­¥ç¼–ç¨‹æ‰‹æ®µã€‚ä½ å°†ä¸€ä¸ªå‡½æ•°ä½œä¸ºå‚æ•°ä¼ é€’ç»™å¼‚æ­¥æ“ä½œï¼Œå½“æ“ä½œå®Œæˆåï¼Œè¿™ä¸ªå‡½æ•°ä¼šè¢«è°ƒç”¨ã€‚ä½†å½“å‡ºç°å±‚å±‚åµŒå¥—çš„å›è°ƒæ—¶ï¼ˆæ‰€è°“çš„â€œå›è°ƒåœ°ç‹±â€ï¼‰ï¼Œä»£ç çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§ä¼šä¸‹é™ã€‚\n",
      "\n",
      "   ç¤ºä¾‹ä»£ç ï¼ˆä½¿ç”¨ setTimeout æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œï¼‰ï¼š\n",
      "   -------------------------------------------------\n",
      "   function fetchData(callback) {\n",
      "     console.log('å¼€å§‹è·å–æ•°æ®...');\n",
      "     setTimeout(() => {\n",
      "       const data = { id: 1, message: 'Hello, Callback!' };\n",
      "       callback(null, data);\n",
      "     }, 2000);  // æ¨¡æ‹Ÿ2ç§’å»¶è¿Ÿ\n",
      "   }\n",
      "   \n",
      "   fetchData((err, data) => {\n",
      "     if (err) {\n",
      "       console.error('å‘ç”Ÿé”™è¯¯:', err);\n",
      "     } else {\n",
      "       console.log('æ•°æ®è·å–æˆåŠŸ:', data);\n",
      "     }\n",
      "   });\n",
      "   -------------------------------------------------\n",
      "\n",
      "2. Promise  \n",
      "   Promise æ˜¯ES6å¼•å…¥çš„ä¸€ä¸ªå¯¹è±¡ï¼Œç”¨äºè¡¨ç¤ºä¸€ä¸ªå¼‚æ­¥æ“ä½œçš„æœ€ç»ˆå®Œæˆï¼ˆæˆ–å¤±è´¥ï¼‰åŠå…¶ç»“æœã€‚é€šè¿‡é“¾å¼è°ƒç”¨.then()å’Œ.catch()æ–¹æ³•ï¼Œå¯ä»¥ä½¿å¼‚æ­¥ä»£ç é€»è¾‘æ›´åŠ æ¸…æ™°ï¼Œé¿å…å±‚å±‚åµŒå¥—çš„é—®é¢˜ã€‚\n",
      "\n",
      "   ç¤ºä¾‹ä»£ç ï¼š\n",
      "   -------------------------------------------------\n",
      "   function fetchDataPromise() {\n",
      "     console.log('å¼€å§‹è·å–æ•°æ®...');\n",
      "     return new Promise((resolve, reject) => {\n",
      "       setTimeout(() => {\n",
      "         const data = { id: 2, message: 'Hello, Promise!' };\n",
      "         resolve(data);\n",
      "       }, 2000);\n",
      "     });\n",
      "   }\n",
      "   \n",
      "   fetchDataPromise()\n",
      "     .then(data => {\n",
      "       console.log('ä½¿ç”¨ Promise è·å–æ•°æ®æˆåŠŸ:', data);\n",
      "     })\n",
      "     .catch(err => {\n",
      "       console.error('å‘ç”Ÿé”™è¯¯:', err);\n",
      "     });\n",
      "   -------------------------------------------------\n",
      "\n",
      "3. async/await  \n",
      "   async/await æ˜¯åŸºäº Promise çš„è¯­æ³•ç³–ï¼Œä½¿å¾—å¼‚æ­¥ä»£ç çœ‹èµ·æ¥æ›´åƒåŒæ­¥ä»£ç ï¼Œä»è€Œå¤§å¤§æé«˜äº†ä»£ç çš„å¯è¯»æ€§å’Œç»´æŠ¤æ€§ã€‚ä½¿ç”¨æ—¶éœ€è¦å°†åŒ…å«å¼‚æ­¥æ“ä½œçš„å‡½æ•°å£°æ˜ä¸º asyncï¼Œå¹¶ä½¿ç”¨ await å…³é”®å­—ç­‰å¾… Promise å¯¹è±¡æ‰§è¡Œå®Œæˆã€‚\n",
      "\n",
      "   ç¤ºä¾‹ä»£ç ï¼š\n",
      "   -------------------------------------------------\n",
      "   async function getData() {\n",
      "     try {\n",
      "       console.log('å¼€å§‹è·å–æ•°æ® (async/await)...');\n",
      "       const data = await fetchDataPromise();\n",
      "       console.log('ä½¿ç”¨ async/await è·å–æ•°æ®æˆåŠŸ:', data);\n",
      "     } catch (err) {\n",
      "       console.error('å‘ç”Ÿé”™è¯¯:', err);\n",
      "     }\n",
      "   }\n",
      "   \n",
      "   getData();\n",
      "   -------------------------------------------------\n",
      "\n",
      "æ€»ç»“ï¼š  \n",
      "- å¼‚æ­¥ç¼–ç¨‹èƒ½å¤Ÿæœ‰æ•ˆåˆ©ç”¨ç­‰å¾…æœŸé—´çš„ CPU ç©ºé—²æ—¶é—´ï¼Œæå‡ç¨‹åºæ•´ä½“æ€§èƒ½ã€‚  \n",
      "- å›è°ƒå‡½æ•°é€‚ç”¨äºç®€å•çš„å¼‚æ­¥åœºæ™¯ï¼Œä½†å®¹æ˜“é€ æˆå›è°ƒåµŒå¥—é—®é¢˜ã€‚  \n",
      "- Promise é€šè¿‡é“¾å¼è°ƒç”¨å‡å°‘äº†åµŒå¥—ï¼Œä½¿å¾—é”™è¯¯å¤„ç†æ›´ç›´è§‚ã€‚  \n",
      "- async/await ä½¿å¾—å¼‚æ­¥ä»£ç ç»“æ„æ›´æ¥è¿‘åŒæ­¥ä»£ç ï¼Œä¾¿äºç†è§£å’Œç»´æŠ¤ã€‚\n",
      "\n",
      "è¿™äº›æ–¹æ³•åœ¨å¾ˆå¤šç¼–ç¨‹è¯­è¨€ä¸­éƒ½æœ‰ç±»ä¼¼çš„å®ç°ï¼Œæ¯”å¦‚ Python çš„ asyncio æ¨¡å—ï¼ŒJava çš„ CompletableFuture ç­‰ã€‚åœ¨å®é™…å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥æ ¹æ®å…·ä½“åº”ç”¨åœºæ™¯é€‰æ‹©æœ€é€‚åˆçš„å¼‚æ­¥ç¼–ç¨‹æ¨¡å¼ã€‚\n",
      "\n",
      "==================================================\n",
      "åˆå­¦è€…\n",
      "==================================================\n",
      "å¼‚æ­¥ç¼–ç¨‹æ˜¯ä¸€ç§ç¨‹åºè®¾è®¡æ–¹å¼ï¼Œå®ƒå…è®¸ç¨‹åºåœ¨ç­‰å¾…æŸä¸ªéœ€è¦è¾ƒé•¿æ—¶é—´æ‰èƒ½å®Œæˆçš„æ“ä½œï¼ˆæ¯”å¦‚è¯»å–æ–‡ä»¶æˆ–ç½‘ç»œè¯·æ±‚ï¼‰æ—¶ï¼Œä¸ä¼šé˜»å¡æ•´ä¸ªç¨‹åºçš„æ‰§è¡Œã€‚è¿™æ ·ï¼Œç¨‹åºå¯ä»¥ç»§ç»­å¤„ç†å…¶ä»–ä»»åŠ¡ï¼Œè€Œä¸æ˜¯ä¸€ç›´ç­‰å¾…ã€‚ç­‰åˆ°ç­‰å¾…çš„æ“ä½œå®Œæˆåï¼Œå†å›æ¥å¤„ç†ç›¸åº”çš„ç»“æœã€‚æ•´ä½“æ¥è¯´ï¼Œå¼‚æ­¥ç¼–ç¨‹å¯ä»¥ä½¿ç¨‹åºè¿è¡Œæ›´é«˜æ•ˆã€æ›´å“åº”ç”¨æˆ·çš„æ“ä½œï¼Œä¹Ÿå¯ä»¥æå‡ç”¨æˆ·ä½“éªŒã€‚\n"
     ]
    }
   ],
   "source": [
    "# ä¸“å®¶ç”¨æˆ·\n",
    "print(\"=\" * 50)\n",
    "print(\"ä¸“å®¶ç”¨æˆ·\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = agent_with_middleware.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"è§£é‡Šå¼‚æ­¥ç¼–ç¨‹\")]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)\n",
    "print()\n",
    "\n",
    "# åˆå­¦è€…\n",
    "print(\"=\" * 50)\n",
    "print(\"åˆå­¦è€…\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "result = agent_with_middleware.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"è§£é‡Šå¼‚æ­¥ç¼–ç¨‹\")]},\n",
    "    context={\"user_role\": \"beginner\"}\n",
    ")\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç¤ºä¾‹ 2: è‡ªå®šä¹‰ä¸­é—´ä»¶ - è¯·æ±‚æ—¥å¿—è®°å½•å™¨ (Request Logger)\n",
    "\n",
    "ä¸­é—´ä»¶å…è®¸ä½  hook è¿› Agent å¾ªç¯ï¼ŒæŸ¥çœ‹æ¯ä¸€æ­¥å‘ç”Ÿäº†ä»€ä¹ˆã€‚è¿™å¯¹äºè°ƒè¯•å’Œç†è§£ Agent çš„å·¥ä½œåŸç†éå¸¸æœ‰ç”¨ã€‚\n",
    "\n",
    "**Agent å¾ªç¯ï¼š**\n",
    "User Input --> [before_model] --> [wrap_model_call] --> Model --> [after_model] --> Tools --> ...\n",
    "\n",
    "**æˆ‘ä»¬å°†æ„å»ºä¸€ä¸ªæ—¥å¿—è®°å½•å™¨ï¼Œåœ¨ä»¥ä¸‹æ­¥éª¤æ‰“å°ä¿¡æ¯ï¼š**\n",
    "- **Before model** - å½“å‰å¯¹è¯ä¸­æœ‰å¤šå°‘æ¡æ¶ˆæ¯ï¼Ÿ\n",
    "- **Wrap model call** - æ­£åœ¨ä½¿ç”¨å“ªä¸ªæ¨¡å‹å’Œå·¥å…·ï¼Ÿ\n",
    "- **After model** - æ¨¡å‹æ˜¯è°ƒç”¨äº†å·¥å…·è¿˜æ˜¯ç»™å‡ºäº†æœ€ç»ˆç­”æ¡ˆï¼Ÿ\n",
    "\n",
    "è¿™å°±åƒæ˜¯æ·»åŠ äº†è°ƒè¯•ç”¨çš„ `print()` è¯­å¥ï¼Œä½†ä»¥ä¸€ç§å¹²å‡€ã€å¯é‡ç”¨çš„æ–¹å¼ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware, AgentState, ModelRequest, ModelResponse\n",
    "from typing import Any, Callable\n",
    "\n",
    "class RequestLoggerMiddleware(AgentMiddleware):\n",
    "    \"\"\"ç”¨äºè°ƒè¯•çš„æ¨¡å‹è¯·æ±‚æ—¥å¿—è®°å½•ä¸­é—´ä»¶ã€‚\"\"\"\n",
    "    \n",
    "    def before_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
    "        \"\"\"åœ¨æ¨¡å‹æ‰§è¡Œå‰è®°å½•æ—¥å¿—ã€‚\n",
    "        \n",
    "        Args:\n",
    "            state: å½“å‰ Agent çš„çŠ¶æ€ (åŒ…å«æ¶ˆæ¯å†å²ç­‰)\n",
    "            runtime: è¿è¡Œæ—¶ä¸Šä¸‹æ–‡\n",
    "        \"\"\"\n",
    "        message_count = len(state.get(\"messages\", []))\n",
    "        print(f\"[æ¨¡å‹å‰] æ­£åœ¨å¤„ç† {message_count} æ¡æ¶ˆæ¯\")\n",
    "        return None  # ä¸ä¿®æ”¹çŠ¶æ€\n",
    "    \n",
    "    def wrap_model_call(\n",
    "        self, \n",
    "        request: ModelRequest,\n",
    "        handler: Callable[[ModelRequest], ModelResponse]\n",
    "    ) -> ModelResponse:\n",
    "        \"\"\"åŒ…è£…æ¨¡å‹è°ƒç”¨ï¼Œè®°å½•è¯·æ±‚è¯¦æƒ…ã€‚\n",
    "        \n",
    "        Args:\n",
    "           request: å³å°†å‘é€ç»™æ¨¡å‹çš„è¯·æ±‚å¯¹è±¡\n",
    "           handler: æ‰§è¡Œå®é™…æ¨¡å‹è°ƒç”¨çš„å‡½æ•°\n",
    "        \"\"\"\n",
    "        print(f\"  [æ¨¡å‹è¯·æ±‚]\")\n",
    "        print(f\"   æ¨¡å‹: {request.model if hasattr(request, 'model') else 'é»˜è®¤'}\")\n",
    "        print(f\"   å¯ç”¨å·¥å…·: {len(request.tools) if request.tools else 0}\")\n",
    "        \n",
    "        # è°ƒç”¨å®é™…çš„æ¨¡å‹å¤„ç†ç¨‹åº\n",
    "        return handler(request)\n",
    "    \n",
    "    def after_model(self, state: AgentState, runtime) -> dict[str, Any] | None:\n",
    "        \"\"\"åœ¨æ¨¡å‹æ‰§è¡Œåè®°å½•æ—¥å¿—ã€‚\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            print(f\" [æ¨¡å‹å] æ¨¡å‹è¯·æ±‚äº† {len(last_message.tool_calls)} ä¸ªå·¥å…·è°ƒç”¨\")\n",
    "        else:\n",
    "            print(f\" [æ¨¡å‹å] æ¨¡å‹æä¾›äº†æœ€ç»ˆå“åº”\")\n",
    "        return None  # ä¸ä¿®æ”¹çŠ¶æ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºå¸¦æœ‰æ—¥å¿—ä¸­é—´ä»¶çš„ Agent\n",
    "agent_with_logger = create_agent(\n",
    "    model=model,\n",
    "    tools=[explain_concept],\n",
    "    middleware=[RequestLoggerMiddleware()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¢„æœŸç»“æœ\n",
    "\n",
    "å½“æˆ‘ä»¬è¿è¡Œå¸¦æ—¥å¿—è®°å½•å™¨çš„ Agent æ—¶ï¼Œä½ ä¼šå®æ—¶çœ‹åˆ°æ‰§è¡Œæµç¨‹ï¼š\n",
    "\n",
    "**ç¬¬ä¸€æ¬¡è¿­ä»£:**\n",
    "1. `[æ¨¡å‹å‰]` - æ˜¾ç¤ºæˆ‘ä»¬å¼€å§‹æ—¶çš„æ¶ˆæ¯æ•°é‡\n",
    "2. `[æ¨¡å‹è¯·æ±‚]` - æ˜¾ç¤ºå¯ç”¨æ¨¡å‹å’Œå·¥å…·\n",
    "3. `[æ¨¡å‹å]` - æ¨¡å‹å†³å®šè°ƒç”¨ `explain_concept` å·¥å…·\n",
    "\n",
    "**ç¬¬äºŒæ¬¡è¿­ä»£ (å·¥å…·æ‰§è¡Œå):**\n",
    "1. `[æ¨¡å‹å‰]` - ç°åœ¨æ¶ˆæ¯æ›´å¤šäº†ï¼ˆåŒ…å«å·¥å…·ç»“æœï¼‰\n",
    "2. `[æ¨¡å‹è¯·æ±‚]` -å†æ¬¡æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯\n",
    "3. `[æ¨¡å‹å]` - æ¨¡å‹ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼ˆä¸å†éœ€è¦å·¥å…·ï¼‰\n",
    "\n",
    "è¿™è®©ä½ èƒ½å¤Ÿæ·±å…¥äº†è§£ Agent çš„å†³ç­–è¿‡ç¨‹ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬è¿è¡Œå®ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "è¿è¡Œå¸¦æœ‰æ—¥å¿—è®°å½•å™¨çš„ Agent\n",
      "==================================================\n",
      "\n",
      "[æ¨¡å‹å‰] æ­£åœ¨å¤„ç† 1 æ¡æ¶ˆæ¯\n",
      "  [æ¨¡å‹è¯·æ±‚]\n",
      "   æ¨¡å‹: profile={'max_input_tokens': 200000, 'max_output_tokens': 100000, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x7f6fc47448b0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f6fc34ba860> root_client=<openai.OpenAI object at 0x7f6fc4744820> root_async_client=<openai.AsyncOpenAI object at 0x7f6ff92dc970> model_name='o3-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "   å¯ç”¨å·¥å…·: 1\n",
      " [æ¨¡å‹å] æ¨¡å‹è¯·æ±‚äº† 1 ä¸ªå·¥å…·è°ƒç”¨\n",
      "[æ¨¡å‹å‰] æ­£åœ¨å¤„ç† 3 æ¡æ¶ˆæ¯\n",
      "  [æ¨¡å‹è¯·æ±‚]\n",
      "   æ¨¡å‹: profile={'max_input_tokens': 200000, 'max_output_tokens': 100000, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True, 'structured_output': True, 'image_url_inputs': True, 'pdf_inputs': True, 'pdf_tool_message': True, 'image_tool_message': True, 'tool_choice': True} client=<openai.resources.chat.completions.completions.Completions object at 0x7f6fc47448b0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f6fc34ba860> root_client=<openai.OpenAI object at 0x7f6fc4744820> root_async_client=<openai.AsyncOpenAI object at 0x7f6ff92dc970> model_name='o3-mini' model_kwargs={} openai_api_key=SecretStr('**********')\n",
      "   å¯ç”¨å·¥å…·: 1\n",
      " [æ¨¡å‹å] æ¨¡å‹æä¾›äº†æœ€ç»ˆå“åº”\n",
      "\n",
      "==================================================\n",
      "æœ€ç»ˆå“åº”\n",
      "==================================================\n",
      "é€’å½’æ˜¯ä¸€ç§åœ¨è§£å†³é—®é¢˜æ—¶ï¼Œè®©å‡½æ•°ç›´æ¥è°ƒç”¨è‡ªèº«çš„æ–¹æ³•ã€‚å®ƒæœ‰ä¸¤ä¸ªå…³é”®éƒ¨åˆ†ï¼š\n",
      "\n",
      "1. åŸºæœ¬æƒ…å†µï¼ˆç»ˆæ­¢æ¡ä»¶ï¼‰ï¼šè¿™æ˜¯æŒ‡åœ¨ä»€ä¹ˆæƒ…å†µä¸‹ä¸å†ç»§ç»­è°ƒç”¨è‡ªèº«ï¼Œä»¥ä¿è¯é€’å½’èƒ½å¤Ÿåœæ­¢ï¼›å¦‚æœæ²¡æœ‰åˆé€‚çš„åŸºæœ¬æƒ…å†µï¼Œå°±ä¼šå¯¼è‡´æ— é™é€’å½’ï¼Œæœ€ç»ˆäº§ç”Ÿæ ˆæº¢å‡ºé”™è¯¯ã€‚\n",
      "2. é€’å½’æ­¥éª¤ï¼šåœ¨ç¡®ä¿æ¯æ¬¡è°ƒç”¨éƒ½å‘åŸºæœ¬æƒ…å†µé æ‹¢çš„å‰æä¸‹ï¼Œå°†ä¸€ä¸ªå¤§é—®é¢˜åˆ†è§£ä¸ºè¾ƒå°çš„åŒç±»é—®é¢˜ï¼Œç„¶åç”¨åŒä¸€ç®—æ³•é€’å½’æ±‚è§£è¿™äº›å­é—®é¢˜ã€‚\n",
      "\n",
      "å¸¸è§çš„é€’å½’åº”ç”¨åŒ…æ‹¬è®¡ç®—é˜¶ä¹˜ã€ç”Ÿæˆæ–æ³¢é‚£å¥‘æ•°åˆ—ã€å¤„ç†æ ‘å½¢ç»“æ„éå†ä»¥åŠä½¿ç”¨åˆ†æ²»ç­–ç•¥è§£å†³å¤æ‚é—®é¢˜ã€‚é€’å½’æ˜¯ä¸€ç§éå¸¸ç›´è§‚å’Œå¼ºå¤§çš„æ–¹æ³•ï¼Œä½†åœ¨å®é™…ä½¿ç”¨ä¸­éœ€è¦æ³¨æ„å†…å­˜å’Œæ•ˆç‡é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨é€’å½’å±‚æ¬¡è¾ƒæ·±æ—¶ã€‚\n"
     ]
    }
   ],
   "source": [
    "# è¿è¡Œå¹¶è§‚å¯Ÿæ—¥å¿—\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"è¿è¡Œå¸¦æœ‰æ—¥å¿—è®°å½•å™¨çš„ Agent\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "result = agent_with_logger.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"è§£é‡Šé€’å½’\"}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"æœ€ç»ˆå“åº”\")\n",
    "print(\"=\" * 50)\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¬¬ 4 éƒ¨åˆ†: ç»“åˆ Middleware å’Œ Human-in-the-loop\n",
    "\n",
    "è®©æˆ‘ä»¬ç»“åˆ Human-in-the-Loop å’Œ Middlewareï¼Œæ„å»ºä¸€ä¸ªç”Ÿäº§å°±ç»ªçš„ Agentï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éœ€è¦æ‰¹å‡†çš„æ•æ„Ÿå·¥å…·\n",
    "@tool\n",
    "def delete_database(database_name: str) -> str:\n",
    "    \"\"\"åˆ é™¤æ•°æ®åº“ã€‚è¿™å¾ˆå±é™©ï¼\"\"\"\n",
    "    \n",
    "    response = interrupt({\n",
    "        \"action\": \"delete_database\",\n",
    "        \"database_name\": database_name,\n",
    "        \"warning\": \"è¿™å°†æ°¸ä¹…åˆ é™¤æ•°æ®åº“ï¼\",\n",
    "        \"message\": \"ä½ ç¡®å®šè¦è¿™æ ·åšå—ï¼Ÿ\"\n",
    "    })\n",
    "    \n",
    "    if response.get(\"confirmed\"):\n",
    "        return f\"æ•°æ®åº“ '{database_name}' å·²è¢«åˆ é™¤ï¼ˆæ¨¡æ‹Ÿï¼‰\"\n",
    "    else:\n",
    "        return \"æ•°æ®åº“åˆ é™¤å·²å–æ¶ˆ\"\n",
    "\n",
    "# ç”¨äºè·Ÿè¸ªå±é™©æ“ä½œçš„ä¸­é—´ä»¶\n",
    "class SafetyMiddleware(AgentMiddleware):\n",
    "    \"\"\"æ·»åŠ å®‰å…¨æ£€æŸ¥å’Œæ—¥å¿—è®°å½•çš„ä¸­é—´ä»¶ã€‚\"\"\"\n",
    "    \n",
    "    name = \"safety_checker\"\n",
    "    \n",
    "    def after_model(self, state: AgentState) -> dict[str, Any] | None:\n",
    "        \"\"\"æ£€æŸ¥å±é™©çš„å·¥å…·è°ƒç”¨ã€‚\"\"\"\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        \n",
    "        if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "            for tool_call in last_message.tool_calls:\n",
    "                if \"delete\" in tool_call[\"name\"].lower():\n",
    "                    print(\"   [å®‰å…¨] æ£€æµ‹åˆ°å±é™©æ“ä½œï¼\")\n",
    "                    print(f\"   å·¥å…·: {tool_call['name']}\")\n",
    "                    print(f\"   å‚æ•°: {tool_call['args']}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# åˆ›å»ºç”Ÿäº§çº§ Agent\n",
    "production_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[delete_database],\n",
    "    middleware=[SafetyMiddleware()],\n",
    "    checkpointer=MemorySaver()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### é¢„æœŸç»“æœï¼šå¤šå±‚å®‰å…¨æœºåˆ¶å®æˆ˜\n",
    "\n",
    "  å½“æˆ‘ä»¬å°è¯•ä¸€ä¸ªå±é™©æ“ä½œæ—¶ï¼Œä½ ä¼šçœ‹åˆ° **ä¸¤ç§** å®‰å…¨æœºåˆ¶è¢«æ¿€æ´»ï¼š\n",
    "\n",
    "  **ç¬¬ 1 å±‚ - Middleware æ£€æµ‹:**\n",
    "  - `[å®‰å…¨] æ£€æµ‹åˆ°å±é™©æ“ä½œï¼` - ä¸­é—´ä»¶å‘ç° delete æ“ä½œ\n",
    "  - è®°å½•å·¥å…·åç§°å’Œå‚æ•°ä»¥ç”¨äºå®¡è®¡è¿½è¸ª\n",
    "\n",
    "  **ç¬¬ 2 å±‚ - äººå·¥æ‰¹å‡† (Interrupt):**\n",
    "  - Agent æ‰§è¡Œåœ¨ `interrupt()` å¤„æš‚åœ\n",
    "  - å‘å®¡æ ¸äººå‘˜å±•ç¤ºè­¦å‘Šä¿¡æ¯\n",
    "  - é™¤éè·å¾—æ˜ç¡®æ‰¹å‡†ï¼Œå¦åˆ™æ‰§è¡Œä¸ä¼šç»§ç»­\n",
    "\n",
    "  **è¿™å°±æ˜¯çºµæ·±é˜²å¾¡ï¼š** Middleware ç›‘æ§æ‰€æœ‰æ“ä½œï¼Œè€Œ Interrupts å¼ºåˆ¶å¯¹å…³é”®è¡ŒåŠ¨è¿›è¡Œäººå·¥æ‰¹å‡†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "å°è¯•å±é™©æ“ä½œ\n",
      "==================================================\n",
      "\n",
      "   [å®‰å…¨] æ£€æµ‹åˆ°å±é™©æ“ä½œï¼\n",
      "   å·¥å…·: delete_database\n",
      "   å‚æ•°: {'database_name': 'production_db'}\n",
      "\n",
      "  éœ€è¦äººå·¥æ‰¹å‡†ï¼š\n",
      "   è¿™å°†æ°¸ä¹…åˆ é™¤æ•°æ®åº“ï¼\n",
      "   Database: production_db\n",
      "\n",
      "(åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œäººç±»ä¼šåœ¨ç»§ç»­ä¹‹å‰å®¡æŸ¥æ­¤æ“ä½œ)\n"
     ]
    }
   ],
   "source": [
    "# æµ‹è¯•ç»„åˆæ¨¡å¼\n",
    "config_4 = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"å°è¯•å±é™©æ“ä½œ\")\n",
    "print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "# è¿è¡Œç›´åˆ° interrupt\n",
    "result = production_agent.invoke(\n",
    "    {\n",
    "        \"messages\": [HumanMessage(content=\"åˆ é™¤ production_db æ•°æ®åº“\")]\n",
    "    },\n",
    "    config=config_4\n",
    ")\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    interrupt_info = result[\"__interrupt__\"][0]\n",
    "    print(\"\\n  éœ€è¦äººå·¥æ‰¹å‡†ï¼š\")\n",
    "    print(f\"   {interrupt_info.value['warning']}\")\n",
    "    print(f\"   Database: {interrupt_info.value['database_name']}\")\n",
    "\n",
    "print(\"\\n(åœ¨çœŸå®åº”ç”¨ä¸­ï¼Œäººç±»ä¼šåœ¨ç»§ç»­ä¹‹å‰å®¡æŸ¥æ­¤æ“ä½œ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ ¸å¿ƒè¦ç‚¹\n",
    "\n",
    "### Human-in-the-Loop (Interrupts)\n",
    "- ä½¿ç”¨ `interrupt()` æš‚åœæ‰§è¡Œ\n",
    "- å¿…é¡»é…åˆ `checkpointer` å®ç°æŒä¹…åŒ–\n",
    "- ä½¿ç”¨ `Command(resume=value)` æ¢å¤æ‰§è¡Œ\n",
    "- éå¸¸é€‚åˆå®¡æ‰¹å·¥ä½œæµå’Œæ•æ„Ÿæ“ä½œ\n",
    "\n",
    "### Middleware\n",
    "- **Node-style hooks**: `before_model`, `after_model` - é¡ºåºé€»è¾‘ã€éªŒè¯ã€æ—¥å¿—\n",
    "- **Wrap-style hooks**: `wrap_model_call`, `wrap_tool_call` - å®Œå…¨æ§åˆ¶ã€é‡è¯•ã€è½¬æ¢\n",
    "- **Decorators**: `@dynamic_prompt`, `@before_model`, `@wrap_model_call` ç”¨äºå¿«é€Ÿå®šä¹‰ä¸­é—´ä»¶\n",
    "- **Classes**: ç»§æ‰¿ `AgentMiddleware` ä»¥æ„å»ºå¤æ‚ã€å¯é‡ç”¨çš„ç»„ä»¶\n",
    "\n",
    "### ä½•æ—¶ä½¿ç”¨ä»€ä¹ˆï¼Ÿ\n",
    "\n",
    "**ä½¿ç”¨ Interrupts å½“ï¼š**\n",
    "- åŠ¨ä½œéœ€è¦äººå·¥æ‰¹å‡†\n",
    "- ä½ æƒ³å®¡æŸ¥/ç¼–è¾‘å·¥å…·è°ƒç”¨\n",
    "- éœ€è¦éªŒè¯ç”¨æˆ·è¾“å…¥\n",
    "\n",
    "**ä½¿ç”¨ Middleware å½“ï¼š**\n",
    "- éœ€è¦åŠ¨æ€ä¿®æ”¹ Agent è¡Œä¸º\n",
    "- æƒ³è¦æ·»åŠ æ—¥å¿—/ç›‘æ§\n",
    "- éœ€è¦æ‰§è¡Œç­–ç•¥ï¼ˆToken é™åˆ¶ã€å®‰å…¨æ£€æŸ¥ï¼‰\n",
    "- æƒ³è¦æ ¹æ®ä¸Šä¸‹æ–‡ä¸ªæ€§åŒ–å“åº”\n",
    "\n",
    "**Node-style vs Wrap-style:**\n",
    "- Node-style ç”¨äºé¡ºåºæ“ä½œï¼ˆæ—¥å¿—ã€éªŒè¯ï¼‰\n",
    "- Wrap-style ç”¨äºæ§åˆ¶æµï¼ˆé‡è¯•ã€å›é€€ã€ç¼“å­˜ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç»ƒä¹  (å¯é€‰)\n",
    "\n",
    "å°è¯•æ„å»ºä¸€ä¸ª Agentï¼š\n",
    "1. æ‹¥æœ‰ä¸€ä¸ªè¿›è¡Œè´­ä¹°æ“ä½œçš„å·¥å…·\n",
    "2. ä½¿ç”¨ä¸­é—´ä»¶æ£€æŸ¥è´­ä¹°é‡‘é¢æ˜¯å¦è¶…è¿‡ `$1000`\n",
    "3. å¦‚æœè¶…è¿‡ `$1000`ï¼Œä½¿ç”¨ interrupt è¦æ±‚æ‰¹å‡†\n",
    "4. å¦‚æœä½äº `$1000`ï¼Œè‡ªåŠ¨å¤„ç†\n",
    "\n",
    "æç¤ºï¼šç»“åˆ `before_model` ä¸­é—´ä»¶å’Œæ¡ä»¶æ€§çš„ `interrupt()` é€»è¾‘ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½ çš„ä»£ç ï¼\n",
    "# æŒ‘æˆ˜ï¼šæ„å»ºè´­ä¹°å®¡æ‰¹ Agent\n",
    "\n",
    "# @tool\n",
    "# def make_purchase(item: str, amount: float) -> str:\n",
    "#     ...\n",
    "#\n",
    "# class PurchaseLimitMiddleware(AgentMiddleware):\n",
    "#     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "æµ‹è¯•åœºæ™¯ 1: å°é¢è´­ä¹° ($300) -> åº”è‡ªåŠ¨é€šè¿‡\n",
      "==================================================\n",
      "Agent å›å¤: è´­ä¹°æˆåŠŸï¼Œæ‚¨çš„æœºæ¢°é”®ç›˜è®¢å•å·²å®Œæˆæ”¯ä»˜ $300.0ã€‚\n",
      "\n",
      "==================================================\n",
      "æµ‹è¯•åœºæ™¯ 2: å¤§é¢è´­ä¹° ($2500) -> åº”æš‚åœç­‰å¾…æ‰¹å‡†\n",
      "==================================================\n",
      ">>> æ­¥éª¤ 1: å‘èµ·è¯·æ±‚\n",
      "\n",
      "[ä¸­é—´ä»¶] âš ï¸  é£æ§è§¦å‘ï¼š'æ–°æœåŠ¡å™¨' é‡‘é¢ $2500 è¶…è¿‡è‡ªåŠ¨æ‰¹å‡†é™é¢ ($1000)ã€‚\n",
      "\n",
      "[ç³»ç»Ÿ] Agent å·²æš‚åœï¼Œç­‰å¾…äººå·¥æŒ‡ä»¤...\n",
      "è¯¦æƒ…: ç”³è¯·è´­ä¹° 'æ–°æœåŠ¡å™¨'ï¼Œé‡‘é¢ $2500ã€‚æ˜¯å¦æ‰¹å‡†ï¼Ÿ\n",
      "\n",
      ">>> æ­¥éª¤ 2: æ¨¡æ‹Ÿäººå·¥æ‰¹å‡† (Resume)\n",
      "\n",
      "[ä¸­é—´ä»¶] âš ï¸  é£æ§è§¦å‘ï¼š'æ–°æœåŠ¡å™¨' é‡‘é¢ $2500 è¶…è¿‡è‡ªåŠ¨æ‰¹å‡†é™é¢ ($1000)ã€‚\n",
      "[ä¸­é—´ä»¶] âœ… è´­ä¹°ç”³è¯·å·²è·æ‰¹å‡†ï¼Œç»§ç»­æ‰§è¡Œ...\n",
      "Agent æœ€ç»ˆå›å¤: è®¢å•å·²æˆåŠŸå®Œæˆã€‚æœåŠ¡å™¨é‡‡è´­å·²æ”¯ä»˜ $2500.0ã€‚è¯·çŸ¥æ‚‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents.middleware import AgentMiddleware\n",
    "from typing import Any, Callable\n",
    "from langgraph.types import interrupt, Command\n",
    "from langchain_core.tools import tool\n",
    "from langchain.messages import HumanMessage\n",
    "from langsmith import uuid7\n",
    "\n",
    "# ==========================================\n",
    "# 1. å®šä¹‰è´­ä¹°å·¥å…·\n",
    "# ==========================================\n",
    "@tool\n",
    "def make_purchase(item: str, amount: float) -> str:\n",
    "    \"\"\"\n",
    "    æ‰§è¡Œè´­ä¹°æ“ä½œçš„å·¥å…·ã€‚\n",
    "    \"\"\"\n",
    "    return f\"æ”¯ä»˜æˆåŠŸï¼šå·²ä¸º '{item}' æ”¯ä»˜ ${amount}ã€‚\"\n",
    "\n",
    "# ==========================================\n",
    "# 2. å®šä¹‰è´­ä¹°é™åˆ¶ä¸­é—´ä»¶ (ä¿®æ­£ç‰ˆ)\n",
    "# ==========================================\n",
    "class PurchaseLimitMiddleware(AgentMiddleware):\n",
    "    \"\"\"\n",
    "    è´­ä¹°é£æ§ä¸­é—´ä»¶ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    def wrap_tool_call(self, request: Any, handler: Callable[[Any], Any]) -> Any:\n",
    "        # ä¿®æ­£ç‚¹ï¼šä» request å¯¹è±¡ä¸­è§£æå·¥å…·è°ƒç”¨ä¿¡æ¯\n",
    "        # request.tool_call æ˜¯ä¸€ä¸ªåŒ…å« name, args ç­‰ä¿¡æ¯çš„å­—å…¸\n",
    "        tool_call = getattr(request, \"tool_call\", {})\n",
    "        tool_name = tool_call.get(\"name\")\n",
    "        tool_args = tool_call.get(\"args\", {})\n",
    "        \n",
    "        # æå–å‚æ•°\n",
    "        amount = tool_args.get(\"amount\", 0)\n",
    "        item = tool_args.get(\"item\", \"æœªçŸ¥å•†å“\")\n",
    "\n",
    "        # æ£€æŸ¥é€»è¾‘ï¼šå¿…é¡»æ˜¯ make_purchase å·¥å…·ï¼Œä¸”é‡‘é¢è¶…è¿‡ 1000\n",
    "        if tool_name == \"make_purchase\" and amount > 1000:\n",
    "            print(f\"\\n[ä¸­é—´ä»¶] âš ï¸  é£æ§è§¦å‘ï¼š'{item}' é‡‘é¢ ${amount} è¶…è¿‡è‡ªåŠ¨æ‰¹å‡†é™é¢ ($1000)ã€‚\")\n",
    "            \n",
    "            # è§¦å‘ä¸­æ–­\n",
    "            approval_result = interrupt({\n",
    "                \"type\": \"purchase_approval\",\n",
    "                \"item\": item,\n",
    "                \"amount\": amount,\n",
    "                \"reason\": \"é‡‘é¢è¶…é™\",\n",
    "                \"message\": f\"ç”³è¯·è´­ä¹° '{item}'ï¼Œé‡‘é¢ ${amount}ã€‚æ˜¯å¦æ‰¹å‡†ï¼Ÿ\"\n",
    "            })\n",
    "            \n",
    "            # æ£€æŸ¥äººå·¥å®¡æ ¸ç»“æœ\n",
    "            if not approval_result.get(\"approved\"):\n",
    "                print(\"[ä¸­é—´ä»¶] âŒ è´­ä¹°ç”³è¯·è¢«äººå·¥æ‹’ç»ã€‚\")\n",
    "                return f\"äº¤æ˜“å–æ¶ˆï¼šç®¡ç†å‘˜æ‹’ç»äº†å¯¹ '{item}' çš„ ${amount} è´­ä¹°ç”³è¯·ã€‚\"\n",
    "            \n",
    "            print(\"[ä¸­é—´ä»¶] âœ… è´­ä¹°ç”³è¯·å·²è·æ‰¹å‡†ï¼Œç»§ç»­æ‰§è¡Œ...\")\n",
    "        \n",
    "        # æ‰§è¡ŒåŸå§‹å·¥å…·è°ƒç”¨\n",
    "        return handler(request)\n",
    "\n",
    "# ==========================================\n",
    "# 3. åˆ›å»º Agent\n",
    "# ==========================================\n",
    "purchase_agent = create_agent(\n",
    "    model=model,\n",
    "    tools=[make_purchase],\n",
    "    middleware=[PurchaseLimitMiddleware()],\n",
    "    checkpointer=MemorySaver(),\n",
    "    system_prompt=\"ä½ æ˜¯ä¸€ä¸ªé‡‡è´­åŠ©æ‰‹ã€‚æ”¶åˆ°è´­ä¹°è¯·æ±‚æ—¶ï¼Œè¯·è°ƒç”¨ make_purchase å·¥å…·ã€‚\"\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 4. éªŒè¯ä¸æµ‹è¯•\n",
    "# ==========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"æµ‹è¯•åœºæ™¯ 1: å°é¢è´­ä¹° ($300) -> åº”è‡ªåŠ¨é€šè¿‡\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "thread_config_1 = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "response_1 = purchase_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"æˆ‘è¦ä¹°ä¸€ä¸ªæœºæ¢°é”®ç›˜ï¼Œä»·æ ¼ 300 ç¾å…ƒ\")]},\n",
    "    config=thread_config_1\n",
    ")\n",
    "print(f\"Agent å›å¤: {response_1['messages'][-1].content}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"æµ‹è¯•åœºæ™¯ 2: å¤§é¢è´­ä¹° ($2500) -> åº”æš‚åœç­‰å¾…æ‰¹å‡†\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "thread_config_2 = {\"configurable\": {\"thread_id\": uuid7()}}\n",
    "\n",
    "# ç¬¬ä¸€æ¬¡è¿è¡Œï¼šè§¦å‘ä¸­æ–­\n",
    "print(\">>> æ­¥éª¤ 1: å‘èµ·è¯·æ±‚\")\n",
    "response_2 = purchase_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"å…¬å¸éœ€è¦é‡‡è´­ä¸€å°æ–°çš„æœåŠ¡å™¨ï¼Œä»·æ ¼ 2500 ç¾å…ƒ\")]},\n",
    "    config=thread_config_2\n",
    ")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦ä¸­æ–­\n",
    "if \"__interrupt__\" in response_2:\n",
    "    interrupt_info = response_2[\"__interrupt__\"][0].value\n",
    "    print(f\"\\n[ç³»ç»Ÿ] Agent å·²æš‚åœï¼Œç­‰å¾…äººå·¥æŒ‡ä»¤...\")\n",
    "    print(f\"è¯¦æƒ…: {interrupt_info['message']}\")\n",
    "    \n",
    "    print(\"\\n>>> æ­¥éª¤ 2: æ¨¡æ‹Ÿäººå·¥æ‰¹å‡† (Resume)\")\n",
    "    final_response = purchase_agent.invoke(\n",
    "        Command(resume={\"approved\": True}),\n",
    "        config=thread_config_2\n",
    "    )\n",
    "    print(f\"Agent æœ€ç»ˆå›å¤: {final_response['messages'][-1].content}\")\n",
    "else:\n",
    "    print(\"é”™è¯¯ï¼šå³é¢„æœŸåº”è¯¥æš‚åœï¼Œä½† Agent ç›´æ¥æ‰§è¡Œå®Œæˆäº†ã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‹ä¸€æ­¥\n",
    "\n",
    "ä½ ç°åœ¨å·²ç»æ‹¥æœ‰äº†æ„å»ºç”Ÿäº§çº§ Agent çš„å¼ºå¤§å·¥å…·ï¼\n",
    "\n",
    "**ç»§ç»­ä½ çš„æ—…ç¨‹ï¼š**\n",
    "1.  æŸ¥çœ‹ `multi_agent.ipynb` äº†è§£å¤š Agent ç³»ç»Ÿ\n",
    "2.  æ¢ç´¢å†…ç½®ä¸­é—´ä»¶ï¼ˆæ‘˜è¦ Summarizationã€Anthropic Prompt Cachingï¼‰\n",
    "3.  ä¸ºä½ è‡ªå·±çš„ç”¨ä¾‹æ„å»ºè‡ªå®šä¹‰ä¸­é—´ä»¶\n",
    "4.  æ·»åŠ  LangSmith è¿›è¡Œè°ƒè¯•å’Œç›‘æ§\n",
    "\n",
    "**èµ„æºï¼š**\n",
    "- [Middleware æ–‡æ¡£](https://docs.langchain.com/oss/python/langchain/middleware)\n",
    "- [Human-in-the-Loop äº¤äº’æŒ‡å—](https://docs.langchain.com/oss/python/langchain/human-in-the-loop)\n",
    "- [LangGraph æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)\n",
    "\n",
    "</br>\n",
    "</br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langchain1.x)",
   "language": "python",
   "name": "langchain1.x"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
