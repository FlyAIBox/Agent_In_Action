{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d0d1bad",
   "metadata": {},
   "source": [
    "### 🔧 环境配置和检查\n",
    "\n",
    "#### 概述\n",
    "\n",
    "本教程需要特定的环境配置以确保最佳学习体验。以下配置将帮助你：\n",
    "\n",
    "- 使用统一的conda环境：激活统一的学习环境\n",
    "- 通过国内镜像源快速安装依赖：配置pip使用清华镜像源\n",
    "- 加速模型下载：设置HuggingFace镜像代理\n",
    "- 检查系统配置：检查硬件和软件配置\n",
    "\n",
    "#### 配置\n",
    "\n",
    "- **所需环境及其依赖已经部署好**\n",
    "- 在`Notebook`右上角选择`jupyter内核`为`python(agent101)`，即可执行下方代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bfe28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda 环境检查报告 (仅针对当前 Bash 子进��) ==\n",
      "=========================================\n",
      "✅ 当前单元格已成功激活到 agent101 环境。\n",
      "✅ 正在使用的环境路径: /root/miniconda3/envs/agent101\n",
      "\n",
      "💡 提示: 后续的Python单元格将使用Notebook当前选择的Jupyter��核。\n",
      "   如果需要后续单元格也使用此环境，请执行以下操作:\n",
      "   1. 检查 Notebook 右上角是否已选择 'python(agent101)'。\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. 激活 conda 环境 (仅对当前单元格有效)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate agent101\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda 环境检查报告 (仅针对当前 Bash 子进程) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. 检查当前激活的环境\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"agent101\" ]; then\n",
    "    echo \"✅ 当前单元格已成功激活到 agent101 环境。\"\n",
    "    echo \"✅ 正在使用的环境路径: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"💡 提示: 后续的Python单元格将使用Notebook当前选择的Jupyter内核。\"\n",
    "    echo \"   如果需要后续单元格也使用此环境，请执行以下操作:\"\n",
    "    echo \"   1. 检查 Notebook 右上角是否已选择 'python(agent101)'。\"\n",
    "else\n",
    "    echo \"❌ 激活失败或环境名称不匹配。当前环境: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"⚠️ 严重提示: 建议将 Notebook 的 Jupyter **内核 (Kernel)** 切换为 'python(agent101)'。\"\n",
    "    echo \"   (通常位于 Notebook 右上角或 '内核' 菜单中)\"\n",
    "    echo \"\"\n",
    "    echo \"📚 备用方法 (不推荐): 如果无法切换内核，则必须在**每个**代码单元格的头部重复以下命令:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# 必须在每个单元格都执行\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate agent101\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "098df42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For variant 'global', will try loading '/etc/xdg/pip/pip.conf'\n",
      "For variant 'global', will try loading '/etc/pip.conf'\n",
      "For variant 'user', will try loading '/root/.pip/pip.conf'\n",
      "For variant 'user', will try loading '/root/.config/pip/pip.conf'\n",
      "For variant 'site', will try loading '/root/miniconda3/envs/agent101/pip.conf'\n",
      "\u001b[31mERROR: Got unexpected number of arguments, expected 0. (example: \"/root/miniconda3/envs/agent101/bin/python -m pip config list\")\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "For variant 'global', will try loading '/etc/xdg/pip/pip.conf'\n",
      "For variant 'global', will try loading '/etc/pip.conf'\n",
      "For variant 'user', will try loading '/root/.pip/pip.conf'\n",
      "For variant 'user', will try loading '/root/.config/pip/pip.conf'\n",
      "For variant 'site', will try loading '/root/miniconda3/envs/agent101/pip.conf'\n",
      "\u001b[31mERROR: Got unexpected number of arguments, expected 0. (example: \"/root/miniconda3/envs/agent101/bin/python -m pip config list\")\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. 设置pip 为清华源\n",
    "%pip config list -v set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list -v list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1406a3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. 设置HuggingFace代理\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# 验证：使用shell命令检查\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194a11bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==2.2.2\n",
      "  Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "Using cached pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.3.1\n",
      "    Uninstalling pandas-2.3.1:\n",
      "      Successfully uninstalled pandas-2.3.1\n",
      "Successfully installed pandas-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### 环境信息\n",
      "| 项目         | 信息                                                                               |\n",
      "|:-------------|:-----------------------------------------------------------------------------------|\n",
      "| 操作系统     | Linux Ubuntu 22.04.4 LTS                                                           |\n",
      "| CPU 信息     | 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (1 physical cores, 4 logical cores) |\n",
      "| 内存信息     | 5.75 GB (Available: 2.92 GB)                                                       |\n",
      "| GPU 信息     | No GPU found (nvidia-smi not found)                                                |\n",
      "| CUDA 信息    | CUDA not found                                                                     |\n",
      "| Python 版本  | 3.10.18                                                                            |\n",
      "| Conda 版本   | conda 24.4.0                                                                       |\n",
      "| 物理磁盘空间 | Total: 145.49 GB, Used: 20.19 GB, Free: 119.08 GB                                  |\n"
     ]
    }
   ],
   "source": [
    "# 🔍 环境信息检查脚本\n",
    "#\n",
    "# 本脚本的作用：\n",
    "# 1. 安装 pandas 库用于数据表格展示\n",
    "# 2. 检查系统的各项配置信息\n",
    "# 3. 生成详细的环境报告表格\n",
    "#\n",
    "# 对于初学者来说，这个步骤帮助你：\n",
    "# - 了解当前运行环境的硬件配置\n",
    "# - 确认是否满足模型运行的最低要求\n",
    "# - 学习如何通过代码获取系统信息\n",
    "\n",
    "# 安装 pandas 库 - 用于创建和展示数据表格\n",
    "# pandas 是 Python 中最流行的数据处理和分析库\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # 导入 platform 模块以获取系统信息\n",
    "import os # 导入 os 模块以与操作系统交互\n",
    "import subprocess # 导入 subprocess 模块以运行外部命令\n",
    "import pandas as pd # 导入 pandas 模块，通常用于数据处理，这里用于创建表格\n",
    "import shutil # 导入 shutil 模块以获取磁盘空间信息\n",
    "\n",
    "# 获取 CPU 信息的函数，包括核心数量\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # 初始化 CPU 信息字符串\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # 如果是 Windows 系统\n",
    "        cpu_info = platform.processor() # 使用 platform.processor() 获取 CPU 信息\n",
    "        try:\n",
    "            # 获取 Windows 上的核心数量 (需要 WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # 如果 WMI 不可用，忽略错误\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取 CPU 信息和核心数量\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # 更新 PATH 环境变量\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/cpuinfo 文件获取 CPU 信息和核心数量\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # 查找以 'model name'开头的行\n",
    "                        if not cpu_info: # 只获取第一个 model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # 查找以 'cpu cores' 开头的行\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # 查找以 'processor' 开头的行\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # 返回 CPU 信息和核心数量\n",
    "\n",
    "\n",
    "# 获取内存信息的函数\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # 初始化内存信息字符串\n",
    "    if platform.system() == \"Windows\":\n",
    "        # 在 Windows 上不容易通过标准库获取，需要外部库或 PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # 设置提示信息\n",
    "    elif platform.system() == \"Darwin\": # 如果是 macOS 系统\n",
    "        # 在 macOS 上使用 sysctl 命令获取内存大小\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # 运行 sysctl 命令\n",
    "        stdout, stderr = process.communicate() # 获取标准输出和标准错误\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # 解析输出，获取内存大小（字节）\n",
    "        mem_gb = mem_bytes / (1024**3) # 转换为 GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # 格式化输出\n",
    "    else:  # Linux 系统\n",
    "        try:\n",
    "            # 在 Linux 上读取 /proc/meminfo 文件获取内存信息\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # 查找以 'MemTotal' 开头的行\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取总内存（KB）\n",
    "                    elif line.startswith('MemAvailable'): # 查找以 'MemAvailable' 开头的行\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # 解析行，获取可用内存（KB）\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # 转换为 GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # 格式化输出总内存\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # 添加可用内存信息\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # 如果读取文件出错，设置错误信息\n",
    "    return mem_info # 返回内存信息\n",
    "\n",
    "# 获取 GPU 信息的函数，包括显存\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # 尝试使用 nvidia-smi 获取 NVIDIA GPU 信息和显存\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # 解析输出，获取 GPU 名称和显存\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # 格式化 GPU 信息\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # 返回 GPU 信息或提示信息\n",
    "        else:\n",
    "             # 尝试使用 lshw 获取其他 GPU 信息 (需要安装 lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # 如果命令成功执行\n",
    "                     # 简单解析输出中的 product 名称和显存\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # 添加最后一个 GPU 的信息\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # 如果找到 GPU 但信息无法解析，设置提示信息\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # 如果两个命令都找不到 GPU，设置提示信息\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # 如果找不到 lshw 命令，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # 如果找不到 nvidia-smi 命令，设置提示信息\n",
    "\n",
    "\n",
    "# 获取 CUDA 版本的函数\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # 尝试使用 nvcc --version 获取 CUDA 版本\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # 查找包含 'release' 的行\n",
    "                    return line.split('release ')[1].split(',')[0] # 解析行，提取版本号\n",
    "        return \"CUDA not found or version not parsed\" # 如果找不到 CUDA 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # 如果找不到 nvcc 命令，设置提示信息\n",
    "\n",
    "# 获取 Python 版本的函数\n",
    "def get_python_version():\n",
    "    return platform.python_version() # 获取 Python 版本\n",
    "\n",
    "# 获取 Conda 版本的函数\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # 尝试使用 conda --version 获取 Conda 版本\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # 如果命令成功执行\n",
    "            return result.stdout.strip() # 返回 Conda 版本\n",
    "        return \"Conda not found or version not parsed\" # 如果找不到 Conda 或版本无法解析，设置提示信息\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # 如果找不到 conda 命令，设置提示信息\n",
    "\n",
    "# 获取物理磁盘空间信息的函数\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # 获取根目录的磁盘使用情况\n",
    "        total_gb = total / (1024**3) # 转换为 GB\n",
    "        used_gb = used / (1024**3) # 转换为 GB\n",
    "        free_gb = free / (1024**3) # 转换为 GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # 格式化输出\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # 如果获取信息出错，设置错误信息\n",
    "\n",
    "# 获取环境信息\n",
    "os_name = platform.system() # 获取操作系统名称\n",
    "os_version = platform.release() # 获取操作系统版本\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # 在 Linux 上尝试获取发行版和版本\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # 如果命令成功执行\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # 查找包含 'Description:' 的行\n",
    "                    os_version = line.split('Description:')[1].strip() # 提取描述信息作为版本\n",
    "                    break # 找到后退出循环\n",
    "                elif 'Release:' in line: # 查找包含 'Release:' 的行\n",
    "                     os_version = line.split('Release:')[1].strip() # 提取版本号\n",
    "                     # 尝试获取 codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # 将 codename 添加到版本信息中\n",
    "                     except:\n",
    "                         pass # 如果获取 codename 失败则忽略\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release 可能未安装，忽略错误\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # 组合完整的操作系统信息\n",
    "cpu_info = get_cpu_info() # 调用函数获取 CPU 信息和核心数量\n",
    "memory_info = get_memory_info() # 调用函数获取内存信息\n",
    "gpu_info = get_gpu_info() # 调用函数获取 GPU 信息和显存\n",
    "cuda_version = get_cuda_version() # 调用函数获取 CUDA 版本\n",
    "python_version = get_python_version() # 调用函数获取 Python 版本\n",
    "conda_version = get_conda_version() # 调用函数获取 Conda 版本\n",
    "disk_info = get_disk_space() # 调用函数获取物理磁盘空间信息\n",
    "\n",
    "\n",
    "# 创建用于存储数据的字典\n",
    "env_data = {\n",
    "    \"项目\": [ # 项目名称列表\n",
    "        \"操作系统\",\n",
    "        \"CPU 信息\",\n",
    "        \"内存信息\",\n",
    "        \"GPU 信息\",\n",
    "        \"CUDA 信息\",\n",
    "        \"Python 版本\",\n",
    "        \"Conda 版本\",\n",
    "        \"物理磁盘空间\" # 添加物理磁盘空间\n",
    "    ],\n",
    "    \"信息\": [ # 对应的信息列表\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # 添加物理磁盘空间信息\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 创建一个 pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# 打印表格\n",
    "print(\"### 环境信息\") # 打印标题\n",
    "print(df.to_markdown(index=False)) # 将 DataFrame 转换为 Markdown 格式并打印，不包含索引\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "854fa9f5-f7e2-42f8-b208-ac339e9140be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /root/miniconda3\n",
      "agent101              *  /root/miniconda3/envs/agent101\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 查看当前使用的Coda环境（带有*）\n",
    "%conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
   "metadata": {
    "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
   },
   "source": [
    "\n",
    "## LangGraph教程：LangChain\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "### **背景介绍**\n",
    "\n",
    "在 LangChain，我们的目标是让构建大型语言模型（LLM）应用变得简单。你可以构建的一种 LLM 应用就是**智能代理（Agent）**。构建智能代理非常令人兴奋，因为它们能够自动化以前不可能完成的各种任务。\n",
    "\n",
    "然而，在实践中，构建能够可靠执行这些任务的系统极其困难。在我们与用户合作将智能代理投入生产的过程中，我们发现通常需要更多的**控制**。例如，你可能需要智能代理始终优先调用某个特定的工具，或者根据其状态使用不同的提示词。\n",
    "\n",
    "为了解决这个问题，我们构建了 [**LangGraph**](https://langchain-ai.github.io/langgraph/) —— 一个用于构建智能代理和多智能体应用的框架。LangGraph 独立于 LangChain 包，其核心设计理念是帮助开发者为智能代理工作流添加更好的**精确性**和**控制力**，使其适合现实世界系统的复杂性。\n",
    "\n",
    "-----\n",
    "\n",
    "### **聊天模型**\n",
    "\n",
    "在本课程中，我们将使用[**聊天模型（Chat Models）**](https://python.langchain.com/v0.2/docs/concepts/#chat-models)，它们的功能是接收一系列消息作为输入，并以聊天消息作为输出。LangChain 本身不托管任何聊天模型，而是依赖于第三方集成。 是 LangChain 中支持的第三方聊天模型集成列表！默认情况下，课程将使用[ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/)  ，因为它既流行又性能出色。正如前面提到的，请确保你已设置好 `OPENAI_API_KEY`。\n",
    "\n",
    "我们将检查你的 `OPENAI_API_KEY` 是否已设置；如果未设置，系统会提示你输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bffd3e-2b2a-4168-a1d6-546a6e770b67",
   "metadata": {
    "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
   },
   "source": [
    "\n",
    "#### OpenAI国内代理\n",
    "\n",
    "为方便大家再国内使用OpenAI，Claude等国外模型\n",
    "\n",
    "和OpenAI国内代理API易社区争取到了如下权益：\n",
    "\n",
    "OpenAI国内代理地址\n",
    "https://api.apiyi.com/register/?aff_code=we80\n",
    "新用户注册送0.1美金，注册成功后在以下表格中填写你的账号，平台会再赠送2美金（5个工作日到账）\n",
    "\n",
    "---- 【腾讯文档】API易账号收集 https://docs.qq.com/form/page/DQm1qb1VBQU9wR2xq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f9a52c8",
   "metadata": {
    "id": "0f9a52c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的依赖包\n",
    "# %pip install 是 Jupyter 中安装 Python 包的命令\n",
    "# --quiet 参数减少输出信息\n",
    "%pip install --quiet langchain_openai==0.3.32 langchain_core==0.3.75 langchain_community==0.3.29 tavily-python==0.7.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2a15227",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2a15227",
    "outputId": "2bc8601c-50bc-4353-fa6a-3c5d5c7ef112"
   },
   "outputs": [],
   "source": [
    "# 导入必要的模块\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"\n",
    "    设置环境变量的辅助函数\n",
    "\n",
    "    参数:\n",
    "        var (str): 要设置的环境变量名称\n",
    "\n",
    "    功能:\n",
    "        - 检查环境变量是否已存在\n",
    "        - 如果不存在，则提示用户输入并设置\n",
    "    \"\"\"\n",
    "    if not os.environ.get(var):  # 检查环境变量是否已设置\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")  # 安全地获取用户输入\n",
    "\n",
    "# 设置 OpenAI API 密钥\n",
    "# 这是使用 OpenAI 模型所必需的\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# 设置 OpenAI API代理地址 (例如：https://api.apiyi.com/v1）\n",
    "_set_env(\"OPENAI_BASE_URL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326f35b",
   "metadata": {
    "id": "a326f35b"
   },
   "source": [
    "我们已经安装了 `langchain-openai` 包。有了这个包，我们可以实例化我们的 `ChatOpenAI` 模型对象。如果你是第一次注册 API，你应该会收到[免费积分](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517)，可以应用于任何模型。你可以[在这里](https://openai.com/api/pricing/)查看各种模型的定价。笔记本将默认使用 `gpt-4o`，因为它在质量、价格和速度之间取得了良好的平衡[更多信息请参见这里](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini)，但你也可以选择价格较低的 `gpt-3.5` 系列模型。\n",
    "\n",
    "聊天模型有几个[标准参数](https://python.langchain.com/v0.2/docs/concepts/#chat-models)可以设置。最常见的两个是：\n",
    "\n",
    "* `model`：模型名称\n",
    "* `temperature`：采样温度\n",
    "\n",
    "`Temperature` 控制模型输出的随机性或创造性，其中低温度（接近 0）产生更确定性和专注的输出。这适合需要准确性或事实性响应的任务。高温度（接近 1）适合创造性任务或生成多样化响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e19a54d3",
   "metadata": {
    "id": "e19a54d3"
   },
   "outputs": [],
   "source": [
    "# 导入 ChatOpenAI 类\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 创建 GPT-4o 聊天模型实例\n",
    "# model=\"gpt-4o\": 使用 GPT-4o 模型，这是 OpenAI 的最新模型\n",
    "# temperature=0: 设置温度为 0，使输出更加确定性和一致\n",
    "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# 创建 GPT-3.5 Turbo 聊天模型实例\n",
    "# model=\"gpt-3.5-turbo-0125\": 使用 GPT-3.5 Turbo 模型\n",
    "# temperature=0: 同样设置为 0，确保输出的一致性\n",
    "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28450d1b",
   "metadata": {
    "id": "28450d1b"
   },
   "source": [
    "LangChain 中的聊天模型有许多[默认方法](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface)。在大多数情况下，我们将使用：\n",
    "\n",
    "* `stream`：流式返回响应的块\n",
    "* `invoke`：在输入上调用链\n",
    "\n",
    "如前所述，聊天模型接受[消息](https://python.langchain.com/v0.2/docs/concepts/#messages)作为输入。消息具有一个角色（描述谁在说消息）和一个内容属性。我们稍后会更多地讨论这个问题，但这里让我们先展示基础知识。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1280e1b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1280e1b",
    "outputId": "08c10f33-0874-413f-870b-d2cf24c7dff8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='你好！有什么可以帮你的吗？😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 17, 'total_tokens': 27, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CRb1WSJGAjl253sNOAofYiRDHxdDa', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--74b2ed1e-d2dc-41ea-b29d-7e52a9d84d47-0', usage_metadata={'input_tokens': 17, 'output_tokens': 10, 'total_tokens': 27, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入 HumanMessage 类，用于创建人类用户的消息\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 创建一个人类消息\n",
    "# content: 消息内容\n",
    "# name: 发送者的名称（可选）\n",
    "msg = HumanMessage(content=\"你好呀\", name=\"萤火AI百宝箱\")\n",
    "\n",
    "# 创建消息列表\n",
    "# 聊天模型通常接受消息列表作为输入\n",
    "messages = [msg]\n",
    "\n",
    "# 使用消息列表调用模型\n",
    "# invoke() 方法会发送消息给模型并返回响应\n",
    "gpt4o_chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac73e4c",
   "metadata": {
    "id": "cac73e4c"
   },
   "source": [
    "我们得到一个 `AIMessage` 响应。另外，请注意我们可以直接用字符串调用聊天模型。当字符串作为输入传递时，它会被转换为 `HumanMessage`，然后传递给底层模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f27c6c9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f27c6c9a",
    "outputId": "ee85a471-3314-43d1-c430-3066ee0844e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='### 通俗解释 LangGraph\\n\\nLangGraph 是一种基于图结构的语言模型技术，它将语言中的词汇、句子、语义关系等信息以图的形式进行建模和处理。简单来说，LangGraph 就像是一个“语言地图”，通过节点和边来表示语言中的各种关系。节点可以是单词、短语或概念，边则表示它们之间的关联，比如语义关系、上下文关系等。\\n\\n这种图结构的优势在于，它能够更好地捕捉语言中的复杂关系，尤其是长距离依赖和多层次语义。相比传统的序列模型（如 RNN 或 Transformer），LangGraph 更擅长处理非线性、非顺序的语言信息。\\n\\n---\\n\\n### LangGraph 的应用场景\\n\\n1. **知识图谱构建与问答**  \\n   LangGraph 可以将文本中的知识点抽取出来，构建知识图谱，并支持基于图谱的问答系统。例如，用户提问“爱因斯坦的主要成就是什么？”系统可以通过图谱快速定位相关节点并生成答案。\\n\\n2. **文本生成与摘要**  \\n   在生成文本或摘要时，LangGraph 能够基于图结构理解文本的核心内容，生成更符合语义逻辑的结果。\\n\\n3. **推荐系统**  \\n   LangGraph 可以将用户行为、兴趣和内容信息建模为图结构，从而实现更精准的推荐。例如，基于用户的历史行为推荐相关的文章或产品。\\n\\n4. **情感分析与舆情监控**  \\n   通过图结构分析文本中的情感词汇及其关联，LangGraph 能够更准确地捕捉用户的情感倾向，帮助企业进行舆情监控。\\n\\n5. **多模态数据处理**  \\n   LangGraph 不仅可以处理文本，还可以结合图像、视频等多模态数据，构建跨模态的语义图结构，用于场景理解或内容生成。\\n\\n---\\n\\n### LangGraph 与其他竞品的对比分析\\n\\n1. **与 Transformer 的对比**  \\n   Transformer 是基于序列的语言模型，擅长处理顺序关系，但在长距离依赖和复杂语义关系上可能存在局限。而 LangGraph 的图结构能够更自然地表示语言中的非线性关系，尤其是在知识图谱和语义网络的构建上更具优势。\\n\\n2. **与知识图谱技术的对比**  \\n   传统知识图谱技术通常依赖人工规则或预定义的关系，而 LangGraph 可以通过语言模型自动生成图结构，减少人工干预，提升效率。\\n\\n3. **与 GNN（图神经网络）的对比**  \\n   GNN 是一种通用的图结构处理技术，而 LangGraph 是专门针对语言数据设计的图模型。LangGraph 在语言理解和生成方面更具针对性，能够结合语言模型的优势。\\n\\n4. **与 GPT 系列的对比**  \\n   GPT 系列模型擅长生成类任务，但在知识推理和复杂关系建模上可能不如 LangGraph。LangGraph 的图结构能够更好地支持推理类任务。\\n\\n---\\n\\n### 企业落地场景\\n\\n1. **智能客服**  \\n   企业可以利用 LangGraph 构建智能客服系统，通过图结构理解用户问题并生成精准答案，提升客户满意度。\\n\\n2. **知识管理与搜索**  \\n   企业可以基于 LangGraph 构建内部知识图谱，支持员工快速搜索和获取信息，提升工作效率。\\n\\n3. **精准营销**  \\n   LangGraph 可以帮助企业分析用户行为和兴趣，构建用户画像，从而实现更精准的营销策略。\\n\\n4. **风险监控与合规审查**  \\n   在金融、法律等领域，LangGraph 可以帮助企业分析文本中的风险点或合规问题，降低运营风险。\\n\\n5. **内容生成与优化**  \\n   媒体和内容创作企业可以利用 LangGraph 生成高质量的文章、广告文案或视频脚本。\\n\\n---\\n\\n### 总结\\n\\nLangGraph 是一种创新的语言模型技术，通过图结构更好地捕捉语言中的复杂关系，适用于知识图谱、推荐系统、文本生成等多个场景。相比传统技术，LangGraph 在语义理解和推理方面更具优势，能够帮助企业实现智能化转型，提升效率和竞争力。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 960, 'prompt_tokens': 28, 'total_tokens': 988, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-11-20', 'system_fingerprint': 'fp_b54fe76834', 'id': 'chatcmpl-CRb1YYhNymnGPGuHGofZu8rCEGgXo', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--1d10cfce-a359-4174-833a-361c66b17a07-0', usage_metadata={'input_tokens': 28, 'output_tokens': 960, 'total_tokens': 988, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 直接使用字符串调用模型\n",
    "# 当传入字符串时，LangChain 会自动将其转换为 HumanMessage\n",
    "gpt4o_chat.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc2f0ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdc2f0ca",
    "outputId": "6adff1a8-56cb-4f18-fbbb-b8a95dae682e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='LangGraph是一种基于语言学的图数据库，用于存储和查询语言相关的数据。它可以帮助用户更好地理解语言之间的关系，如词汇之间的关联、语法结构等。\\n\\n应用场景：\\n1. 语言学研究：研究人员可以利用LangGraph来分析不同语言之间的共同点和差异，探索语言演化的规律。\\n2. 自然语言处理：LangGraph可以用于构建自然语言处理模型，帮助机器理解和生成自然语言。\\n3. 知识图谱：LangGraph可以作为知识图谱的一部分，帮助用户更好地组织和查询语言相关的知识。\\n\\n竞品对比分析：\\n与传统的关系型数据库相比，LangGraph更适合存储和查询语言相关的数据，因为它能够更好地表达语言之间的复杂关系。与其他图数据库相比，LangGraph专注于语言学领域，提供了更多针对语言的特定功能和查询语言。\\n\\n企业落地：\\n企业可以利用LangGraph来构建自己的语言知识图谱，帮助员工更好地理解和利用语言相关的知识。例如，一家语言学研究机构可以利用LangGraph来存储和分析大量的语言数据，从而推动语言学研究的进展。另外，一家自然语言处理公司可以利用LangGraph来构建更加智能的语言处理系统，提升产品的竞争力。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 450, 'prompt_tokens': 36, 'total_tokens': 486, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_0165350fbb', 'id': 'chatcmpl-CRb1lIxAaNQWYy02X7SoiM7LPjjab', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--af86283b-5591-4706-841c-01a80539b5e3-0', usage_metadata={'input_tokens': 36, 'output_tokens': 450, 'total_tokens': 486, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用 GPT-3.5 Turbo 模型调用\n",
    "# 同样可以直接传入字符串\n",
    "gpt35_chat.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d65a1a2-9828-4388-9510-1afb5453f1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangGraph是一种基于语言学的图数据库，用于存储和查询语言相关的数据。它可以帮助用户更有效地管理和分析大量的语言数据，如文本、语音、图像等。\n",
      "\n",
      "应用场景：\n",
      "1. 自然语言处理：LangGraph可以用于构建自然语言处理模型，帮助机器理解和处理人类语言。\n",
      "2. 知识图谱：LangGraph可以用于构建知识图谱，帮助用户更好地组织和查询知识。\n",
      "3. 情感分析：LangGraph可以用于分析文本中的情感色彩，帮助用户了解用户情感倾向。\n",
      "\n",
      "竞品对比分析：\n",
      "1. Neo4j：Neo4j是一种流行的图数据库，但它主要用于一般的图数据存储和查询，不太适用于语言相关的数据。\n",
      "2. TigerGraph：TigerGraph是一种高性能的图数据库，但它也不专注于语言相关的数据存储和查询。\n",
      "\n",
      "企业落地：\n",
      "企业可以通过将LangGraph集成到他们的现有系统中，来提高他们对语言数据的管理和分析能力。例如，企业可以将LangGraph用于构建智能客服系统，帮助客户更快速地解决问题；或者将LangGraph用于构建智能搜索引擎，提供更准确的搜索结果。通过这些应用，企业可以提高效率，降低成本，提升用户体验。\n",
      "--- 流式输出结束 ---\n"
     ]
    }
   ],
   "source": [
    "# 调用 stream 方法并遍历返回的流\n",
    "stream_response = gpt35_chat.stream(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")\n",
    "\n",
    "# 打印每个分块的内容，并把它们连接起来形成完整回复\n",
    "full_response = \"\"\n",
    "for chunk in stream_response:\n",
    "    # 提取每个分块（chunk）中的内容（content）\n",
    "    content = chunk.content\n",
    "    print(content, end=\"\", flush=True) # 使用 print 实时输出内容\n",
    "    full_response += content\n",
    "\n",
    "print(\"\\n--- 流式输出结束 ---\")\n",
    "# print(f\"\\n完整回复:\\n{full_response}\") # 打印完整回复"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582c0e5a",
   "metadata": {
    "id": "582c0e5a"
   },
   "source": [
    "所有聊天模型的接口都是一致的，模型通常在每个笔记本启动时初始化一次。\n",
    "\n",
    "因此，如果你强烈偏好另一个提供商，你可以轻松地在模型之间切换，而无需更改下游代码。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad0069a",
   "metadata": {
    "id": "3ad0069a"
   },
   "source": [
    "## 搜索工具\n",
    "\n",
    "你还会在 README 中看到 [Tavily](https://tavily.com/)，这是一个为 LLM 和 RAG 优化的搜索引擎，旨在提供高效、快速和持久的搜索结果。如前所述，注册很容易，并提供慷慨的免费层级。一些课程（在模块 4 中）将默认使用 Tavily，但当然，如果你想为自己修改代码，也可以使用其他搜索工具。\n",
    "\n",
    "**Tavily API 密钥 (API Key) 的注册和申请流程**通常非常直接，但具体步骤可能会有细微变化。以下是一般性的指导和你可能需要遵循的步骤：\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 访问 Tavily 官方网站\n",
    "\n",
    "首先，你需要访问 **Tavily 的官方网站**（你已经提供了链接：[https://tavily.com/](https://tavily.com/)）。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 查找注册/登录或 API 页面\n",
    "\n",
    "在网站上，你需要找到以下选项之一：\n",
    "* **“Sign Up”（注册）** 或 **“Log In”（登录）** 按钮。\n",
    "* **“API”、“Developers”（开发者）** 或 **“Pricing”（定价）** 页面。\n",
    "\n",
    "通常，你需要先创建一个用户账户才能申请 API 密钥。\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 注册新账户\n",
    "\n",
    "如果你是新用户：\n",
    "* 点击 **“Sign Up”** 或 **“注册”**。\n",
    "* 你可能需要提供一个 **电子邮件地址** 和设置一个 **密码**。\n",
    "* 有些服务也支持使用 **Google** 或其他第三方账户直接登录/注册。\n",
    "* 完成注册后，你可能需要**验证你的邮箱**（通过点击发送到你邮箱的链接）。\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 访问 API 仪表板/设置\n",
    "\n",
    "登录你的账户后，你需要进入到管理 API 密钥的区域，这通常被称为：\n",
    "* **“Dashboard”（仪表板）**\n",
    "* **“API Keys”（API 密钥）**\n",
    "* **“Settings”（设置）** 或 **“Profile”（个人资料）**\n",
    "\n",
    "---\n",
    "\n",
    "### 5. 生成 API 密钥\n",
    "\n",
    "在这个 API 密钥管理页面：\n",
    "* 查找类似于 **“Create New Key”（创建新密钥）**、**“Generate API Key”（生成 API 密钥）** 或 **“Get Started”（开始使用）** 的按钮。\n",
    "* 点击该按钮，系统会**立即生成**一串独特的长字符，这就是你的 **Tavily API Key**。\n",
    "\n",
    "---\n",
    "\n",
    "### 6. 保存你的 API 密钥\n",
    "\n",
    "**重要提示：**\n",
    "* 生成的 API 密钥通常只会在**生成时显示一次**。\n",
    "* **请务必立即复制并安全地存储**（例如，在密码管理器或安全文档中）你的密钥。\n",
    "* 如果丢失，你可能需要生成一个新的密钥，而旧的密钥可能会被吊销。\n",
    "\n",
    "完成这些步骤后，你就可以在你的应用程序或开发环境中使用这个 **TAVILY\\_API\\_KEY** 来调用 Tavily 的搜索 API 了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "091dff13",
   "metadata": {
    "id": "091dff13"
   },
   "outputs": [],
   "source": [
    "# 设置 Tavily API 密钥\n",
    "# 这是使用 Tavily 搜索功能所必需的\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52d69da9",
   "metadata": {
    "id": "52d69da9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_70482/4221156914.py:6: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily_search = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "# 导入 Tavily 搜索结果工具\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# 创建 Tavily 搜索实例\n",
    "# max_results=3: 设置最大返回结果数量为 3\n",
    "tavily_search = TavilySearchResults(max_results=3)\n",
    "\n",
    "# 执行搜索\n",
    "# 搜索 \"What is LangGraph?\" 并获取结果\n",
    "search_docs = tavily_search.invoke(\"通俗解释LangGraph?包含应用场景，其他竞品对比分析，企业落地\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d06f87e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d06f87e6",
    "outputId": "5f9fe70c-6e2d-4f56-8217-563309f0118f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '从零构建企业级多智能体系统：LangGraph 全流程实战+ 部署指南 ...',\n",
       "  'url': 'https://blog.csdn.net/sinat_28461591/article/details/147076125',\n",
       "  'content': 'LangGraph 是 LangChain 生态中的高阶 API 框架，专为多角色协作 Agent 设计，核心思想是将复杂工作流分解为有向图结构，节点为原子操作，边为执行逻辑。与传统工作流引擎相比，LangGraph 支持 Python 函数/LLM 调用、内存共享状态管理和 LLM 实时决策，并集成 LangSmith 可视化调试工具。其架构演进从 2023 年 Q4 的原型设计到 2024 年 Q3 的多租户线程管理，逐步完善。LangGraph 通过增量更新策略优化状态管理，支持持久化检查点和多租户线\\n\\n革新！AI应用架构师助力数据分析师AI智能体在智能化数据洞察中革新\\n\\n最新发布\\n\\nAI天才研究院\\n\\n08-02\\n\\n927 [...] 一、LangGraph 是什么？为什么说它重新定义了 Agent 编排\\n 二、核心原理解析：图结构 + 状态驱动，才是智能体的最佳形式\\n 三、实战项目：打造一个企业级多角色 Agent 系统（含工具链）\\n  + 3.1 项目目标与场景模拟\\n  + 3.2 项目架构图与核心流程设计\\n  + 3.3 代码实战：节点构建与图结构编排\\n  + 3.4 工具调用与图表生成节点实战\\n  + 3.5 前后端部署与接口封装实战\\n  + 3.6 部署建议与工程总结\\n 四、部署实战：如何将 LangGraph 系统跑在本地或线上？\\n 五、AutoGen vs LangGraph：哪个才适合生产环境？\\n 六、最佳实践建议 + 常见问题汇总\\n 七、总结：为什么 LangGraph 是 Agent 工程落地的必经之路\\n\\n## 一、LangGraph 是什么？为什么说它重新定义了 Agent 编排\\n\\n> 单个模型 ≠ 有用的产品，真正复杂的智能体系统，需要 任务分解、角色分工、状态管理和异常回滚。\\n\\nLangChain 虽然为我们提供了丰富的工具封装和链式调用接口，但当面对以下场景时，它开始显得力不从心： [...] 多个 Agent 之间需要协同、顺序执行、互相等待\\n 工具调用需要在特定状态下才触发\\n 用户输入的状态可能随时间演变，甚至中断后要恢复执行\\n\\n这时候，LangGraph 作为 LangChain 的“下一代协作框架”应运而生。\\n\\n### 1.1 LangGraph 的定位：面向工程场景的智能体系统框架\\n\\nLangGraph 是由 LangChain 团队在 2024 年底正式推出的开源框架。它不是用来替代 LangChain，而是专为以下问题而生的：\\n\\n| 场景 | LangChain（传统链式编排） | LangGraph（图结构执行） |\\n --- \\n| 多 Agent 协同 | 不擅长 | 原生支持 |\\n| 任务状态转移 | 需手动编码判断 | 自动推理状态流转 |\\n| 调试和回滚 | 无状态追踪 | 可记录每一步状态并回滚 |\\n| 工程级部署 | 需配合自建组件 | 内置缓存 / 跳转 / 容错 |\\n\\n简而言之：LangGraph 是一套专为「大模型系统化开发」而设计的状态驱动执行框架，适用于多 Agent 协作、复杂任务控制、流程回滚、状态追踪等场景。',\n",
       "  'score': 0.62852204},\n",
       " {'title': 'LangChain+LlamaIndex+AutoGen+LangGraph框架对比_A尘埃',\n",
       "  'url': 'https://devpress.csdn.net/aibjcy/68cd2d3fa6dc56200e86b5c4.html',\n",
       "  'content': '金融、医疗等对流程要求严格的企业应用：LangGraph 的显式工作流提供了最高的可控性和可审计性。\\n 营销、创意等追求结果的场景：AutoGen 的自主协作可能产生意想不到的好结果，但过程更难追踪。\\n\\n团队的技术背景如何？\\n\\n 团队刚开始接触LLM应用：从 LangChain 开始，学习资料最丰富。\\n 团队有很强的工程背景，熟悉状态机、工作流引擎：LangGraph 会非常顺手。\\n 团队专注于搜索和数据库领域：LlamaIndex 的概念会更容易理解。\\n\\n### 常见的混合使用\\n\\nLangChain + LlamaIndex：经典组合。用LangChain作为高级编排框架，管理Agent、工具和记忆，而用LlamaIndex作为其内部一个高性能的、专门的检索工具。这是目前生产级RAG系统的常见架构。 [...] ### LangGraph：实现整体可控的工作流与状态管理\\n\\n作为应用的大脑，编排整个业务流程，管理状态，实现循环和判断。 [...] ### 企业选型\\n\\n核心业务场景是什么？\\n\\n 主要是问答（Q&A）和文档分析？ -> 首选 LlamaIndex。它在RAG上的专业性能可以让你省去大量底层优化工作。\\n 需要调用工具、连接外部API？ -> LangChain 是安全且成熟的选择，它的Tool和Agent抽象非常完善。\\n 是涉及多个角色、多轮对话的复杂任务？ -> 认真考虑 AutoGen。\\n 是有明确步骤、状态和循环的业务流程？ -> LangGraph 是最佳利器。\\n\\n需要快速原型还是生产级部署？\\n\\n 原型阶段/概念验证（POC）：LangChain 是绝佳的起点，它能让你快速看到效果。\\n 生产部署：往往需要组合使用。例如：LlamaIndex (用于核心RAG) + LangChain (用于工具调用和总体编排)。LangGraph (用于定义整体工作流状态) + LlamaIndex (作为工作流中的一个检索节点)。用 LangChain 或 LangGraph 构建主框架，在需要多智能体协作的子模块中引入 AutoGen。\\n\\n可控性和可调试性有多重要？',\n",
       "  'score': 0.62742686},\n",
       " {'title': '快时尚电商行业智能体设计思路与应用实践（二）借助LangChain ...',\n",
       "  'url': 'https://aws.amazon.com/cn/blogs/china/fast-fashion-e-commerce-agent-design-ideas-and-application-practice-part-two/',\n",
       "  'content': '多轮对话状态管理：在多轮对话系统中，用户需求往往跨越多个阶段，涉及意图识别、信息收集、异常处理等环节。LangGraph 通过“状态驱动的图结构”，可以将每个对话阶段拆解为独立节点，每个节点专注于特定任务，并通过条件边灵活流转。例如，针对客户服务流程，可以用 State 结构体显式管理意图、订单详情、补偿等级、升级需求等关键状态信息。节点函数根据当前状态动态决定下一个节点，实现流程的自动分支和升级。在多代理协作场景下，LangGraph 支持将不同领域专家（如订单专家、物流专家、高级专家）作为独立节点，根据实时上下文和复杂度自动路由请求至最合适的专家节点，极大提升了多智能体系统的协作效率和灵活性。 [...] 显式的状态管理：每个节点只关心自己处理的那部分状态，极大降低了耦合度，也方便后续维护和调试。\\n 动态、灵活的代理路由：通过条件边和循环结构，系统可以根据实时上下文动态选择执行路径，实现高度个性化和智能的对话或决策流程。\\n 易于扩展和维护：新增节点或调整路由只需局部修改，不会影响整体架构，极大提升了系统的可维护性。\\n 支持复杂的状态转换逻辑：无论是多轮对话、条件推理还是长流程任务，LangGraph 都能胜任。\\n 人机协同决策支持：通过人机协同（Human-in-the-Loop）机制，LangGraph 能够在工作流的关键节点暂停执行，等待人工干预、审核或决策输入，然后基于人类反馈继续执行后续流程。\\n\\n总体而言，LangGraph 以其图结构和显式状态管理，为构建复杂、动态、多智能体协作的智能系统提供了强大工具。随着业务复杂度提升，LangGraph 让 Agent 系统更灵活、可控、具有扩展能力。针对基于 LangGraph 的 Multi-Agent 与复杂路由的场景，我们将在后续博客中进行进一步演示。\\n\\n### 工具生态（MCP）',\n",
       "  'score': 0.52597815}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 显示搜索结果\n",
    "# 这将输出搜索到的文档列表，包含 URL 和内容摘要\n",
    "search_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd7d5d",
   "metadata": {
    "id": "bafd7d5d"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (agent101)",
   "language": "python",
   "name": "agent101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
