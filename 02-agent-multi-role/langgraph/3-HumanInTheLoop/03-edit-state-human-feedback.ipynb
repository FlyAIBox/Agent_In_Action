{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "env_config_overview",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### üîß ÁéØÂ¢ÉÈÖçÁΩÆÂíåÊ£ÄÊü•\n",
    "\n",
    "#### Ê¶ÇËø∞\n",
    "\n",
    "Êú¨ÊïôÁ®ãÈúÄË¶ÅÁâπÂÆöÁöÑÁéØÂ¢ÉÈÖçÁΩÆ‰ª•Á°Æ‰øùÊúÄ‰Ω≥Â≠¶‰π†‰ΩìÈ™å„ÄÇ‰ª•‰∏ãÈÖçÁΩÆÂ∞ÜÂ∏ÆÂä©‰Ω†Ôºö\n",
    "\n",
    "- ‰ΩøÁî®Áªü‰∏ÄÁöÑcondaÁéØÂ¢ÉÔºöÊøÄÊ¥ªÁªü‰∏ÄÁöÑÂ≠¶‰π†ÁéØÂ¢É\n",
    "- ÈÄöËøáÂõΩÂÜÖÈïúÂÉèÊ∫êÂø´ÈÄüÂÆâË£Ö‰æùËµñÔºöÈÖçÁΩÆpip‰ΩøÁî®Ê∏ÖÂçéÈïúÂÉèÊ∫ê\n",
    "- Âä†ÈÄüÊ®°Âûã‰∏ãËΩΩÔºöËÆæÁΩÆHuggingFaceÈïúÂÉè‰ª£ÁêÜ\n",
    "- Ê£ÄÊü•Á≥ªÁªüÈÖçÁΩÆÔºöÊ£ÄÊü•Á°¨‰ª∂ÂíåËΩØ‰ª∂ÈÖçÁΩÆ\n",
    "\n",
    "#### ÈÖçÁΩÆ\n",
    "\n",
    "- **ÊâÄÈúÄÁéØÂ¢ÉÂèäÂÖ∂‰æùËµñÂ∑≤ÁªèÈÉ®ÁΩ≤Â•Ω**\n",
    "- Âú®`Notebook`Âè≥‰∏äËßíÈÄâÊã©`jupyterÂÜÖÊ†∏`‰∏∫`python(agent101)`ÔºåÂç≥ÂèØÊâßË°å‰∏ãÊñπ‰ª£Á†Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "env_conda_activate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================\n",
      "== Conda ÁéØÂ¢ÉÊ£ÄÊü•Êä•Âëä (‰ªÖÈíàÂØπÂΩìÂâç Bash Â≠êËøõÁ®ã) ==\n",
      "=========================================\n",
      "‚úÖ ÂΩìÂâçÂçïÂÖÉÊ†ºÂ∑≤ÊàêÂäüÊøÄÊ¥ªÂà∞ agent101 ÁéØÂ¢É„ÄÇ\n",
      "‚úÖ Ê≠£Âú®‰ΩøÁî®ÁöÑÁéØÂ¢ÉË∑ØÂæÑ: /root/miniconda3/envs/agent101\n",
      "\n",
      "üí° ÊèêÁ§∫: ÂêéÁª≠ÁöÑPythonÂçïÂÖÉÊ†ºÂ∞Ü‰ΩøÁî®NotebookÂΩìÂâçÈÄâÊã©ÁöÑJupyterÔøΩÔøΩÊ†∏„ÄÇ\n",
      "   Â¶ÇÊûúÈúÄË¶ÅÂêéÁª≠ÂçïÂÖÉÊ†º‰πü‰ΩøÁî®Ê≠§ÁéØÂ¢ÉÔºåËØ∑ÊâßË°å‰ª•‰∏ãÊìç‰Ωú:\n",
      "   1. Ê£ÄÊü• Notebook Âè≥‰∏äËßíÊòØÂê¶Â∑≤ÈÄâÊã© 'python(agent101)'„ÄÇ\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "%%script bash\n",
    "\n",
    "# 1. ÊøÄÊ¥ª conda ÁéØÂ¢É (‰ªÖÂØπÂΩìÂâçÂçïÂÖÉÊ†ºÊúâÊïà)\n",
    "eval \"$(conda shell.bash hook)\"\n",
    "conda activate agent101\n",
    "\n",
    "echo \"=========================================\"\n",
    "echo \"== Conda ÁéØÂ¢ÉÊ£ÄÊü•Êä•Âëä (‰ªÖÈíàÂØπÂΩìÂâç Bash Â≠êËøõÁ®ã) ==\"\n",
    "echo \"=========================================\"\n",
    "\n",
    "# 2. Ê£ÄÊü•ÂΩìÂâçÊøÄÊ¥ªÁöÑÁéØÂ¢É\n",
    "CURRENT_ENV_NAME=$(basename $CONDA_PREFIX)\n",
    "\n",
    "if [ \"$CURRENT_ENV_NAME\" = \"agent101\" ]; then\n",
    "    echo \"‚úÖ ÂΩìÂâçÂçïÂÖÉÊ†ºÂ∑≤ÊàêÂäüÊøÄÊ¥ªÂà∞ agent101 ÁéØÂ¢É„ÄÇ\"\n",
    "    echo \"‚úÖ Ê≠£Âú®‰ΩøÁî®ÁöÑÁéØÂ¢ÉË∑ØÂæÑ: $CONDA_PREFIX\"\n",
    "    echo \"\"\n",
    "    echo \"üí° ÊèêÁ§∫: ÂêéÁª≠ÁöÑPythonÂçïÂÖÉÊ†ºÂ∞Ü‰ΩøÁî®NotebookÂΩìÂâçÈÄâÊã©ÁöÑJupyterÂÜÖÊ†∏„ÄÇ\"\n",
    "    echo \"   Â¶ÇÊûúÈúÄË¶ÅÂêéÁª≠ÂçïÂÖÉÊ†º‰πü‰ΩøÁî®Ê≠§ÁéØÂ¢ÉÔºåËØ∑ÊâßË°å‰ª•‰∏ãÊìç‰Ωú:\"\n",
    "    echo \"   1. Ê£ÄÊü• Notebook Âè≥‰∏äËßíÊòØÂê¶Â∑≤ÈÄâÊã© 'python(agent101)'„ÄÇ\"\n",
    "else\n",
    "    echo \"‚ùå ÊøÄÊ¥ªÂ§±Ë¥•ÊàñÁéØÂ¢ÉÂêçÁß∞‰∏çÂåπÈÖç„ÄÇÂΩìÂâçÁéØÂ¢É: $CURRENT_ENV_NAME\"\n",
    "    echo \"\"\n",
    "    echo \"‚ö†Ô∏è ‰∏•ÈáçÊèêÁ§∫: Âª∫ËÆÆÂ∞Ü Notebook ÁöÑ Jupyter **ÂÜÖÊ†∏ (Kernel)** ÂàáÊç¢‰∏∫ 'python(agent101)'„ÄÇ\"\n",
    "    echo \"   (ÈÄöÂ∏∏‰Ωç‰∫é Notebook Âè≥‰∏äËßíÊàñ 'ÂÜÖÊ†∏' ËèúÂçï‰∏≠)\"\n",
    "    echo \"\"\n",
    "    echo \"üìö Â§áÁî®ÊñπÊ≥ï (‰∏çÊé®Ëçê): Â¶ÇÊûúÊó†Ê≥ïÂàáÊç¢ÂÜÖÊ†∏ÔºåÂàôÂøÖÈ°ªÂú®**ÊØè‰∏™**‰ª£Á†ÅÂçïÂÖÉÊ†ºÁöÑÂ§¥ÈÉ®ÈáçÂ§ç‰ª•‰∏ãÂëΩ‰ª§:\"\n",
    "    echo \"\"\n",
    "    echo \"%%script bash\"\n",
    "    echo \"# ÂøÖÈ°ªÂú®ÊØè‰∏™ÂçïÂÖÉÊ†ºÈÉΩÊâßË°å\"\n",
    "    echo \"eval \\\"\\$(conda shell.bash hook)\\\"\"\n",
    "    echo \"conda activate agent101\"\n",
    "fi\n",
    "\n",
    "echo \"=========================================\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "env_pip_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For variant 'global', will try loading '/etc/xdg/pip/pip.conf'\n",
      "For variant 'global', will try loading '/etc/pip.conf'\n",
      "For variant 'user', will try loading '/root/.pip/pip.conf'\n",
      "For variant 'user', will try loading '/root/.config/pip/pip.conf'\n",
      "For variant 'site', will try loading '/root/miniconda3/envs/agent101/pip.conf'\n",
      "\u001b[31mERROR: Got unexpected number of arguments, expected 0. (example: \"/root/miniconda3/envs/agent101/bin/python -m pip config list\")\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "For variant 'global', will try loading '/etc/xdg/pip/pip.conf'\n",
      "For variant 'global', will try loading '/etc/pip.conf'\n",
      "For variant 'user', will try loading '/root/.pip/pip.conf'\n",
      "For variant 'user', will try loading '/root/.config/pip/pip.conf'\n",
      "For variant 'site', will try loading '/root/miniconda3/envs/agent101/pip.conf'\n",
      "\u001b[31mERROR: Got unexpected number of arguments, expected 0. (example: \"/root/miniconda3/envs/agent101/bin/python -m pip config list\")\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 2. ËÆæÁΩÆpip ‰∏∫Ê∏ÖÂçéÊ∫ê\n",
    "%pip config list -v set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "%pip config list -v list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "env_hf_proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_ENDPOINT=https://hf-mirror.com\n",
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "# 3. ËÆæÁΩÆHuggingFace‰ª£ÁêÜ\n",
    "%env HF_ENDPOINT=https://hf-mirror.com\n",
    "# È™åËØÅÔºö‰ΩøÁî®shellÂëΩ‰ª§Ê£ÄÊü•\n",
    "!echo $HF_ENDPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "env_system_check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==2.2.2 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from pandas==2.2.2) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/agent101/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "### ÁéØÂ¢É‰ø°ÊÅØ\n",
      "| È°πÁõÆ         | ‰ø°ÊÅØ                                                                               |\n",
      "|:-------------|:-----------------------------------------------------------------------------------|\n",
      "| Êìç‰ΩúÁ≥ªÁªü     | Linux Ubuntu 22.04.4 LTS                                                           |\n",
      "| CPU ‰ø°ÊÅØ     | 11th Gen Intel(R) Core(TM) i5-1135G7 @ 2.40GHz (1 physical cores, 4 logical cores) |\n",
      "| ÂÜÖÂ≠ò‰ø°ÊÅØ     | 5.75 GB (Available: 1.76 GB)                                                       |\n",
      "| GPU ‰ø°ÊÅØ     | No GPU found (nvidia-smi not found)                                                |\n",
      "| CUDA ‰ø°ÊÅØ    | CUDA not found                                                                     |\n",
      "| Python ÁâàÊú¨  | 3.10.18                                                                            |\n",
      "| Conda ÁâàÊú¨   | conda 24.4.0                                                                       |\n",
      "| Áâ©ÁêÜÁ£ÅÁõòÁ©∫Èó¥ | Total: 145.49 GB, Used: 20.20 GB, Free: 119.07 GB                                  |\n"
     ]
    }
   ],
   "source": [
    "# üîç ÁéØÂ¢É‰ø°ÊÅØÊ£ÄÊü•ËÑöÊú¨\n",
    "#\n",
    "# Êú¨ËÑöÊú¨ÁöÑ‰ΩúÁî®Ôºö\n",
    "# 1. ÂÆâË£Ö pandas Â∫ìÁî®‰∫éÊï∞ÊçÆË°®Ê†ºÂ±ïÁ§∫\n",
    "# 2. Ê£ÄÊü•Á≥ªÁªüÁöÑÂêÑÈ°πÈÖçÁΩÆ‰ø°ÊÅØ\n",
    "# 3. ÁîüÊàêËØ¶ÁªÜÁöÑÁéØÂ¢ÉÊä•ÂëäË°®Ê†º\n",
    "#\n",
    "# ÂØπ‰∫éÂàùÂ≠¶ËÄÖÊù•ËØ¥ÔºåËøô‰∏™Ê≠•È™§Â∏ÆÂä©‰Ω†Ôºö\n",
    "# - ‰∫ÜËß£ÂΩìÂâçËøêË°åÁéØÂ¢ÉÁöÑÁ°¨‰ª∂ÈÖçÁΩÆ\n",
    "# - Á°ÆËÆ§ÊòØÂê¶Êª°Ë∂≥Ê®°ÂûãËøêË°åÁöÑÊúÄ‰ΩéË¶ÅÊ±Ç\n",
    "# - Â≠¶‰π†Â¶Ç‰ΩïÈÄöËøá‰ª£Á†ÅËé∑ÂèñÁ≥ªÁªü‰ø°ÊÅØ\n",
    "\n",
    "# ÂÆâË£Ö pandas Â∫ì - Áî®‰∫éÂàõÂª∫ÂíåÂ±ïÁ§∫Êï∞ÊçÆË°®Ê†º\n",
    "# pandas ÊòØ Python ‰∏≠ÊúÄÊµÅË°åÁöÑÊï∞ÊçÆÂ§ÑÁêÜÂíåÂàÜÊûêÂ∫ì\n",
    "%pip install pandas==2.2.2 tabulate==0.9.0\n",
    "\n",
    "import platform # ÂØºÂÖ• platform Ê®°Âùó‰ª•Ëé∑ÂèñÁ≥ªÁªü‰ø°ÊÅØ\n",
    "import os # ÂØºÂÖ• os Ê®°Âùó‰ª•‰∏éÊìç‰ΩúÁ≥ªÁªü‰∫§‰∫í\n",
    "import subprocess # ÂØºÂÖ• subprocess Ê®°Âùó‰ª•ËøêË°åÂ§ñÈÉ®ÂëΩ‰ª§\n",
    "import pandas as pd # ÂØºÂÖ• pandas Ê®°ÂùóÔºåÈÄöÂ∏∏Áî®‰∫éÊï∞ÊçÆÂ§ÑÁêÜÔºåËøôÈáåÁî®‰∫éÂàõÂª∫Ë°®Ê†º\n",
    "import shutil # ÂØºÂÖ• shutil Ê®°Âùó‰ª•Ëé∑ÂèñÁ£ÅÁõòÁ©∫Èó¥‰ø°ÊÅØ\n",
    "\n",
    "# Ëé∑Âèñ CPU ‰ø°ÊÅØÁöÑÂáΩÊï∞ÔºåÂåÖÊã¨Ê†∏ÂøÉÊï∞Èáè\n",
    "def get_cpu_info():\n",
    "    cpu_info = \"\" # ÂàùÂßãÂåñ CPU ‰ø°ÊÅØÂ≠óÁ¨¶‰∏≤\n",
    "    physical_cores = \"N/A\"\n",
    "    logical_cores = \"N/A\"\n",
    "\n",
    "    if platform.system() == \"Windows\": # Â¶ÇÊûúÊòØ Windows Á≥ªÁªü\n",
    "        cpu_info = platform.processor() # ‰ΩøÁî® platform.processor() Ëé∑Âèñ CPU ‰ø°ÊÅØ\n",
    "        try:\n",
    "            # Ëé∑Âèñ Windows ‰∏äÁöÑÊ†∏ÂøÉÊï∞Èáè (ÈúÄË¶Å WMI)\n",
    "            import wmi\n",
    "            c = wmi.WMI()\n",
    "            for proc in c.Win32_Processor():\n",
    "                physical_cores = proc.NumberOfCores\n",
    "                logical_cores = proc.NumberOfLogicalProcessors\n",
    "        except:\n",
    "            pass # Â¶ÇÊûú WMI ‰∏çÂèØÁî®ÔºåÂøΩÁï•ÈîôËØØ\n",
    "\n",
    "    elif platform.system() == \"Darwin\": # Â¶ÇÊûúÊòØ macOS Á≥ªÁªü\n",
    "        # Âú® macOS ‰∏ä‰ΩøÁî® sysctl ÂëΩ‰ª§Ëé∑Âèñ CPU ‰ø°ÊÅØÂíåÊ†∏ÂøÉÊï∞Èáè\n",
    "        os.environ['PATH'] = os.environ['PATH'] + os.pathsep + '/usr/sbin' # Êõ¥Êñ∞ PATH ÁéØÂ¢ÉÂèòÈáè\n",
    "        try:\n",
    "            process_brand = subprocess.Popen(['sysctl', \"machdep.cpu.brand_string\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_brand, stderr_brand = process_brand.communicate()\n",
    "            cpu_info = stdout_brand.decode().split(': ')[1].strip() if stdout_brand else \"Could not retrieve CPU info\"\n",
    "\n",
    "            process_physical = subprocess.Popen(['sysctl', \"hw.physicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_physical, stderr_physical = process_physical.communicate()\n",
    "            physical_cores = stdout_physical.decode().split(': ')[1].strip() if stdout_physical else \"N/A\"\n",
    "\n",
    "            process_logical = subprocess.Popen(['sysctl', \"hw.logicalcpu\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "            stdout_logical, stderr_logical = process_logical.communicate()\n",
    "            logical_cores = stdout_logical.decode().split(': ')[1].strip() if stdout_logical else \"N/A\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    else:  # Linux Á≥ªÁªü\n",
    "        try:\n",
    "            # Âú® Linux ‰∏äËØªÂèñ /proc/cpuinfo Êñá‰ª∂Ëé∑Âèñ CPU ‰ø°ÊÅØÂíåÊ†∏ÂøÉÊï∞Èáè\n",
    "            with open('/proc/cpuinfo') as f:\n",
    "                physical_cores_count = 0\n",
    "                logical_cores_count = 0\n",
    "                cpu_info_lines = []\n",
    "                for line in f:\n",
    "                    if line.startswith('model name'): # Êü•Êâæ‰ª• 'model name'ÂºÄÂ§¥ÁöÑË°å\n",
    "                        if not cpu_info: # Âè™Ëé∑ÂèñÁ¨¨‰∏Ä‰∏™ model name\n",
    "                            cpu_info = line.split(': ')[1].strip()\n",
    "                    elif line.startswith('cpu cores'): # Êü•Êâæ‰ª• 'cpu cores' ÂºÄÂ§¥ÁöÑË°å\n",
    "                        physical_cores_count = int(line.split(': ')[1].strip())\n",
    "                    elif line.startswith('processor'): # Êü•Êâæ‰ª• 'processor' ÂºÄÂ§¥ÁöÑË°å\n",
    "                        logical_cores_count += 1\n",
    "                physical_cores = str(physical_cores_count) if physical_cores_count > 0 else \"N/A\"\n",
    "                logical_cores = str(logical_cores_count) if logical_cores_count > 0 else \"N/A\"\n",
    "                if not cpu_info:\n",
    "                     cpu_info = \"Could not retrieve CPU info\"\n",
    "\n",
    "        except:\n",
    "            cpu_info = \"Could not retrieve CPU info\"\n",
    "            physical_cores = \"N/A\"\n",
    "            logical_cores = \"N/A\"\n",
    "\n",
    "    return f\"{cpu_info} ({physical_cores} physical cores, {logical_cores} logical cores)\" # ËøîÂõû CPU ‰ø°ÊÅØÂíåÊ†∏ÂøÉÊï∞Èáè\n",
    "\n",
    "\n",
    "# Ëé∑ÂèñÂÜÖÂ≠ò‰ø°ÊÅØÁöÑÂáΩÊï∞\n",
    "def get_memory_info():\n",
    "    mem_info = \"\" # ÂàùÂßãÂåñÂÜÖÂ≠ò‰ø°ÊÅØÂ≠óÁ¨¶‰∏≤\n",
    "    if platform.system() == \"Windows\":\n",
    "        # Âú® Windows ‰∏ä‰∏çÂÆπÊòìÈÄöËøáÊ†áÂáÜÂ∫ìËé∑ÂèñÔºåÈúÄË¶ÅÂ§ñÈÉ®Â∫ìÊàñ PowerShell\n",
    "        mem_info = \"Requires external tools on Windows\" # ËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "    elif platform.system() == \"Darwin\": # Â¶ÇÊûúÊòØ macOS Á≥ªÁªü\n",
    "        # Âú® macOS ‰∏ä‰ΩøÁî® sysctl ÂëΩ‰ª§Ëé∑ÂèñÂÜÖÂ≠òÂ§ßÂ∞è\n",
    "        process = subprocess.Popen(['sysctl', \"hw.memsize\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # ËøêË°å sysctl ÂëΩ‰ª§\n",
    "        stdout, stderr = process.communicate() # Ëé∑ÂèñÊ†áÂáÜËæìÂá∫ÂíåÊ†áÂáÜÈîôËØØ\n",
    "        mem_bytes = int(stdout.decode().split(': ')[1].strip()) # Ëß£ÊûêËæìÂá∫ÔºåËé∑ÂèñÂÜÖÂ≠òÂ§ßÂ∞èÔºàÂ≠óËäÇÔºâ\n",
    "        mem_gb = mem_bytes / (1024**3) # ËΩ¨Êç¢‰∏∫ GB\n",
    "        mem_info = f\"{mem_gb:.2f} GB\" # Ê†ºÂºèÂåñËæìÂá∫\n",
    "    else:  # Linux Á≥ªÁªü\n",
    "        try:\n",
    "            # Âú® Linux ‰∏äËØªÂèñ /proc/meminfo Êñá‰ª∂Ëé∑ÂèñÂÜÖÂ≠ò‰ø°ÊÅØ\n",
    "            with open('/proc/meminfo') as f:\n",
    "                total_mem_kb = 0\n",
    "                available_mem_kb = 0\n",
    "                for line in f:\n",
    "                    if line.startswith('MemTotal'): # Êü•Êâæ‰ª• 'MemTotal' ÂºÄÂ§¥ÁöÑË°å\n",
    "                        total_mem_kb = int(line.split(':')[1].strip().split()[0]) # Ëß£ÊûêË°åÔºåËé∑ÂèñÊÄªÂÜÖÂ≠òÔºàKBÔºâ\n",
    "                    elif line.startswith('MemAvailable'): # Êü•Êâæ‰ª• 'MemAvailable' ÂºÄÂ§¥ÁöÑË°å\n",
    "                         available_mem_kb = int(line.split(':')[1].strip().split()[0]) # Ëß£ÊûêË°åÔºåËé∑ÂèñÂèØÁî®ÂÜÖÂ≠òÔºàKBÔºâ\n",
    "\n",
    "                if total_mem_kb > 0:\n",
    "                    total_mem_gb = total_mem_kb / (1024**2) # ËΩ¨Êç¢‰∏∫ GB\n",
    "                    mem_info = f\"{total_mem_gb:.2f} GB\" # Ê†ºÂºèÂåñËæìÂá∫ÊÄªÂÜÖÂ≠ò\n",
    "                    if available_mem_kb > 0:\n",
    "                        available_mem_gb = available_mem_kb / (1024**2)\n",
    "                        mem_info += f\" (Available: {available_mem_gb:.2f} GB)\" # Ê∑ªÂä†ÂèØÁî®ÂÜÖÂ≠ò‰ø°ÊÅØ\n",
    "                else:\n",
    "                     mem_info = \"Could not retrieve memory info\" # Â¶ÇÊûúËØªÂèñÊñá‰ª∂Âá∫ÈîôÔºåËÆæÁΩÆÈîôËØØ‰ø°ÊÅØ\n",
    "\n",
    "        except:\n",
    "            mem_info = \"Could not retrieve memory info\" # Â¶ÇÊûúËØªÂèñÊñá‰ª∂Âá∫ÈîôÔºåËÆæÁΩÆÈîôËØØ‰ø°ÊÅØ\n",
    "    return mem_info # ËøîÂõûÂÜÖÂ≠ò‰ø°ÊÅØ\n",
    "\n",
    "# Ëé∑Âèñ GPU ‰ø°ÊÅØÁöÑÂáΩÊï∞ÔºåÂåÖÊã¨ÊòæÂ≠ò\n",
    "def get_gpu_info():\n",
    "    try:\n",
    "        # Â∞ùËØï‰ΩøÁî® nvidia-smi Ëé∑Âèñ NVIDIA GPU ‰ø°ÊÅØÂíåÊòæÂ≠ò\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', '--format=csv,noheader'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # Â¶ÇÊûúÂëΩ‰ª§ÊàêÂäüÊâßË°å\n",
    "            gpu_lines = result.stdout.strip().split('\\n') # Ëß£ÊûêËæìÂá∫ÔºåËé∑Âèñ GPU ÂêçÁß∞ÂíåÊòæÂ≠ò\n",
    "            gpu_info_list = []\n",
    "            for line in gpu_lines:\n",
    "                name, memory = line.split(', ')\n",
    "                gpu_info_list.append(f\"{name} ({memory})\") # Ê†ºÂºèÂåñ GPU ‰ø°ÊÅØ\n",
    "            return \", \".join(gpu_info_list) if gpu_info_list else \"NVIDIA GPU found, but info not listed\" # ËøîÂõû GPU ‰ø°ÊÅØÊàñÊèêÁ§∫‰ø°ÊÅØ\n",
    "        else:\n",
    "             # Â∞ùËØï‰ΩøÁî® lshw Ëé∑ÂèñÂÖ∂‰ªñ GPU ‰ø°ÊÅØ (ÈúÄË¶ÅÂÆâË£Ö lshw)\n",
    "            try:\n",
    "                result_lshw = subprocess.run(['lshw', '-C', 'display'], capture_output=True, text=True)\n",
    "                if result_lshw.returncode == 0: # Â¶ÇÊûúÂëΩ‰ª§ÊàêÂäüÊâßË°å\n",
    "                     # ÁÆÄÂçïËß£ÊûêËæìÂá∫‰∏≠ÁöÑ product ÂêçÁß∞ÂíåÊòæÂ≠ò\n",
    "                    gpu_info_lines = []\n",
    "                    current_gpu = {}\n",
    "                    for line in result_lshw.stdout.splitlines():\n",
    "                        if 'product:' in line:\n",
    "                             if current_gpu:\n",
    "                                 gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "                             current_gpu = {'product': line.split('product:')[1].strip()}\n",
    "                        elif 'size:' in line and 'memory' in line:\n",
    "                             current_gpu['memory'] = line.split('size:')[1].strip()\n",
    "\n",
    "                    if current_gpu: # Ê∑ªÂä†ÊúÄÂêé‰∏Ä‰∏™ GPU ÁöÑ‰ø°ÊÅØ\n",
    "                        gpu_info_lines.append(f\"{current_gpu.get('product', 'GPU')} ({current_gpu.get('memory', 'N/A')})\")\n",
    "\n",
    "                    return \", \".join(gpu_info_lines) if gpu_info_lines else \"GPU found (via lshw), but info not parsed\" # Â¶ÇÊûúÊâæÂà∞ GPU ‰ΩÜ‰ø°ÊÅØÊó†Ê≥ïËß£ÊûêÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "                else:\n",
    "                    return \"No GPU found (checked nvidia-smi and lshw)\" # Â¶ÇÊûú‰∏§‰∏™ÂëΩ‰ª§ÈÉΩÊâæ‰∏çÂà∞ GPUÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "            except FileNotFoundError:\n",
    "                 return \"No GPU found (checked nvidia-smi, lshw not found)\" # Â¶ÇÊûúÊâæ‰∏çÂà∞ lshw ÂëΩ‰ª§ÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "    except FileNotFoundError:\n",
    "        return \"No GPU found (nvidia-smi not found)\" # Â¶ÇÊûúÊâæ‰∏çÂà∞ nvidia-smi ÂëΩ‰ª§ÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "\n",
    "\n",
    "# Ëé∑Âèñ CUDA ÁâàÊú¨ÁöÑÂáΩÊï∞\n",
    "def get_cuda_version():\n",
    "    try:\n",
    "        # Â∞ùËØï‰ΩøÁî® nvcc --version Ëé∑Âèñ CUDA ÁâàÊú¨\n",
    "        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # Â¶ÇÊûúÂëΩ‰ª§ÊàêÂäüÊâßË°å\n",
    "            for line in result.stdout.splitlines():\n",
    "                if 'release' in line: # Êü•ÊâæÂåÖÂê´ 'release' ÁöÑË°å\n",
    "                    return line.split('release ')[1].split(',')[0] # Ëß£ÊûêË°åÔºåÊèêÂèñÁâàÊú¨Âè∑\n",
    "        return \"CUDA not found or version not parsed\" # Â¶ÇÊûúÊâæ‰∏çÂà∞ CUDA ÊàñÁâàÊú¨Êó†Ê≥ïËß£ÊûêÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "    except FileNotFoundError:\n",
    "        return \"CUDA not found\" # Â¶ÇÊûúÊâæ‰∏çÂà∞ nvcc ÂëΩ‰ª§ÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "\n",
    "# Ëé∑Âèñ Python ÁâàÊú¨ÁöÑÂáΩÊï∞\n",
    "def get_python_version():\n",
    "    return platform.python_version() # Ëé∑Âèñ Python ÁâàÊú¨\n",
    "\n",
    "# Ëé∑Âèñ Conda ÁâàÊú¨ÁöÑÂáΩÊï∞\n",
    "def get_conda_version():\n",
    "    try:\n",
    "        # Â∞ùËØï‰ΩøÁî® conda --version Ëé∑Âèñ Conda ÁâàÊú¨\n",
    "        result = subprocess.run(['conda', '--version'], capture_output=True, text=True)\n",
    "        if result.returncode == 0: # Â¶ÇÊûúÂëΩ‰ª§ÊàêÂäüÊâßË°å\n",
    "            return result.stdout.strip() # ËøîÂõû Conda ÁâàÊú¨\n",
    "        return \"Conda not found or version not parsed\" # Â¶ÇÊûúÊâæ‰∏çÂà∞ Conda ÊàñÁâàÊú¨Êó†Ê≥ïËß£ÊûêÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "    except FileNotFoundError:\n",
    "        return \"Conda not found\" # Â¶ÇÊûúÊâæ‰∏çÂà∞ conda ÂëΩ‰ª§ÔºåËÆæÁΩÆÊèêÁ§∫‰ø°ÊÅØ\n",
    "\n",
    "# Ëé∑ÂèñÁâ©ÁêÜÁ£ÅÁõòÁ©∫Èó¥‰ø°ÊÅØÁöÑÂáΩÊï∞\n",
    "def get_disk_space():\n",
    "    try:\n",
    "        total, used, free = shutil.disk_usage(\"/\") # Ëé∑ÂèñÊ†πÁõÆÂΩïÁöÑÁ£ÅÁõò‰ΩøÁî®ÊÉÖÂÜµ\n",
    "        total_gb = total / (1024**3) # ËΩ¨Êç¢‰∏∫ GB\n",
    "        used_gb = used / (1024**3) # ËΩ¨Êç¢‰∏∫ GB\n",
    "        free_gb = free / (1024**3) # ËΩ¨Êç¢‰∏∫ GB\n",
    "        return f\"Total: {total_gb:.2f} GB, Used: {used_gb:.2f} GB, Free: {free_gb:.2f} GB\" # Ê†ºÂºèÂåñËæìÂá∫\n",
    "    except Exception as e:\n",
    "        return f\"Could not retrieve disk info: {e}\" # Â¶ÇÊûúËé∑Âèñ‰ø°ÊÅØÂá∫ÈîôÔºåËÆæÁΩÆÈîôËØØ‰ø°ÊÅØ\n",
    "\n",
    "# Ëé∑ÂèñÁéØÂ¢É‰ø°ÊÅØ\n",
    "os_name = platform.system() # Ëé∑ÂèñÊìç‰ΩúÁ≥ªÁªüÂêçÁß∞\n",
    "os_version = platform.release() # Ëé∑ÂèñÊìç‰ΩúÁ≥ªÁªüÁâàÊú¨\n",
    "if os_name == \"Linux\":\n",
    "    try:\n",
    "        # Âú® Linux ‰∏äÂ∞ùËØïËé∑ÂèñÂèëË°åÁâàÂíåÁâàÊú¨\n",
    "        lsb_info = subprocess.run(['lsb_release', '-a'], capture_output=True, text=True)\n",
    "        if lsb_info.returncode == 0: # Â¶ÇÊûúÂëΩ‰ª§ÊàêÂäüÊâßË°å\n",
    "            for line in lsb_info.stdout.splitlines():\n",
    "                if 'Description:' in line: # Êü•ÊâæÂåÖÂê´ 'Description:' ÁöÑË°å\n",
    "                    os_version = line.split('Description:')[1].strip() # ÊèêÂèñÊèèËø∞‰ø°ÊÅØ‰Ωú‰∏∫ÁâàÊú¨\n",
    "                    break # ÊâæÂà∞ÂêéÈÄÄÂá∫Âæ™ÁéØ\n",
    "                elif 'Release:' in line: # Êü•ÊâæÂåÖÂê´ 'Release:' ÁöÑË°å\n",
    "                     os_version = line.split('Release:')[1].strip() # ÊèêÂèñÁâàÊú¨Âè∑\n",
    "                     # Â∞ùËØïËé∑Âèñ codename\n",
    "                     try:\n",
    "                         codename_info = subprocess.run(['lsb_release', '-c'], capture_output=True, text=True)\n",
    "                         if codename_info.returncode == 0:\n",
    "                             os_version += f\" ({codename_info.stdout.split(':')[1].strip()})\" # Â∞Ü codename Ê∑ªÂä†Âà∞ÁâàÊú¨‰ø°ÊÅØ‰∏≠\n",
    "                     except:\n",
    "                         pass # Â¶ÇÊûúËé∑Âèñ codename Â§±Ë¥•ÂàôÂøΩÁï•\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # lsb_release ÂèØËÉΩÊú™ÂÆâË£ÖÔºåÂøΩÁï•ÈîôËØØ\n",
    "\n",
    "full_os_info = f\"{os_name} {os_version}\" # ÁªÑÂêàÂÆåÊï¥ÁöÑÊìç‰ΩúÁ≥ªÁªü‰ø°ÊÅØ\n",
    "cpu_info = get_cpu_info() # Ë∞ÉÁî®ÂáΩÊï∞Ëé∑Âèñ CPU ‰ø°ÊÅØÂíåÊ†∏ÂøÉÊï∞Èáè\n",
    "memory_info = get_memory_info() # Ë∞ÉÁî®ÂáΩÊï∞Ëé∑ÂèñÂÜÖÂ≠ò‰ø°ÊÅØ\n",
    "gpu_info = get_gpu_info() # Ë∞ÉÁî®ÂáΩÊï∞Ëé∑Âèñ GPU ‰ø°ÊÅØÂíåÊòæÂ≠ò\n",
    "cuda_version = get_cuda_version() # Ë∞ÉÁî®ÂáΩÊï∞Ëé∑Âèñ CUDA ÁâàÊú¨\n",
    "python_version = get_python_version() # Ë∞ÉÁî®ÂáΩÊï∞Ëé∑Âèñ Python ÁâàÊú¨\n",
    "conda_version = get_conda_version() # Ë∞ÉÁî®ÂáΩÊï∞Ëé∑Âèñ Conda ÁâàÊú¨\n",
    "disk_info = get_disk_space() # Ë∞ÉÁî®ÂáΩÊï∞Ëé∑ÂèñÁâ©ÁêÜÁ£ÅÁõòÁ©∫Èó¥‰ø°ÊÅØ\n",
    "\n",
    "\n",
    "# ÂàõÂª∫Áî®‰∫éÂ≠òÂÇ®Êï∞ÊçÆÁöÑÂ≠óÂÖ∏\n",
    "env_data = {\n",
    "    \"È°πÁõÆ\": [ # È°πÁõÆÂêçÁß∞ÂàóË°®\n",
    "        \"Êìç‰ΩúÁ≥ªÁªü\",\n",
    "        \"CPU ‰ø°ÊÅØ\",\n",
    "        \"ÂÜÖÂ≠ò‰ø°ÊÅØ\",\n",
    "        \"GPU ‰ø°ÊÅØ\",\n",
    "        \"CUDA ‰ø°ÊÅØ\",\n",
    "        \"Python ÁâàÊú¨\",\n",
    "        \"Conda ÁâàÊú¨\",\n",
    "        \"Áâ©ÁêÜÁ£ÅÁõòÁ©∫Èó¥\" # Ê∑ªÂä†Áâ©ÁêÜÁ£ÅÁõòÁ©∫Èó¥\n",
    "    ],\n",
    "    \"‰ø°ÊÅØ\": [ # ÂØπÂ∫îÁöÑ‰ø°ÊÅØÂàóË°®\n",
    "        full_os_info,\n",
    "        cpu_info,\n",
    "        memory_info,\n",
    "        gpu_info,\n",
    "        cuda_version,\n",
    "        python_version,\n",
    "        conda_version,\n",
    "        disk_info # Ê∑ªÂä†Áâ©ÁêÜÁ£ÅÁõòÁ©∫Èó¥‰ø°ÊÅØ\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ÂàõÂª∫‰∏Ä‰∏™ pandas DataFrame\n",
    "df = pd.DataFrame(env_data)\n",
    "\n",
    "# ÊâìÂç∞Ë°®Ê†º\n",
    "print(\"### ÁéØÂ¢É‰ø°ÊÅØ\") # ÊâìÂç∞Ê†áÈ¢ò\n",
    "print(df.to_markdown(index=False)) # Â∞Ü DataFrame ËΩ¨Êç¢‰∏∫ Markdown Ê†ºÂºèÂπ∂ÊâìÂç∞Ôºå‰∏çÂåÖÂê´Á¥¢Âºï\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147e576c",
   "metadata": {
    "id": "147e576c"
   },
   "source": [
    "# ÂõæÁä∂ÊÄÅÁºñËæë‰∏é‰∫∫Â∑•ÂèçÈ¶à\n",
    "\n",
    "\n",
    "## Êú¨ÊïôÁ®ãÁÆÄ‰ªã\n",
    "\n",
    "Êú¨ÊïôÁ®ãÂ∞ÜÂêë‰Ω†Â±ïÁ§∫Â¶Ç‰ΩïÂú®LangGraph‰∏≠ÁºñËæëÂõæÁä∂ÊÄÅÂπ∂ÈõÜÊàê‰∫∫Â∑•ÂèçÈ¶à„ÄÇËøôÊòØÊûÑÂª∫‰∫∫Êú∫Âçè‰ΩúAIÁ≥ªÁªüÁöÑÂÖ≥ÈîÆÊäÄÊúØÔºåËÆ©‰∫∫Á±ªÂèØ‰ª•Âú®AIÊâßË°åËøáÁ®ã‰∏≠ËøõË°åÂπ≤È¢ÑÂíåÊåáÂØº„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f2448-21c3-4196-9e61-0b47e7d0048b",
   "metadata": {
    "id": "3b2f2448-21c3-4196-9e61-0b47e7d0048b"
   },
   "source": [
    "# ÁºñËæëÂõæÁä∂ÊÄÅ\n",
    "\n",
    "## ÂõûÈ°æ\n",
    "\n",
    "Êàë‰ª¨‰πãÂâçËÆ®ËÆ∫‰∫Ü‰∫∫Êú∫Âçè‰ΩúÔºàhuman-in-the-loopÔºâÁöÑÂä®Êú∫Ôºö\n",
    "\n",
    "**(1) ÂÆ°ÊâπÔºàApprovalÔºâ** - Êàë‰ª¨ÂèØ‰ª•‰∏≠Êñ≠Êô∫ËÉΩ‰ΩìÔºåÂêëÁî®Êà∑Â±ïÁ§∫Áä∂ÊÄÅÔºåÂπ∂ÂÖÅËÆ∏Áî®Êà∑Êé•ÂèóÊüê‰∏™Êìç‰Ωú\n",
    "\n",
    "**(2) Ë∞ÉËØïÔºàDebuggingÔºâ** - Êàë‰ª¨ÂèØ‰ª•ÂõûÊªöÂõæÊù•ÈáçÁé∞ÊàñÈÅøÂÖçÈóÆÈ¢ò\n",
    "\n",
    "**(3) ÁºñËæëÔºàEditingÔºâ** - ‰Ω†ÂèØ‰ª•‰øÆÊîπÁä∂ÊÄÅ\n",
    "\n",
    "Êàë‰ª¨Â±ïÁ§∫‰∫ÜÊñ≠ÁÇπÂ¶Ç‰ΩïÊîØÊåÅÁî®Êà∑ÂÆ°ÊâπÔºå‰ΩÜËøò‰∏çÁü•ÈÅìÂ¶Ç‰ΩïÂú®ÂõæË¢´‰∏≠Êñ≠Âêé‰øÆÊîπÂõæÁä∂ÊÄÅÔºÅ\n",
    "\n",
    "## Â≠¶‰π†ÁõÆÊ†á\n",
    "\n",
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨Â±ïÁ§∫Â¶Ç‰ΩïÁõ¥Êé•ÁºñËæëÂõæÁä∂ÊÄÅÂπ∂ÊèíÂÖ•‰∫∫Â∑•ÂèçÈ¶à„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95d26b8c-d958-4d21-9ca4-4636d3dfe45c",
   "metadata": {
    "id": "95d26b8c-d958-4d21-9ca4-4636d3dfe45c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ÂÆâË£ÖÂøÖË¶ÅÁöÑ‰æùËµñÂåÖ\n",
    "# Ëøô‰∫õÂåÖÊòØÊûÑÂª∫LangGraphÂ∫îÁî®ÁöÑÊ†∏ÂøÉÁªÑ‰ª∂\n",
    "%pip install --quiet langgraph==0.6.7 langchain_openai==0.3.32 langgraph_sdk==0.2.6 langgraph-prebuilt==0.6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5948594",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5948594",
    "outputId": "90174103-6cdd-4be5-a4c9-7c84d2b01331"
   },
   "outputs": [],
   "source": [
    "# ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáè\n",
    "import os, getpass\n",
    "\n",
    "def _set_env(var: str):\n",
    "    \"\"\"ÂÆâÂÖ®Âú∞ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºåÂ¶ÇÊûú‰∏çÂ≠òÂú®ÂàôÊèêÁ§∫Áî®Êà∑ËæìÂÖ•\"\"\"\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "# ËÆæÁΩÆOpenAI APIÂØÜÈí•ÔºåËøôÊòØ‰ΩøÁî®OpenAIÊ®°ÂûãÊâÄÂøÖÈúÄÁöÑ\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "# ËÆæÁΩÆ OpenAI API‰ª£ÁêÜÂú∞ÂùÄ (‰æãÂ¶ÇÔºöhttps://api.apiyi.com/v1Ôºâ\n",
    "_set_env(\"OPENAI_BASE_URL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8df1f-a76a-4803-a532-ea9802106ac8",
   "metadata": {
    "id": "65a8df1f-a76a-4803-a532-ea9802106ac8"
   },
   "source": [
    "## ÁºñËæëÁä∂ÊÄÅ\n",
    "\n",
    "‰πãÂâçÔºåÊàë‰ª¨‰ªãÁªç‰∫ÜÊñ≠ÁÇπÔºàbreakpointsÔºâ„ÄÇ\n",
    "\n",
    "Êàë‰ª¨‰ΩøÁî®Êñ≠ÁÇπÊù•‰∏≠Êñ≠ÂõæÔºåÂπ∂Âú®ÊâßË°å‰∏ã‰∏Ä‰∏™ËäÇÁÇπ‰πãÂâçÁ≠âÂæÖÁî®Êà∑ÂÆ°Êâπ„ÄÇ\n",
    "\n",
    "‰ΩÜ**Êñ≠ÁÇπ‰πüÊòØ[‰øÆÊîπÂõæÁä∂ÊÄÅÁöÑÊú∫‰ºö](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/edit-graph-state/)„ÄÇ**\n",
    "\n",
    "ËÆ©Êàë‰ª¨Âú®`assistant`ËäÇÁÇπ‰πãÂâçËÆæÁΩÆ‰∏Ä‰∏™Êñ≠ÁÇπÊù•ÊûÑÂª∫Êàë‰ª¨ÁöÑÊô∫ËÉΩ‰Ωì„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcf24f05-ac2b-455e-846c-0c50ac86e1f4",
   "metadata": {
    "id": "bcf24f05-ac2b-455e-846c-0c50ac86e1f4"
   },
   "outputs": [],
   "source": [
    "# ÂØºÂÖ•ÂøÖË¶ÅÁöÑÂ∫ì\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ÂÆö‰πâÊï∞Â≠¶ËøêÁÆóÂ∑•ÂÖ∑ÂáΩÊï∞\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"‰πòÊ≥ïËøêÁÆóÔºöËÆ°ÁÆó‰∏§‰∏™Êï¥Êï∞ÁöÑ‰πòÁßØ\n",
    "\n",
    "    Args:\n",
    "        a: Á¨¨‰∏Ä‰∏™Êï¥Êï∞\n",
    "        b: Á¨¨‰∫å‰∏™Êï¥Êï∞\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Âä†Ê≥ïËøêÁÆóÔºöËÆ°ÁÆó‰∏§‰∏™Êï¥Êï∞ÁöÑÂíå\n",
    "\n",
    "    Args:\n",
    "        a: Á¨¨‰∏Ä‰∏™Êï¥Êï∞\n",
    "        b: Á¨¨‰∫å‰∏™Êï¥Êï∞\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"\"\"Èô§Ê≥ïËøêÁÆóÔºöËÆ°ÁÆó‰∏§‰∏™Êï¥Êï∞ÁöÑÂïÜ\n",
    "\n",
    "    Args:\n",
    "        a: Ë¢´Èô§Êï∞\n",
    "        b: Èô§Êï∞\n",
    "    \"\"\"\n",
    "    return a / b\n",
    "\n",
    "# Â∞ÜÂ∑•ÂÖ∑ÂáΩÊï∞ÁªÑÂêàÊàêÂ∑•ÂÖ∑ÂàóË°®\n",
    "tools = [add, multiply, divide]\n",
    "\n",
    "# ÂàùÂßãÂåñOpenAIËÅäÂ§©Ê®°Âûã\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# Â∞ÜÂ∑•ÂÖ∑ÁªëÂÆöÂà∞ËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÊ®°ÂûãËÉΩÂ§üË∞ÉÁî®Ëøô‰∫õÂ∑•ÂÖ∑\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfe84af-5c62-4c3f-8ed7-96b5261f0b7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "id": "5dfe84af-5c62-4c3f-8ed7-96b5261f0b7b",
    "outputId": "b0a5516f-31c0-495d-d439-caf0e80e825b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂõæÂèØËßÜÂåñÔºö\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÂõæÊ∏≤ÊüìÊàêÂäüÔºÅ\n"
     ]
    }
   ],
   "source": [
    "# ÂØºÂÖ•ÂøÖË¶ÅÁöÑÂ∫ì\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.prebuilt import tools_condition, ToolNode\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# ÂÆö‰πâÁ≥ªÁªüÊ∂àÊÅØÔºåÊåáÂØºAIÂä©ÊâãÁöÑË°å‰∏∫\n",
    "sys_msg = SystemMessage(content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\")\n",
    "\n",
    "# ÂÆö‰πâÂä©ÊâãËäÇÁÇπÂáΩÊï∞\n",
    "def assistant(state: MessagesState):\n",
    "    \"\"\"Âä©ÊâãËäÇÁÇπÔºö‰ΩøÁî®ËØ≠Ë®ÄÊ®°ÂûãÂ§ÑÁêÜÊ∂àÊÅØÂπ∂ÂèØËÉΩË∞ÉÁî®Â∑•ÂÖ∑\n",
    "\n",
    "    Args:\n",
    "        state: ÂåÖÂê´Ê∂àÊÅØÂàóË°®ÁöÑÂõæÁä∂ÊÄÅ\n",
    "\n",
    "    Returns:\n",
    "        ÂåÖÂê´AIÂìçÂ∫îÁöÑÊñ∞Áä∂ÊÄÅÊõ¥Êñ∞\n",
    "    \"\"\"\n",
    "    return {\"messages\": [llm_with_tools.invoke([sys_msg] + state[\"messages\"])]}\n",
    "\n",
    "# ÂàõÂª∫Áä∂ÊÄÅÂõæÊûÑÂª∫Âô®\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# ÂÆö‰πâËäÇÁÇπÔºöËøô‰∫õËäÇÁÇπÊâßË°åÂÖ∑‰ΩìÁöÑÂ∑•‰Ωú\n",
    "builder.add_node(\"assistant\", assistant)  # AIÂä©ÊâãËäÇÁÇπ\n",
    "builder.add_node(\"tools\", ToolNode(tools))  # Â∑•ÂÖ∑ÊâßË°åËäÇÁÇπ\n",
    "\n",
    "# ÂÆö‰πâËæπÔºöËøô‰∫õËæπÂÜ≥ÂÆöÊéßÂà∂ÊµÅÁ®ã\n",
    "builder.add_edge(START, \"assistant\")  # ‰ªéÂºÄÂßãÂà∞Âä©Êâã\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # Â¶ÇÊûúÂä©ÊâãÁöÑÊúÄÊñ∞Ê∂àÊÅØÊòØÂ∑•ÂÖ∑Ë∞ÉÁî® -> tools_condition Ë∑ØÁî±Âà∞Â∑•ÂÖ∑ËäÇÁÇπ\n",
    "    # Â¶ÇÊûúÂä©ÊâãÁöÑÊúÄÊñ∞Ê∂àÊÅØ‰∏çÊòØÂ∑•ÂÖ∑Ë∞ÉÁî® -> tools_condition Ë∑ØÁî±Âà∞ÁªìÊùü\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")  # ‰ªéÂ∑•ÂÖ∑ÂõûÂà∞Âä©Êâã\n",
    "\n",
    "# ÂàõÂª∫ÂÜÖÂ≠òÊ£ÄÊü•ÁÇπ‰øùÂ≠òÂô®ÔºåÁî®‰∫é‰øùÂ≠òÂõæÁä∂ÊÄÅ\n",
    "memory = MemorySaver()\n",
    "\n",
    "# ÁºñËØëÂõæÔºåÂú®assistantËäÇÁÇπ‰πãÂâçËÆæÁΩÆ‰∏≠Êñ≠ÁÇπ\n",
    "graph = builder.compile(interrupt_before=[\"assistant\"], checkpointer=memory)\n",
    "\n",
    "# Â±ïÁ§∫ÂõæÁªìÊûÑ\n",
    "# ÂõæÂèØËßÜÂåñ\n",
    "print(\"ÂõæÂèØËßÜÂåñÔºö\")\n",
    "\n",
    "# ÊñπÊ°à1ÔºöÂ∞ùËØï‰ΩøÁî® Pyppeteer Êú¨Âú∞Ê∏≤ÊüìÔºàÊé®ËçêÔºâ\n",
    "try:\n",
    "    # ÂèØËßÜÂåñÔºöÈÄöËøá Mermaid Ê∏≤ÊüìÂõæÁªìÊûÑ\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "    print(\"‚úÖ ÂõæÊ∏≤ÊüìÊàêÂäüÔºÅ\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Pyppeteer Ê∏≤ÊüìÂ§±Ë¥•: {e}\")\n",
    "    \n",
    "    # ÊñπÊ°à2ÔºöÊòæÁ§∫ Mermaid ÊñáÊú¨Ê†ºÂºè\n",
    "    print(\"\\nüìù ÂõæÁªìÊûÑÔºàMermaid ÊñáÊú¨Ê†ºÂºèÔºâÔºö\")\n",
    "    print(\"=\" * 50)\n",
    "    mermaid_text = graph.get_graph().draw_mermaid()\n",
    "    print(mermaid_text)\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ÊñπÊ°à3ÔºöÊòæÁ§∫ÂõæÁöÑËäÇÁÇπÂíåËæπ‰ø°ÊÅØ\n",
    "    print(\"\\nüîó ÂõæÁªìÊûÑ‰ø°ÊÅØÔºö\")\n",
    "    print(\"ËäÇÁÇπ:\", list(graph.get_graph().nodes.keys()))\n",
    "    print(\"Ëæπ:\", list(graph.get_graph().edges))\n",
    "    \n",
    "    # ÊñπÊ°à4ÔºöÊèê‰æõÊâãÂä®Ê∏≤ÊüìËØ¥Êòé\n",
    "    print(\"\\nüí° ÊâãÂä®Ê∏≤ÊüìËØ¥ÊòéÔºö\")\n",
    "    print(\"1. Â§çÂà∂‰∏äÈù¢ÁöÑ Mermaid ÊñáÊú¨\")\n",
    "    print(\"2. ËÆøÈóÆ https://mermaid.live/\")\n",
    "    print(\"3. Á≤òË¥¥ÊñáÊú¨Âà∞ÁºñËæëÂô®‰∏≠Êü•ÁúãÂõæÂΩ¢\")\n",
    "    print(\"4. ÊàñËÄÖ‰ΩøÁî®ÊîØÊåÅ Mermaid ÁöÑ Markdown ÁºñËæëÂô®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a47fd5-1f60-41dc-9206-698ed8ece530",
   "metadata": {
    "id": "92a47fd5-1f60-41dc-9206-698ed8ece530"
   },
   "source": [
    "ËÆ©Êàë‰ª¨ËøêË°åÂõæÔºÅ\n",
    "\n",
    "Êàë‰ª¨ÂèØ‰ª•ÁúãÂà∞ÂõæÂú®ËÅäÂ§©Ê®°ÂûãÂìçÂ∫î‰πãÂâçË¢´‰∏≠Êñ≠‰∫Ü„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2ce488d-00e4-492e-a62c-dd98702c313f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a2ce488d-00e4-492e-a62c-dd98702c313f",
    "outputId": "7736c7bb-0693-433a-99c8-9a04c1458e72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n"
     ]
    }
   ],
   "source": [
    "# ÂÆö‰πâÂàùÂßãËæìÂÖ•ÔºöÁî®Êà∑Ë¶ÅÊ±ÇËøõË°åÊï∞Â≠¶ËøêÁÆó\n",
    "initial_input = {\"messages\": \"Multiply 2 and 3\"}\n",
    "\n",
    "# ÂàõÂª∫Á∫øÁ®ãÈÖçÁΩÆÔºåÁî®‰∫éË∑üË∏™ÂØπËØùÁä∂ÊÄÅ\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "# ËøêË°åÂõæÁõ¥Âà∞Á¨¨‰∏Ä‰∏™‰∏≠Êñ≠ÁÇπ\n",
    "# Áî±‰∫éËÆæÁΩÆ‰∫Üinterrupt_before=[\"assistant\"]ÔºåÂõæ‰ºöÂú®assistantËäÇÁÇπÊâßË°åÂâçÂÅúÊ≠¢\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4be478ef-bd60-4d32-8a05-5f56c93a8396",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4be478ef-bd60-4d32-8a05-5f56c93a8396",
    "outputId": "d898817a-3556-4785-c4e3-2fc1cc4a1229"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StateSnapshot(values={'messages': [HumanMessage(content='Multiply 2 and 3', additional_kwargs={}, response_metadata={}, id='9e69316f-22d0-42ac-9e77-3e3e58a51e0c')]}, next=('assistant',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0ab3e5-b9e4-6258-8000-5e26474f0c03'}}, metadata={'source': 'loop', 'step': 0, 'parents': {}}, created_at='2025-10-17T09:47:56.190270+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f0ab3e5-b97f-687c-bfff-c54209c6fa69'}}, tasks=(PregelTask(id='6249a2f1-962f-90e5-e09e-189f761a209d', name='assistant', path=('__pregel_pull', 'assistant'), error=None, interrupts=(), state=None, result=None),), interrupts=())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ëé∑ÂèñÂΩìÂâçÂõæÁä∂ÊÄÅÔºåÊü•Áúã‰∏≠Êñ≠Êó∂ÁöÑÁä∂ÊÄÅ‰ø°ÊÅØ\n",
    "state = graph.get_state(thread)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ef63a1-2ab8-416d-babf-d35054e294f0",
   "metadata": {
    "id": "36ef63a1-2ab8-416d-babf-d35054e294f0"
   },
   "source": [
    "Áé∞Âú®ÔºåÊàë‰ª¨ÂèØ‰ª•Áõ¥Êé•Â∫îÁî®Áä∂ÊÄÅÊõ¥Êñ∞„ÄÇ\n",
    "\n",
    "ËÆ∞‰ΩèÔºåÂØπ`messages`ÈîÆÁöÑÊõ¥Êñ∞Â∞Ü‰ΩøÁî®`add_messages`ÂΩíÁ∫¶Âô®Ôºö\n",
    "\n",
    "* Â¶ÇÊûúÊàë‰ª¨ÊÉ≥Ë¶ÅË¶ÜÁõñÁé∞ÊúâÊ∂àÊÅØÔºåÂèØ‰ª•Êèê‰æõÊ∂àÊÅØÁöÑ`id`„ÄÇ\n",
    "* Â¶ÇÊûúÊàë‰ª¨Âè™ÊòØÊÉ≥Ë¶ÅËøΩÂä†Âà∞Ê∂àÊÅØÂàóË°®‰∏≠ÔºåÈÇ£‰πàÂèØ‰ª•‰º†ÈÄí‰∏Ä‰∏™Ê≤°ÊúâÊåáÂÆö`id`ÁöÑÊ∂àÊÅØÔºåÂ¶Ç‰∏ãÊâÄÁ§∫„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9179cff1-e529-473a-9ce2-e23b932c2063",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9179cff1-e529-473a-9ce2-e23b932c2063",
    "outputId": "1ea55096-4c34-41bd-8f31-6795d6cef635"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'configurable': {'thread_id': '1',\n",
       "  'checkpoint_ns': '',\n",
       "  'checkpoint_id': '1f0ab3e5-bae2-65f3-8001-d7fd62c17a5e'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Êõ¥Êñ∞ÂõæÁä∂ÊÄÅÔºöÊ∑ªÂä†Êñ∞ÁöÑ‰∫∫Á±ªÊ∂àÊÅØ\n",
    "# ËøôÈáåÊàë‰ª¨Ê∑ªÂä†‰∏ÄÊù°Êñ∞Ê∂àÊÅØÊù•‰øÆÊîπÁî®Êà∑ÁöÑÂéüÂßãËØ∑Ê±Ç\n",
    "graph.update_state(\n",
    "    thread,\n",
    "    {\"messages\": [HumanMessage(content=\"No, actually multiply 3 and 3!\")]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77b8d6a-8c7b-4f7a-b723-121af25ac829",
   "metadata": {
    "id": "d77b8d6a-8c7b-4f7a-b723-121af25ac829"
   },
   "source": [
    "ËÆ©Êàë‰ª¨Áúã‰∏Ä‰∏ãÁªìÊûú„ÄÇ\n",
    "\n",
    "Êàë‰ª¨‰ΩøÁî®Êñ∞Ê∂àÊÅØË∞ÉÁî®‰∫Ü`update_state`„ÄÇ\n",
    "\n",
    "`add_messages`ÂΩíÁ∫¶Âô®Â∞ÜÂÖ∂ËøΩÂä†Âà∞Êàë‰ª¨ÁöÑÁä∂ÊÄÅÈîÆ`messages`‰∏≠„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "141b6aab-ec6d-44f3-beb1-6c22ac5f2158",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "141b6aab-ec6d-44f3-beb1-6c22ac5f2158",
    "outputId": "29f6863a-e7ed-492a-8754-ad65919747d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 2 and 3\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "No, actually multiply 3 and 3!\n"
     ]
    }
   ],
   "source": [
    "# Ëé∑ÂèñÊõ¥Êñ∞ÂêéÁöÑÁä∂ÊÄÅÂπ∂ÊòæÁ§∫ÊâÄÊúâÊ∂àÊÅØ\n",
    "new_state = graph.get_state(thread).values\n",
    "for m in new_state['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4041959-cc3a-4168-8cf7-06d1711921d8",
   "metadata": {
    "id": "e4041959-cc3a-4168-8cf7-06d1711921d8"
   },
   "source": [
    "Áé∞Âú®ÔºåËÆ©Êàë‰ª¨ÁªßÁª≠ÊâßË°åÊàë‰ª¨ÁöÑÊô∫ËÉΩ‰ΩìÔºåÂè™ÈúÄ‰º†ÈÄí`None`Âπ∂ÂÖÅËÆ∏ÂÆÉ‰ªéÂΩìÂâçÁä∂ÊÄÅÁªßÁª≠„ÄÇ\n",
    "\n",
    "Êàë‰ª¨ÂèëÂá∫ÂΩìÂâçÁä∂ÊÄÅÔºåÁÑ∂ÂêéÁªßÁª≠ÊâßË°åÂâ©‰ΩôÁöÑËäÇÁÇπ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f166bed2-87c9-41ec-b235-0305721c2d6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f166bed2-87c9-41ec-b235-0305721c2d6b",
    "outputId": "ee0bd980-4ec0-49d3-e382-a65aa66b03d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "No, actually multiply 3 and 3!\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_f7Xx8j8fDZaYKLh6754OGYqT)\n",
      " Call ID: call_f7Xx8j8fDZaYKLh6754OGYqT\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 3\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# ÁªßÁª≠ÊâßË°åÂõæÔºå‰ªéÂΩìÂâç‰∏≠Êñ≠ÁÇπÂºÄÂßã\n",
    "# ‰º†ÈÄíNoneË°®Á§∫‰ªéÂΩìÂâçÁä∂ÊÄÅÁªßÁª≠ÔºåËÄå‰∏çÊòØÈáçÊñ∞ÂºÄÂßã\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18dc1ca",
   "metadata": {
    "id": "b18dc1ca"
   },
   "source": [
    "Áé∞Âú®ÔºåÊàë‰ª¨ÂèàÂõûÂà∞‰∫Ü`assistant`ËäÇÁÇπÔºåÂÆÉËÆæÁΩÆ‰∫ÜÊàë‰ª¨ÁöÑ`breakpoint`„ÄÇ\n",
    "\n",
    "Êàë‰ª¨ÂèØ‰ª•ÂÜçÊ¨°‰º†ÈÄí`None`Êù•ÁªßÁª≠ÊâßË°å„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5952731-0170-4589-a399-ee787df35400",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5952731-0170-4589-a399-ee787df35400",
    "outputId": "2f880747-c7b3-41d7-f6e1-165ab9ca18f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "9\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The result of multiplying 3 and 3 is 9.\n"
     ]
    }
   ],
   "source": [
    "# ÁªßÁª≠ÊâßË°åÂõæÔºåÂÆåÊàêÂâ©‰ΩôÁöÑËÆ°ÁÆó\n",
    "for event in graph.stream(None, thread, stream_mode=\"values\"):\n",
    "    event['messages'][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (agent101)",
   "language": "python",
   "name": "agent101"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
